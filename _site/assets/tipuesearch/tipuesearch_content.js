var tipuesearch = {"pages": [{
    "title": "[Setup] 딥러닝 개발 환경 구축 한방에 끝내기",
    "text": "개요 딥러닝이라는 긴 여정을 위한 첫 단계. 딥러닝 개발 환경 구축을 위한 포스팅입니다. 환경설정 때문에 시간을 뺐기거나 귀차니즘을 최소화 하고자 가급적 모든 내용을 총정리합니다. 목차 사전 확인사항 및 GPU 준비 아나콘다(Anaconda) 설치 텐서플로(TensorFlow)를 위한 사전 호환성 검토 Visual Studio 2019 설치 CUDA 10.1 설치 cuDNN v7.6.5 설치 가상 개발 환경 만들기 및 접속 텐서플로 및 기타 라이브러리 설치 설치 환경 테스트 마치며… 사전 확인사항 및 GPU 준비 시간낭비를 방지하기 위해 먼저 아래의 구축할 기술 스택을 확인하시기 바란다. Ubuntu 등 Linux 환경은 시간이 나는대로 별도의 포스팅을 통해 기술할 예정이다. Mac은 2019년도 말 Nvidia와 결별했으므로 작성하지 않을 예정이다. 더불어 귀차니즘 때문에 딥러닝 및 머신러닝 학습을 포기하는 일이 없도록 거의 모든 환경 설정을 다룰 예정이다. 따라서 상단 개요의 목차를 잘 확인하시어 불필요한 부분은 건너뛰며 시간을 절약하시기 바란다. GPU 없이도 작업이 가능한 경우 상당 부분 건너뛸 수 있다. 구축할 기술 스택 Window10 64-bit Python 3.7 (아나콘다 conda 4.7.12) TensorFlow GPU 2.0 CUDA Toolkit 10.1 update2 NVIDIA® GPU drivers 418.x or higher. Visual Studio 2019 cuDNN v7.6.5 LightGBM, XGBoost 등 GPU 체크 1. 내 PC에 장착된 GPU 모델 확인 [윈도우키+X] &gt; 장치관리자 클릭 &gt; 디스플레이 어댑터 클릭 &gt; 모델명 확인 2. Nvidia 제품이 아니라면 구매 후 다시 읽어주시기 바란다. 물론 AMD나 Intel에서도 OpenCL 프레임워크로 딥러닝을 활용할 수는 있으나 결코 쉽지 않은 영역이다. 관심있으면 딥러닝프레임워크 비교를 참고하시기 바란다. 3. 내 GPU가 CUDA 툴킷을 활용할 수 있는지 Nvidia에 접속하여 확인한다. 아래 그림과 같이 CUDA-Enabled GeForce and TITAN Products 버튼을 클릭하면 좌측에 PC, 우측에 노트북 기준의 모델별 Capability가 나오는데 목록에 존재하지 않거나 3.0 미만이면 활용할 수 없다. 이상이 없다면 본격적으로 설치를 진행하자. 아나콘다(Anaconda) 설치 아나콘다는 파이썬 기반의 범용 패키지로 머신러닝을 비롯한 과학 계산용도로 인기가 좋다. 아나콘다 대신 Python을 직접 설치할 수도 있지만 필요한 패키지를 전부 찾아 pip 명령어로 별도로 설치해야 하거나, XGBoost 등 윈도우에서 설치가 까다로운 패키지들도 쉽게 설치할 수 있기 때문에 본 장에서는 아나콘다를 설치한다. 더욱이 텐서플로는 공식적으로 윈도우에서 아나콘다 배포판을 설치할 것을 권장한다. 아나콘다 공식 사이트에 접속하여 다운로드 한다. Download 버튼을 누르면 OS 및 32/64-Bit와 일치하는 버전이 자동으로 다운로드된다. 아래 그림과 같은 단계를 제외하고는 다운로드 된 파일(필자의 경우 Anaconda3-2019.10-Windows-x86_64.exe)을 실행한 후 디폴트 설정대로 Next만 눌러나가면 된다. 아래 그림을 만나게 될 경우 체크박스 2개 모두 체크 후 Install을 누른다. (체크 시 이미 기존에 설치된 Python이 있는 경우 충돌 이슈가 있는데 그런 상황을 맞닥드리는 고수분께서는 현명하게 알아서 체크 옵션을 조절할 능력이 되므로 초보자의 기준으로 설명한다.) 설치 완료 후, 시작버튼 &gt; Anaconda &gt; Anaconda Prompt 를 클릭하면 윈도우 명령프롬프트(cmd)와 비슷한 화면이 나온다. 아래 그림과 같이 python이라고 입력 시 Python의 버전이 나온다면 정상적으로 설치된 것이다. Ctrl+Z 명령어로 빠져나온 후 conda --version 명령어를 통해 아나콘다의 버전도 확인해본다. 만약 아나콘다를 최신 버전으로 업데이트 하고 싶을 경우 Anaconda Prompt를 관리자 권한으로 실행하여 아래와 같이 명령어를 입력한다. 여기에선 생략한다. conda update -n base -c defaults conda # 아나콘다 업그레이드 conda update --all # 파이썬 패키지 업그레이드 텐서플로(TensorFlow)를 위한 사전 호환성 검토 본 장에서는 텐서플로를 설치하기 위해 CUDA, cuDNN, Visual Studio와의 호환성을 먼저 검토한다. 흔히들 tensorflow-gpu를 처음 설치할 때 한번에 성공하는 사람이 드문데 바로 이 호환성 문제를 너무 쉽게 생각하기 때문이다. 필자의 동료들도 질문하는 분들이 많아 이 참에 호환성을 검토하는 방법을 짚고 넘어가려 한다.(최초 설치 뿐만아니라 추후 최신 버전의 업그레이드시에도 참고하시기 바란다.) 텐서플로를 설치하려면 당연히 공식문서를 확인해야 한다. https://www.tensorflow.org/install에 접속하면 먼저 상단에 지원 가능한 시스템이 나온다. 앞 장에서 윈도우10에 아나콘다를 설치한 것도 그냥 설치한 것이 아니라 사실 호환성을 검토하여 Python 3.5–3.7, Windows 7 or later의 요건을 확인 후 설치한 것이므로 반드시 본 문서를 버전별로 숙지하고 설치해야 한다. (그냥 감으로 때려맞추며 설치했다간 나중에 전부 다 재설치하는 봉변을 겪게 될지도 모른다.) 공식문서를 보자. 좌측 메뉴에 여러 특성들이 안내 되어있지만 이 중 눈여겨봐야 할 메뉴는 GPU 지원이다. 아래 그림과 같이 표시한 부분이 핵심이다. Hardware requirements CUDA-enabled GPU cards 항목이 첫번째 장에서 소개한 지원 가능한 GPU 목록 링크이다. 이미 확인했으므로 건너뛴다. Software requirements 첫번째, NVIDIA® GPU drivers 항목을 클릭하면 드라이버를 다운로드 받는 창이 뜬다. CUDA 10.1 requires 418.x or higher.라고 명시가 되어있으므로 우리 PC에 설치된 드라이버의 버전은 당연히 418. 버전 이상이어야 한다. 아닐 경우 아래와 화면과 같이 붉은색으로 표시한 부분에 내 PC의 GPU 정보를 입력 후 최신버전을 다운로드 받아 설치한다. 두번째, CUDA® Toolkit은 CUDA 툴킷에 대한 요건이다. TensorFlow &gt;= 2.1.0 이상인 경우에 CUDA 10.1을 지원한다는 것을 알 수 있다. 항목을 클릭하면 아래 화면과 같이 CUDA Toolkit 10.1 update2라는 10.1 버전의 최신 정보가 보인다. 우측의 Versioned Online Documentation를 클릭해서 조금 더 자세히 살펴보자. CUDA Toolkit v10.1.243 문서 클릭하면 CUDA Toolkit 가이드가 나오는데 Installation Guides &gt; Installation Guide Windows을 차례대로 클릭해보자. 아래 붉은색으로 표시한 부분에서 Cuda 10.1이 윈도우 10에서 지원한다는 사실과 컴파일러 도구로 Visual Studio 2019 16.x 버전을 설치해야 한다는 것을 알게 되었다. 이것으로 호환성을 위한 조사는 거의 끝났다. 비록 영어지만 공식 문서에 관련된 모든 정보가 꼼꼼하고 친절하게 기입되어있다. 추가적인 정보가 필요한 분들은 나머지 항목들도 유심히 살펴보시기 바란다. Visual Studio 2019 설치 위에서 조사한바와 같이 윈도우 환경에서 컴파일러 도구로 Visual Studio가 필요하다는 것을 확인했다. Visual Studio를 설치하는 목적은 LightGBM, Surprise등의 패키지를 활용하기 위함이므로 Visual Studio 2019용 Build Tools을 설치해도 되는데 OpenCL등의 활용 가능성을 위해 여기서는 Community 라이센스를 설치한다. 버전은 위에서 확인한대로 2019을 설치한다. https://visualstudio.microsoft.com/ko/downloads/에 접속하여 아래 그림과 같이 Visual Studio 2019 Community 버전을 다운로드한다. 다운로드 한 파일을 더블클릭하여 설치 중 아래 화면과 같이 C++를 사용한 데스크톱 개발에 클릭한 후 설치한다. CUDA 10.1 설치 위에서 호환성 조사 시 접속했던 CUDA Toolkit Archive에 다시 접속한다. CUDA Toolkit 10.1 update2 (Aug 2019)을 클릭한 후 아래와 같이 PC 환경에 맞는 값을 선택 후 다운로드 한다. 다운로드가 완료되면 해당 파일을 관리자 권한으로 실행하여 설치한다. cuDNN v7.6.5 설치 마찬가지로 software_requirements에 다시 접속한다. CUPTI는 CUDA Toolkit과 같이 제공된다고 명시되어 있으므로 별도의 설치가 필요없다. cuDNN SDK을 클릭한다. cudnn 페이지로 연결되는데 Download cuDNN 버튼을 클릭한다. 이어서 Login버튼을 클릭하여 로그인한다. 계정이 없는 경우 새로 만들어야 하는데 로그인 화면 하단의 Login with your social account를 클릭하면 페이스북 등의 SNS 계정으로 보다 쉽게 가입할 수 있다. 이메일 본인인증 및 간단한 설문조사를 마치고 나면 아래 화면과 같이 다운로드 페이지가 활성화된다. cuDNN Library for Windows 10을 클릭하여 다운로드한다. 설치 방법은 역시 공식문서에 자세히 나와있다. install-windows 링크를 참고하면 된다. 일단 다운로드 받은 파일을 압축풀기 하면 cuda 폴더 밑에 bin, include, lib 등 3개의 폴더가 존재함을 확인할 수 있다. 3개의 폴더를 복사하여 CUDA가 설치된 폴더인 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 폴더에 그대로 붙여넣기(덮어씌우기) 하면 된다. 윈도우 명령프롬프트에서 control sysdm.cpl &gt; 고급 탭 선택 &gt; 환경변수 버튼을 클릭한다. 시스템 변수에 CUDA_PATH 변수의 값이 잘 지정되었는지 확인하고, 없으면 공식 문서에서 언급한 바와 같이 다음의 환경변수를 추가한다.(일반적으로 별도의 환경변수를 추가할 필요는 없다.) Variable Name: CUDA_PATH Variable Value: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 가상 개발 환경 만들기 및 접속 이로써 GPU를 활용하기 위한 사전 환경설정은 끝났다. 이제 본격적으로 텐서플로 등의 패키지를 설치해야 하는데 그 전에 가상 개발환경을 구성한다. 이를 구성하는 이유는 프로젝트 별로 파이썬의 라이브러리 사용여부 및 버전이 다를 경우 충돌을 방지하기 위해서이다. 예를들어 최근 텐서플로 2.0 버전이 출시되었는데 1.x 버전을 사용하던 기존 프로젝트의 레거시 규모가 거대할 경우 2.0 버전으로 포팅을 완료하기 전까지는 1.x 버전을 활용해야 한다. 그런데 지금처럼 텐서플로 2.0을 활용하여 신규 프로젝트를 진행하고 싶은 경우 버전 충돌이 일어난다. 이를 피하기 위해서 프로젝트 단위별로 가상 환경을 구성하는 것이 중요하다. 우선 아나콘다 프롬프트에 접속한 후(시작버튼 &gt; Anaconda &gt; Anaconda Prompt), 아래와 같이 test용 폴더를 만든다. (base) C:\\projects\\dl\\test&gt; 이어서 필요 시 아나콘다의 버전을 최신화한다. (현 버전이 마음에 들면 건너뛰어도 무방하다.) (base) conda update -n base -c defaults conda 새로운 가상환경을 만든다. (base) conda create -n test python=3.7 가상환경으로 접속한다. (base) C:\\projects\\dl\\test&gt;activate test 텐서플로 및 기타 라이브러리 설치 가상환경에 접속하면 프롬프트 맨 앞이 (base) -&gt; (test)로 변경되는 것을 확인할 수 있다. 이제 가상환경에 필요한 버전의 텐서플로와 기타 필요 라이브러리를 설치한다. 쥬피터 노트북과 관련된 라이브러리를 제일 먼저 설치한다. (test) conda install -n test ipython notebook jupyter 다음으로 머신러닝 및 딥러닝에 자주 활용되는 패키지를 설치한다. 패키지명 각각의 기능이 궁금하다면 예전에 작성한 머신러닝을 위한 파이썬의 도구들을 참고하시기 바란다. (test) conda install -n test numpy scipy matplotlib spyder pandas seaborn scikit-learn h5py pillow matplotlib tqdm tensorflow를 설치한다. (test) conda install -n test tensorflow-gpu keras를 설치한다. 텐서플로 2 버전부터는 케라스가 내장되어 별도의 설치가 필요없으나 이전 버전인 경우 아래와 같이 설치한다. (test) conda install -n test keras 마지막으로 일반적인 방법으로는 설치되지 않는 까다로운 패키지들을 설치해보자. 먼저 그라디언트 부스팅으로 자주 활용되는 xgboost를 설치해보자. 위 방식과 동일하게 일반적인 방법으로 설치하면 “The following packages are not available from current channels”이라는 오류메시지가 나온다. 이럴 경우 보통 설치 명령어에 -c conda-forge 옵션을 추가하면 해결되는 경우가 많다. 이는 conda install에서 제공하는 다운로드 기본 채널에 패키지가 존재하지 않아서 발생하는 문제인데 conda-forge 옵션을 통해 검증된 패키지들이 모인 채널에서 강제로 다운로드 받아 설치하겠다는 옵션이다. 하지만 xgboost는 이 방법으로도 해결되지 않는다. 이럴 경우 최상의 솔루션은 위에서 언급한 바와 같이 공식 문서에서 찾는 것이다. https://anaconda.org/anaconda/py-xgboost링크에 접속하면 윈도우 64비트 버전의 xgboost를 설치하는 방법이 안내되므로 아래와 같이 해당 페이지에서 제시하는 명령어로 설치하면 이상없이 설치된다. (test) conda install -n test -c anaconda py-xgboost 마찬가지로 catboost도 위와 같이 공식문서를 참고하여 설치한다. (test) conda install -n test -c conda-forge catboost 동일하게 lightgbm 등도 위와 같이 설치한다. (test) conda install -n test -c conda-forge lightgbm # lightgbm (test) conda install -n test -c conda-forge pydotplus # pydotplus (test) conda install -n test -c conda-forge pydot # pydot 이상으로 패키지 설치를 마친다. 그 외 Kaggle 혹은 실무 프로젝트의 필요에 의해 다른 패키지 설치가 필요한 경우 https://anaconda.org/conda-forge/lightgbm와 같은 공식문서의 검색을 활용하시기 바란다. 설치 환경 테스트 이제 모든 준비는 끝났다. 남은 것은 지금까지 설치한 환경이 잘 돌아가는지 테스트하는 것이다. 아나콘다 프롬프트에서 가상환경에 접속한 후 아래의 명령어로 주피터 노트북에 접속한다. (test) C:\\projects\\dl\\test&gt;jupyter notebook 아래와 같이 예쁜(?) 쥬피터 노트북이 http://localhost:8888/tree 주소로 실행되는 것을 볼 수 있다. 우측의 New &gt; Python3 버튼을 차례로 클릭한 후 아래 예제를 차례로 실행해본다. 텐서플로 GPU 버전 설치여부 확인 빈 셀에 아래의 코드를 입력한 후 import tensorflow from tensorflow.python.client import device_lib print(device_lib.list_local_devices()) Shift + Enter 단축키를 누르면 아래 그림과 같은 GPU 정보를 확인할 수 있다. 이를 통해 tensorflow(CPU버전)가 아닌 tensorflow-gpu가 이상없이 설치되었음을 확인할 수 있다. 추가로 GPU 모델은 GeForce GTX 1660 Ti이고(스펙이 낮다고 비웃지 마시길…ㅜㅜ 이것도 와이프 몰래 사비를 털어서 어렵게 어렵게 장만했습니다.), compute capability는 7.5이며, 메모리는 약 4GB 정도 된다는 것을 확인할 수 있고 GPU는 1개가 있음 등의 정보를 확인할 수 있다. 그 외 기타 패키지들이 정상적으로 설치되었는지 아래의 코드를 통해 확인한다. import tensorflow from tensorflow import keras import pandas import sklearn import scipy import numpy import matplotlib import pydotplus import pydot import h5py print('tensorflow ' + tensorflow.__version__) print('keras ' + keras.__version__) print('pandas ' + pandas.__version__) print('sklearn ' + sklearn.__version__) print('scipy ' + scipy.__version__) print('numpy ' + numpy.__version__) print('matplotlib ' + matplotlib.__version__) print('h5py ' + h5py.__version__) 별다른 이상이 없다면 아래와 같은 결과가 나올 것이다. tensorflow 2.0.0 keras 2.2.4-tf pandas 1.0.1 sklearn 0.22.1 scipy 1.4.1 numpy 1.18.1 matplotlib 3.1.3 h5py 2.10.0 마치며… 드디어 딥러닝 개발 환경이 구축되었다. 여기까지 따라오시느라 정말 고생하셨다는 말씀을 드리고 싶다. 추후 업무 도메인별, 플랫폼별 별도의 환경설정 방식을 정리하여 하나씩 추가할 예정이다. 특히 캐글에 관련된 환경설정은 한번 반드시 짚고 넘어갈 생각이다. 더불어 딥러닝 개발과 관련하여 예전에 정리한 문서를 링크로 남긴다. [Colab] Google Colab 환경설정 및 사용법 [Jekyll Blog] 마크다운(Markdown) 사용법 및 예제 [R] R 설치 및 환경구성(10분만에 끝내는) 아나콘다 전용 파이참 다운로드 긴 글 읽어주심에 감사드리며 궁금하시거나 문제점이 발생된다면 댓글 부탁드린다. 앞으로 개인적으로 진행했던 흥미로운 예제를 바탕으로 머신러닝 및 딥러닝의 포스팅을 종종 올릴 생각이니 잊지말고 자주 들려주시면 감사하겠다.",
    "tags": "dl data science ai python local deep learning setup build dev",
    "url": "/dev/2020/02/14/dev-dl-setting-local-python/"
  },{
    "title": "[리뷰] 태블로 굿모닝 굿애프터눈",
    "text": "개요 본 리뷰는 비제이퍼블릭 출판사 \"태블로 굿모닝 굿애프터눈(강승일, 송재환 저)\"을 읽고 지식을 정리한 글입니다. 목차 태블로(Tableau), 가트너 선정 BI분야 최고의 리더 본 도서와 함께하는 다양한 태블로(Tableau)의 기능 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 태블로(Tableau), 가트너 선정 BI분야 최고의 리더 최근 AI, 빅데이터 트렌드와 더불어 데이터 시각화(Data Visualization)의 중요성이 급부상하고 있다. 통계, 머신러닝, 딥러닝을 활용하여 유의미한 인사이트를 도출하거나 예측, 판별, 생성 등의 모델을 구현하는 것은 여전히 복잡한 문제이다. 이런 문제를 해결하는데 있어 데이터 시각화는 다양한 방법으로 도움을 준다. 흔히 블랙박스라고 칭하는 딥러닝 내부 학습 과정을 파악하고 싶거나, 다양한 포맷과 피처를 가진 대규모 데이터 속에서 패턴을 찾는 과정에 있어 시각화를 이용하면 원하는 답을 쉽게 얻거나 답을 얻기 위한 시간을 상당히 절약할 수 있다. 태블로는 데이터를 분석 및 시각화하는 BI 솔루션으로 전세계 8만개 이상의 기업이 이용하고 있으며, 태블로가 차지하는 영향력은 최근 가트너가 BI분야 최고의 리더로 선정한 일례로 대신할 수 있다. 태블로가 어떤 솔루션인지 백번 설명하는 것보다 아래 그림을 보는 것이 나을 듯하다. 최근 신종 코로나 바이러스 확진자의 이동 경로를 시각화한 그림으로 태블로로 구현한 것이다. 한국 태블로(Tableau) 사용자 모임 페이스북 그룹에 공유된 게시글에서 발췌하였으며 Il Sup Lee 회원님께서 구현 및 공유하셨다. 본 도서와 함께하는 다양한 태블로(Tableau)의 기능 Part1(굿모닝)에서는 먼저 태블로의 Tableau Desktop, Server, Prep 등 다양한 제품군에 대해 소개한 후 아래 그림과 같이 Tableau Desktop 설치를 통해 간단한 기능을 맛보게 해준다. 여느 프로그램이나 그러하듯 설치 후 처음 실행하면 생소하다. 하지만 시작페이지의 연결, 열기, 검색 영역의 기능부터 시작하여 데이터 원본 페이지, 작업 영역의 메뉴별 다양한 기능까지 상세히 설명하므로 읽다보면 빠르게 사용 감각을 익힐 수 있다. 특히 태블로의 다양한 파일 유형 .twb, .tds 등에 대해 자세히 알아보고 측정값, 차원 등의 생소한 기본개념을 먼저잡고 출발할 수 있어 초보자도 후반부의 복잡한 실습 예제를 수행할 수 있도록 도움을 준다. 이어서 3장. 기본 차트 만들기라는 기본 예제들을 실습하게 되는데 개인적으로 상당히 중요한 부분이라 생각한다. 실습을 따라하기에 전혀 부담이 없고 컴포넌트 하나 하나에 집중하여 예제를 개발하며 상세한 기능을 소개하는 방향으로 진행이 되는데 이 부분을 확실히 알아야만 나중에 실무에서 독자들이 원하는 숲을 그려낼 수 있다. 최근 데이터 사이언스의 영향으로 R의 ggplot2나 Python의 matplotlib, pyecharts 등을 활용하는 분들도 많을텐데 대부분의 시각화 구현의 문제는 라이브러리를 활용한 구현 자체에 있는것이 아니라 최종 산출물을 만드는데에 별도의 감각이 필요하다는 사실이다. 필자도 마찬가지였는데 기본에 충실한 실습들을 진행하며 데이터가 가지는 특징에 따라 어떤 시각화 방법이 가능한지 일종의 표현 방법론에 대한 감을 익힐 수 있었다. 태블로 자체의 기능을 익히는 것은 물론 데이터의 주어진 상황과 특징에 따라 태블로 개발자들은 이러한 방식을 활용했구나라는 아이디어 및 인사이트를 얻을 수 있다. 기본 실습의 구체적인 예제는 아래 목록과 같으며 시각화를 자주 다루는 분이라면 이 목록들의 수준이 기초이지만 꽤 중요한 개념들이라는 사실을 깨달으실 것이라 생각한다. 시간이 넉넉하지 않은 분들은 본 장은 건너뛰신 후 필요할 때마다 책장에서 꺼내보는 일종의 레퍼런스로 활용하셔도 되겠다. 막대 차트 만들기, 라인 차트 만들기, 파이 차트 만들기, 도넛 차트 만들기 임시 계산, 분산형 차트 만들기, 트리맵 차트 만들기, 하이라이트 테이블 만들기 영역 차트 만들기, 누적 막대 차트 만들기, 간트 차트 만들기, 히스토그램 만들기 이중 축 만들기(Dual axis), 결합된 축 만들기(Combined axis) 라운드형 막대 차트 만들기, 평균 라인 만들기, 워드 클라우드 만들기 캘린더 차트 만들기, 총계 만들기, 계층 만들기, 지리적 역할 부여하기 채워진 맵 만들기, 기호 맵 만들기, 밀도 맵 만들기, 이중 축 맵(면+기호) 그룹과 집합 차이, 데이터 설명 Part2(굿애프터눈1) 파트에서는 위에서 배운 기본차트 구현의 수준을 넘어 주어진 데이터를 가공하여 시각화하는 방법을 학습한다. 먼저 퀵 테이블 계산방법을 배우게 되는데 조금이라도 데이터에 관련된 업무를 수행한 분이라면 흔히 접하게 되는 전월대비 및 전년대비 성장률의 시각화 등을 구현할 수 있다. 주가 차트의 이평선 등 약간은 복잡할 수 있는 시각화 표현 방법을 어떻게 쉽게 효율적으로 표현할 수 있는지 도와준다. 이어 IF로 계산된 필드를 만드는 방법, 목록형 매개 변수의 생성 방법을 익힌 후 전체 범위로 필터를 적용하거나 컨텍스트 필터에 추가하는 상세한 방법을 다룬다. 대시보드 액션 적용하기 실습을 따라하다보면 아래 그림과 같은 뛰어난 시각적 표현도 가능하다. Part3(굿애프터눈2)에서는 드디어 본격적으로 데이터를 주무르기 시작한다. 필자가 가장 재미있게 실습했던 파트이기도 하며, 태블로가 얼마나 다양한 데이터를 손쉽게 다룰 수 있는지 장점을 맛볼 수 있는 파트이기도 하다. 데이터 원본을 설정하는 방법으로 조인, 교차 조인, 블렌딩, 유니온, Split의 실습을 통해 일종의 데이터 전처리 방법을 학습하며 데이터 해석기를 사용하여 원본 필터 등을 적용함으로써 데이터를 직관적으로 이해할 수 있도록 돕는다. 다음으로 Prep을 다룬다. Prep은 데이터를 결합, 정리, 변형하는데 편리한 기능을 제공하는 도구로 데이터 분석 직군 혹은 개발자가 아닌 기획 및 비지니스 직군에 계신분들도 매우 직관적으로 편리하게 데이터를 활용할 수 있다는 장점이 있다. 태블로가 BI업계 시각화 리더가 된 결정적인 계기가 Prep과 같은 비전문가들도 사용하기 쉬운 도구의 제공 덕분이 아니었을까 싶다. 아래 그림은 Prep을 이용하여 데이터 결합과정을 시각화한 사진이다. 사진이 보여주듯 Prep에서 제공하는 대부분의 기능들은 비 전문가들이 다루기 매우 편리하게 도와준다. 본 파트의 마지막엔 구글 스프레드시트를 활용한 재미있는 예제를 다룬다. 지금까지 우리가 필요로 하는 입맛에 맞춘 데이터로만 실습을 해왔다면 본 장에서는 웹에 널린 데이터를 직접 크롤링한 후 태블로와 연동하는 보다 실전적인 예제를 다룬다. 아래 그림과 같이 구글 스프레드시트를 활용하여 미국 메이저리그 야구장 리스트를 웹페이지에서 크롤링한 후 태블로의 맵형태로 구장들의 위치를 표현할 수 있다. 마지막으로 Part4(굿이브닝)파트에서는 태블로가 가진 보다 강력한 심화기능을 다룬다. LOD 세부수준식인 Fixed, Exclude, Include를 활용하여 보다 사용자가 원하는 기준과 관점에 맞는 세부적인 집계 및 통계 자료를 추출할 수 있다. 더불어 타임라인에 특화된 처리가 가능한 복합 매개변수를 다루는 방법을 학습한다. 이어 PDF에 담긴 주식 데이터를 태블로에 연동하는 방법, 변경된 데이터의 증분만 서버에 업로드 하는 노하우, 그리고 태블로 퍼블릭에 업로드하여 전세계에 본인이 만든 대시보드를 공유하는 방법까지 태블로가 가진 확장성을 맛볼 수 있다. 본 도서의 저자들은 태블로에 직접 몸담고 있는 전문 강사들로 본 도서와 더불어 아래 사진에서 안내하는 바와 같이 유튜브 채널을 통해서도 노하우를 습득할 수 있다는 장점이 있다. 책소개 Link PLANIT DATAV 유튜브 채널 누가 읽어야 하는가? 태블로에 관심이 많거나 처음 접하셔서 활용에 어려움을 겪는 분. 태블로 플랫폼을 활용하는 실무자. 그 외 데이터 시각화에 관심있는 분. 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 태블로 굿모닝 (Part1) 태블로 제품 소개 및 설치 태블로의 기본 UI 활용방법에 대한 설명, 기본개념(측정값 및 연속형 등) 기본차트 만들기 예제 실습 2. 태블로 굿애프터눈 (Part2,3) 퀵테이블, 계산된 필드 만들기 범위형, 목록형 매개변수 만드는 방법 대시보드 액션 적용하기 (필터, 하이라이트, URL/시트 이동, 매개변수 및 집합 값 변경) 데이터 전처리(조인, 교차 조인, 블렌딩, 유니온, Split 등) Prep과 함께하는 데이터 연동 및 정제 방법 구글 Spreadsheet를 활용한 크롤링 및 태블로 연동 3. 태블로 굿이브닝 (Part4) LOD Expressions(세부 수준 식) 및 복합 매개변수 만들기 상황에 맞는 대시보드 만들기(PDF 연동 등) 서버에 업로드하기(Live 연결, 퍼블릭 업로드, 질문하기 등) 숨겨진 보석들 부록 : 거래절차, 세금총정리, 투자참고 사이트 목록 등 요약하며… 데이터 시각화는 통계 등을 활용한 정보의 요약 전달 및 의사결정 등 자체로도 다양한 장점을 갖고 있지만, 특히 빅데이터 및 AI와 관련하여서도 내부 학습 과정을 파악하는데 도움이 되거나 다양한 포맷과 피처를 가진 대규모 데이터 속에서 패턴 인사이트를 찾는 과정에 유용하다. 본 도서는 초반에 태블로의 상세 기능을 다루고 특히 3장 “기본 차트 만들기” 파트에서 거의 모든 컴포넌트들을 구현해보는 실습 과정을 거치기 때문에 초반 빠른 사용 감각을 익히는데 도움을 준다. 시각화에서 겪는 대부분의 문제는 라이브러리 구현 자체에 있는것이 아니라 최종 결과물을 만드는데에 별도 감각이 필요하다는 점인데 본 도서를 통해 데이터 특징에 따라 어떤 유형의 시각화 방법이 가능한지 감을 익힐 수 있다는 점이 큰 장점 중의 하나이다. 후반부에 다양한 데이터의 연동 방법을 익힐 수 있으며 퀵 테이블 계산방법, 계산된 필드 만들기, 조인, 교차 조인, 블렌딩, 유니온, Split 등 데이터 전처리 방법을 학습하여 태블로 연동에 필요한 데이터를 가공 스킬을 쌓을 수 있다. 이를 바탕으로 시각화 자체에도 충실할 수 있음은 물론 구글 Spreadsheet를 활용하여 크롤링 후 태블로에 연동하거나, PDF 문서와의 연동을 통한 시각화 등 태블로가 가진 다양한 확장성을 음미할 수 있게 된다. 본 도서는 태블로를 상세하게 다루는 국내 최초의 도서라는 점에서 의의가 있고, 저자들은 태블로에 직접 소속된 전문 강사들로 직접 운영하는 유튜브 채널 시청을 통해 학습 효과를 높일 수 있다는 장점이 있다. 반면 전체 내용이 특정 프로젝트의 완성을 향한 흐름을 유지하며 통일성있게 구성되었다면 보다 전달력을 높이고 실전에 도움이 되지 않았을까 하는 아쉬움이 있다. 더불어 너무 상세한 기능 구현과 설명에 집중한 나머지 약간의 메뉴얼 같은 딱딱한 느낌을 갖게한 점과 그에 따른 독자 친화적인 구성과 거리가 있다는 점이 아쉽지만 태블로를 다루는 최초의 책인만큼 상세한 레퍼런스를 다루는 책도 필요하겠다는 생각도 든다. 실무에 태블로의 의존도가 높거나 데이터 시각화에 관심이 많은 분이라면 일독을 권하는 바이다. &lt;비제이퍼블릭 출판사&gt; 한 분야의 실전적 절대고수가 그간 쌓아온 지식과 경험을 바탕으로 내공의 정수를 담은 장인의 향기가 나는 책을 출판하는 회사입니다. 참신한 해외 번역서, 최신 트렌트를 겨냥하는 양질의 서적을 자주 출간하여 필자의 책장에도 어느덧 십여권의 책이 꼽혀있네요. 특히, IT 기술을 향상시키는데 있어 제자리 걸음이거나 시간투자 대비 내공이 채워지지 않는 공허함이 생긴다면 비제이퍼블릭 출판사의 책 목록을 한번 살펴보시기 바랍니다. 비제이퍼블릭 바로가기",
    "tags": "review book tableau bi bigdata visualization software",
    "url": "/review/2020/01/22/review-book-tableau/"
  },{
    "title": "[리뷰] 알파제로를 분석하며 배우는 인공지능",
    "text": "개요 본 리뷰는 제이펍 출판사 \"알파제로를 분석하며 배우는 인공지능(후루카와 히데카즈 저)\"을 읽고 얻은 지식을 정리한 글입니다. 목차 2020년 처음으로 읽은 책 알파고, 알파고제로, 알파제로 딥러닝, 강화학습, 탐색의 핵심개요 깨알같은 디테일로 독자의 진입장벽을 낮춘다. 딥러닝 강화학습 탐색 대망의 알파제로 구현! 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 2020년 처음으로 읽은 책 새해 처음으로 올리는 리뷰를 본 도서로 시작할 수 있어 상쾌하다. 지금까지 읽었던 딥러닝 계열의 도서 중 가장 재미있고, 가장 포괄적인 내용을 다루고 있으며, 그럼에도 가장 쉬워, 가장 극찬하고 싶은 책이다. 단 370 페이지의 짧은 분량으로 알파고의 발전사, 파이썬 문법, 구글 Colab 및 TPU, 딥러닝, CNN, ResNet, 정책경사법, Sarsa, Q, DQN, 미니맥스법, 알파베타법, 몬테카를로 트리 탐색, 알파제로 구현, thinker를 활용한 UI 개발, 틱택토, 커넥트4, 오셀로, 간이장기…를 어떻게 다 가르쳐 줄 수 있는건지 또, 어떻게 이렇게 쉽고 재미있게 알려줄 수 있는건지 두 번에 걸쳐 정독한 지금도 믿기지 않는다. 딥러닝을 몰라도, 심지어는 python을 몰라도, 조금 더 오버하자면 프로그램의 기초 개념 정도만 알아도 본 도서를 이해하는데 그리 어렵지 않을 것이다. 가시적으로 실행 가능한 코드가 제공되고, 딥러닝의 발전 방향에 맞춰 모델 간 장단점을 비교해 나가며 핵심 차이점만 깔끔하게 전달하는 전개 방식, 그리고 심지어 구글 Colab의 자세한 사용법과 파이썬 문법까지 알려준다. 각 단원마다 재미있는 게임을 예로 들어 상상력을 전개하기 쉽게 도와주고 상상력이 원동력이 되어 이해를 돕도록 구성한 점도 특징이다. 왜 이렇게 극찬하는지 궁금하신 분들은 잠깐 시간내어 본 리뷰를 읽어주시기 바란다. 성격 급한 분들은 당장 책을 구매하셔서 보시는게 시간 절약 측면에서 나을 수도 있겠다. 구체적인 리뷰를 시작하기에 앞서 아래 로드맵을 참고하기 바란다. 본 도서는 로드맵이 목차보다도 깔끔하며 본 리뷰 또한 동일한 순서로 진행할 예정이다. 알파고, 알파고제로, 알파제로 알파고는 구글 딥마인드에서 개발한 인공지능 바둑 프로그램으로 이세돌 9단에게 4승 1패의 승리를 거두며 전세계에 AI, 데이터 사이언스의 열풍을 일으켰다. 알파고 제로는 알파고를 상대로 100대 0으로 압승을 거두었으며, 기보없이 셀프 플레이만으로 학습하였다. 알파제로는 알파고 제로보다 승률은 뛰어나고 학습 속도는 향상된 장점을 가진 최신 버전이다. 본 포스팅을 읽고 계신 분이라면 일반적인 특징 보다는 기술적인 차이점에 관심이 많으실 것이다. 알파고는 지도학습과 강화학습을 혼용한데 반해 알파제로는 강화학습만 사용했다는 점이 큰 특징이고, 딥러닝 네트워크 구조에 있어 알파고는 CNN을, 알파제로는 ResNet을 사용했다는 차이점이 있다. 기타 자세한 차이점은 본 도서에서 발췌한 아래 그림을 참고하시기 바란다. 딥러닝, 강화학습, 탐색의 핵심개요 1장에서는 흥미진진한 스토리로 알파제로를 간단히 설명한 후 퍼셉트론, 딥러닝, 지도학습(분류, 회귀), 비지도학습, CNN, RNN 등 딥러닝과 관련된 핵심 개념을 다룬다. 딥러닝을 이미 알고 있는 독자에게 핵심 지식을 깔끔하게 정리할 기회를 주고, 전혀 모르는 분도 중요한 개념을 짚고 넘어가게 해준다. 아래 그림은 AND 함수를 구현한 퍼셉트론 및 딥러닝 학습 모델에 대한 도식도이다. 이어서 에이전트, 환경, 행동, 상태, 보상, 정책, 수익, 가치 등 강화학습의 핵심 개념을 다룬다. 아래 그림은 강화학습의 용어 및 학습사이클을 설명한 부분이다. 이어 탐색의 핵심 개념을 다룬다. 완전 게임 트리와 부분 게임 트리를 다룸으로써 알파베타법이 가지는 한계와 이를 대체하기 위한 수단인 몬테카를로 트리 탐색의 개념을 미리 소개한다. 깨알같은 디테일로 독자의 진입장벽을 낮춘다. 딥러닝과 알파제로의 핵심 주제에는 벗어나지만 이 책에 숨은 또 다른 공신을 찾자면 알파제로를 구현하기 위한 파이썬 문법, 구글 Colab의 사용법, 로컬 환경 및 코랩 환경의 패키지 버전 소개 등 개발 환경 구축을 위한 정확하고 디테일한 설명 등을 들 수 있겠다. 예를들면 아래와 같이 책의 예제를 구현할 때 사용한 패키지 버전까지 상세하게 명시한다. 따라하기만 해도 알파제로를 만들 수 있도록 저자의 친절한 배려가 돋보인다. 심지어 파이썬을 파이썬 답게 사용할 수 있는 문법을 총망라함으써 파이썬에 생소한 독자들도 쉽게 파이썬 문법에 적응하게 해준다. 두꺼운 파이썬 책을 읽다가 망각하는 것 보다 처음에 본 도서로 파이썬을 익혀도 되겠다는 생각이 들 정도였다. 더불어 돈 한푼 없어도, 뛰어난 장비 없어도 개발할 수 있도록 구글 Colab에 관하여 필수 지식을 모두 전수해준다. 90분 룰 회피하기, 12시간 룰 회피하기 같은 Tip도 전수해주며 GPU가 아닌 TPU를 활용한 개발 방법까지도 자세하게 안내해 준다. 모든 책들이 흔히 말하지 않는 고수들만 알 법한 깨알같은 팁을 아낌없이 공유한다. 이러니 도저히 개발을 못할 수가 없다. 독자들이 무엇때문에 알파제로를 개발하지 못할지 그 경우의 수를 수도 없이 고민한 장인의 냄새가 난다. 덕분에 독자들의 진입장벽이 현저히 낮아져 단언컨데 열정만 있는 독자라면 책을 덮고 난 뒤에는 프로그램을 거의 몰라도 알파제로를 만들고 상당 부분 이해하며 뿌듯해 할 것이다. 딥러닝 딥러닝을 조금이라도 학습해 본 분들은 누구나 MNIST 예제 정도는 한번쯤 돌려보셨을 것이다. 방심하지 말고 다시 돌려보자. 그저 그런줄 알았던 MNIST 일반적인 분류 모델이 CNN과 어떻게 다른지 명확히 느낄 수 있는 기회다. 더불어 ResNet과는 어떤 차이가 있는지 비교가능하다. 아래 그림과 같은 이해를 돕는 구조도와 함께 말이다. 실습 코드 구현을 통해 이미지 인식 분야의 발전 과정을 명확히 이해할 수 있다. 발전 과정에서 등장한 모델을 서로 비교하며 구현하는 과정에서 기억이 오래 남도록 유지해주는 효과를 얻을 수 있다. 각 장별로 사용한 색상도 달라 현재 내가 어느 파트를 읽고 있는지 도와주기도 한다. 이런 저자와 편집자가 구성한 기억 지속 장치 덕분에 후반부의 알파제로를 구현할 때 본 장에서 배운 ResNet 지식이 대부분 되살아남을 느꼈다. 강화학습 다중 슬롯머신 문제를 통해 어떤 팔을 당겨야 가장 많은 돈을 벌 수 있을지 일종의 게임이론과 같은 흥미로운 주제로 강화학습에 접근한다. 탐색과 이용이라는 트레이드 오프 관계를 이해하고 균형을 맞추기 위한 아래 그림과 같은 UCB1에 관해 설명한다. 수학없이 코딩만 등장하는 허접한 책이 아니다. 놀라운 것은 수식이 제법 나오는데도 거의 이해되는 신기한 책이라는 점이다. 강화학습은 처음 접하면 생소한 개념에 좌절하기 쉬운데 각 절마다 개념을 알기 쉽게 예제를 통해 이해를 돕는다. 정책 경사법 또한 미로찾기 같은 재미있는 게임을 통해 핵심 개념을 설명한다. Sarsa와 Q의 개념 또한 앞서 미로찾기에 대한 설명을 활용하여 이해를 돕고 행동 및 상태 가치 함수를 벨만 방정식으로 정리한다. 탄탄히 정리된 기초 개념을 통해 DQN을 정리한다. 카트-폴 게임을 가시적인 예제로 사용하여 앞서 나왔던 개념들의 장단점을 총정리하게 되며 이 과정에서 알파제로에서 사용할 정책망과 가치망이 어떤 식으로 구현되는지 감을 잡을 수 있다. 탐색 일반적인 음성, 이미지, NLP 분야와 달리 게임분야 AI의 색다른 점이 있다면 바로 탐색이 아닐까 한다. 유한 확정 완전 정보 게임이라면 더욱 탐색 기법을 사용하지 않을 수 없는데 알파제로 구현에 있어서도 역시 탐색이 핵심 기법 중 하나이다. 먼저 미니맥스법을 통해 게임트리의 개념을 이해한 후 알파베타법의 가지치기 방법을 통해 계산의 속도를 높이는 방법을 배운다. 아래 그림과 같이 세부적인 예시를 들어 어떻게 탐색을 하지 않고 가지를 쳐 속도를 높일 수 있는지 이해를 돕는다. 이어 완전 게임 트리가 경우의 수가 많아질 수록 활용하기 어려운 이유를 설명하고 대체 수단인 몬테카를로 트리 탐색의 개념을 설명한다. 1 ~ N 회차의 시뮬레이션을 반복하며 트리 노드에 큰 변화가 있을 때 마다 아래 그림과 같이 알기 쉽게 설명해준다. 대망의 알파제로 구현! 앞서 배운 딥러닝, 강화학습, 탐색 등 핵심 기술을 집대성하여 알파제로를 만든다. 알파제로에서 사용할 강화학습 사이클 및 딥러닝 네트워크의 구성 그리고 최종 완성된 틱택토 게임은 아래와 같다. 이 장에서는 딥러닝, 강화학습, 탐색을 어떻게 결합할 수 있는지 소프트웨어 공학의 진수가 담겨있는 장이기도 하다. 더불어 8장에서는 알파제로를 구현하며 만들었던 모델을 재활용하고, 모델의 저장과 로드를 통해 커넥트4, 오셀로, 간이장기를 개발한다. 이 과정에서 스프트웨어 공학은 물론 실무에서 이미 개발한 모델을 어떻게 변형된 모델에 적용할 수 있는지 실무 감각을 높일 수 있다. 이 뿐만이 아니다. 알파제로에서는 몬테카를로 트리 탐색 시 앞서 설명한 UCB1을 활용하지 않고 아래 그림과 같은 공식의 아크 평갓값을 사용한다. 이처럼 필요한 분야에 적합한 수학적 모델링을 도출하고 직관적으로 이해하여 데이터와 비교 대응할 수 있는 데이터 사이언티스트의 역량을 엿볼 수도 있다. 도서 미리보기 GitHub 소스코드 누가 읽어야 하는가? ResNet, 몬테카를로 탐색 트리, DQN에 부딪혀 딥러닝 학습에 좌절을 겪은 분 가시적인 제품 구현을 통해 딥러닝의 핵심 개념을 명확하게 이해하고 싶은 분 알파제로를 구현하고 싶거나 알파고 관련 논문 이해에 어려움을 겪는 분 기타 딥러닝에 관심이 있는 모든 분 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 머신러닝의 개요, 파이썬, 구글 Colab 알파고, 딥러닝, 강화학습, 탐색의 개요 구글 Colab의 사용법 Python 문법 학습 등 2. 딥러닝, 강화학습, 탐색의 핵심 개념 딥러닝 : 분류, 회귀, CNN, ResNet 강화학습 : 정책경사법, Sarsa, Q, DQN 탐색 : 미니맥스법, 알파베타법, 몬테카를로 탐색 트리 등 3. 알파제로의 구현 Part2의 핵심 개념을 종합한 알파제로의 구현 알파제로의 모델을 재사용하여 커넥트4, 오셀로, 간이장기 등의 구현 Thinker를 활용한 UI 구현(사람과의 대전) 요약하며… 2020년 새해. 처음으로 읽어도 될만한 가치가 있는 귀한 책을 만났다. 리뷰를 작성할 때는 책의 장점을 진솔하게 전달하기 위해 노력하는 편이지만 특히 이번 책의 리뷰를 소홀히 작성하는 것은 예의가 아니라는 생각이 들만큼 훌륭한 내용과 짜임새있는 품격이 느껴져 두 번에 걸쳐 정독했다. 책의 내용을 한바탕 정리하고 싶은 마음이 가득했으나 그러기엔 리뷰 수준의 분량을 넘어설 듯 하여 가급적 책의 장점만 피력하고자 노력했다. 앞서 설명한 바와 같이 지금까지 읽었던 딥러닝 계열의 도서 중 가장 재미있고, 가장 넓은 영역을 다루고 있으며, 그럼에도 가장 쉬워, 가장 극찬하고 싶은 책이다. 단 370페이지의 짧은 지면으로 알파고의 발전사, 파이썬 문법, 구글 Colab 및 TPU, 딥러닝, CNN, ResNet, 정책경사법, Sarsa, Q, DQN, 미니맥스법, 알파베타법, 몬테카를로 트리 탐색, 알파제로 구현, thinker를 활용한 UI 개발, 틱택토, 커넥트4, 오셀로, 간이장기 구현방법 등을 모두 알려준다. 딥러닝을 전혀 몰라도, 심지어는 파이썬을 몰라도, 조금 더 오버하자면 프로그램의 기초 개념 정도만 알아도 본 도서를 이해하는데 그리 어렵지 않을 것이다. 가시적으로 실행 가능한 코드가 제공되고, 딥러닝의 진화 방향에 맞춰 모델 간 장단점을 비교해 나가며, 심지어는 구글 Colab의 자세한 사용법과 파이썬 문법까지 알려준다. 각 단원마다 재미있는 게임을 통해 개념을 전달하고자 한 시도도 눈에 띄는 특징이다. 지난 2019년 한해동안 GAN, Reinforcement Learning으로 대표되는 AI기술들과 연구 결과는 끊임없이 쏟아졌고 축적되는 관련 지식의 Volume은 점차 가속화되고 있다. BERT와 같은 기술 하나에 집중하기도 어려운데 정신을 차리고 주위를 둘러보면 MuseNet이니 GPT-2이니 주옥같은 기술들이 이미 등장하여 상용화 되어간다. 본 도서를 통해 딥러닝의 빠른 발전 속에도 길을 잃지 않도록 알파제로까지의 핵심 개념을 명확히 이해하여 허리를 튼튼히 하고, 알파제로의 구현을 통해 이해에 이해를 덧칠한다면 2020년에 다가올 폭풍우도 그리 두렵지 않을 것이다. 책의 내용은 말할 것도 없고 기억을 오랫동안 지속시키기 위한 편집과 구성도 일품이라는 생각이 든다. 아쉬운 점은 사실 없다. 굳이 애써 찾자면 텐서플로 2.0을 사용했으면 더 좋았을 뻔 했다는 점, 알파제로의 모델로 바둑을 구현했으면 좋겠다는 점이 있겠으나, 그러기엔 이 책을 늦게 만나게 되거나 실습을 포기해야 하는 트레이드 오프를 감당해야 하므로 현재로도 대 만족이다. AI, 딥러닝에 관심이 있는 분이라면 초보자, 고수 가리지 않고 반드시 읽어보시길 바란다. 분명 무엇이든 얻게 될 것이다. &lt;제이펍 출판사&gt; 보통 서적에 담긴 내용의 기술적 깊이가 뛰어나면 가독성과 재미는 떨어지게 마련인데, 제이펍의 책들은 그 트레이드 오프가 성립하지 않아 개인적으로 매우 신기하다고 생각했습니다. 진정성 듬뿍담긴 깊이있는 내공을 즐겁게 흡수하고 싶다면 꼭 제이펍의 도서목록을 살펴보시기 바랍니다. 숨은 진주를 얻게 되실 겁니다. 제이펍 바로가기",
    "tags": "review book alpha zero deep learning dqn mcts thinker tic tac toe",
    "url": "/review/2020/01/17/review-book-alphazero/"
  },{
    "title": "[리뷰] 미국 주식 스타터팩",
    "text": "개요 본 리뷰는 비제이퍼블릭 출판사 \"미국 주식 스타터팩(정두현 저)\"을 읽고 지식을 정리한 글입니다. 목차 달러에 관심이 생겼다.(feat. 미-중 무역분쟁) 미국 주식이 매력적인 이유 시간에 구애받지 않는 두고두고 볼 수 있는 책 책의 화룡점정 - 사업보고서 분석과 경험기반의 통찰력 마무리 &amp; 책에서 추천하는 주요사이트 URL 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 달러에 관심이 생겼다.(feat. 미-중 무역분쟁) 미-중 무역분쟁이 장기화되고있던 2019년 8월. 환율이 급등하여 최근 10년간의 박스권을 뚫고 최고점을 경신할 뻔한 위기가 있었다. IMF를 겪은지 20년이 지났지만 당시 평소 대비 2배를 넘겼던 환율과 그로 인해 파탄난 경제위기를 직접 겪었기에 그 악몽이 얼마나 끔찍했는지 생생하다. 경제위기 자체도 문제였지만 환율이 급등하면서 생기는 원화 가치의 하락은 그 위기를 가중시켰다. 8월 13일. 환율이 1,223원까지 치솟으며 미-중 무역분쟁은 해소될 기미를 보이지 않은데다 무역 의존도가 높은 우리나라의 산업 특성을 고려해볼 때 달러 매입을 심각하게 고려해야 하는 시점이 되었다고 생각했다. 자산 몇푼 보유하지 못한 서민이 경제위기로 반토막나고, 원화가치 하락으로 4토막 난다면 가정경제는 큰 위기에 빠질것임은 불을 보듯 뻔하다. 최고점에 육박했을 때의 매입은 하수나 하는 행동임을 그간의 뼈아픈 주식 경험으로 알고 있었기에 적당한 시점을 기다려 대비하고자 했다. 결국 은행에서 달러통장을 개설하고 관련 정보를 조사하기 시작했다. 그 과정에서 환율이 2배로 상승할 경우 달러보다는 주식이 약 2배 가까이 더 가치있다는 것을 알게 되었고, 더불어 채권의 세계도 알게 되었다. 그렇게 투자를 결심했지만 문제는 미국 주식의 가치투자 방법과 세금 문제를 비롯 전반적인 지식이 없다는 데에 있었다. 인터넷에서 정보를 수집하고 시행착오를 겪으면 어떻게든 해결은 되겠지만, 그러기엔 너무 많은 시간과 노동력을 낭비해야 한다. 그런 시점에 마침 미국 주식에 관련된 지식을 한방에 정리해주는 도서를 만난것은 행운이었다. 미국 주식이 매력적인 이유 아래 그림은 본 도서에서 발췌한 최근 5년간 한국, 미국, 일본의 각 국가를 대표하는 주식 상승률을 비교한 그래프이다. 보시는 바와 같이 적어도 최근 5년간 한국의 주식 시장은 전혀 매력적이지 못했다. 비록 5년이라는 함정이 숨어있으므로 장기적인 관점에서 비판할 여지가 있다. 그런데… 과연 그럴까? 아래 그림은 한국 부동산 투자의 대명사가 되어버린 은마아파트 가격의 상승률과 코카콜라의 상승률을 비교한 그림이다. 동일기간 환율을 배제한 상승률은 코카콜라가 은마아파트에게 크게 뒤지지 않는다. 오히려 환율을 반영했을 때 코카콜라의 수익률이 비교도 안되게 월등해진다. 달러는 세계의 기축통화이고 경제가 호황일 때는 양적완화 정책때문에, 불황일때는 IMF와 같은 환율 상승때문에 세계 각국의 경제가 미국을 결코 이길 수 없다는 것은 이미 기정사실화되었다. 때문에 본 도서를 읽으며 필자는 결국 장기적 관점에서 볼 때 미국주식은 좋은 투자임을 확신하게 되었다. 본 도서의 저자가 짚어주는 미국 주식이 매력적인 부가적인 이유를 아래와 같이 요약해 보았다. 좋은 투자처의 부재 : 예금이자 2% 내외 / 상가 오피스 평균 임대 수익률 연 4% 장기 수익률 우수 : (최근 5년간) 코스피 상승률 3.75% vs S&amp;P 500 상승률 35.69% 주주보호시스템 : 한국의 화이트 컬러 경제사범들에 대한 형량 미미 시차를 심리적으로 활용 : 개장시간(밤 11시 30분 ~ 새벽 6시)이 수면시간대인 관계로 본업에 구애받지 않음 환차익의 기회 가격 제한폭 없음 : 한국은 최대 30%이므로 다음날의 추가 상승분을 놓칠 우려가 있음 시간에 구애받지 않는 두고두고 볼 수 있는 책 본 도서의 매력중 하나는 기본에 충실한 투자 원칙 및 내공을 전달한다는 점이다. 이 챕터의 소제목은 저자가 머리말에서 소개한 문구로 세월이 흘러도 독자분들께 도움이 되는 지식을 담고자 노력했다고 한다. 덕분에 우리나라 주식을 기반으로 한 지식으로는 알 수 없는 미국 주식 시장의 도메인을 익힐 수 있는 기회를 얻게 되었다. 리뷰 지면에는 한계가 있으므로 책이 다루고 있는 내공을 아래와 같이 요약해보았다. 미국의 3대 증권거래소 : 뉴욕증권거래소 / 나스닥 / 아멕스 3대 주요지수 : S&amp;P 500 / 나스닥 지수 / 다우지수 성장주 vs 배당주 / 현금배당 / 주식배당 / 정기배당 / 특별배당 / 배당락일 / 배당지급일 / 배당수익률 / 연간배당금 / 배당성향 / 배당성장 국가적요소 : 심리적요소 / 세금(법인세 등) / 금리 / 경제성장률 / 고용률 / 제조업경쟁력 산업적요소 : 원자재가격 / 계절적요인 기업한정요소 : 상품(서비스) / 산업지배력 / 법적, 제도적 위험 거시경제지표 : 기준금리 / 고용률 / 국제유가 / 월간자동차판매량 / 계속실업급여청구건수 / 생산자물가지수 / 내구재수주 GICS(세상의 모든 산업을 10개 섹터로 분류한 국제기준) ETF(국산/미국산 차이점) / 인버스 / 레버리지 / 저변동성 꼭 주식과 투자를 떠나 경제상식은 물론 각종 지표가 경제의 관계에 대해 포괄적인 눈을 키울 수 있었던 점이 큰 장점이었다. 저자가 언급한 바와 같이 세월이 흘러도 변하지 않는 내공을 전달하는 책이므로, 미국 주식에 관심있는 분은 물론이거나와 재미있게 경제 지식을 학습하고 싶은 분들께 추천드리고 싶다. 책의 화룡점정 - 사업보고서 분석과 경험기반의 통찰력 사업보고서는 일종의 자기소개서와 유사하여 정량보다는 정성적 분석이 필요하나 미래의 사업방향, 자체 진단한 영업 이익의 감소 등이 언급되어 있으므로 투자에 있어 반드시 필요한 1순위 정보라 할 수 있다. 아래 그림은 미국의 사업보고서에 대한 개요이다. 영문으로 되어 낯설지만 저자가 항목별로 어떤점을 살펴보아야 하는지 사례중심으로 설명하기에 굉장히 이해하기 쉽다. 사업보고서는 미국 주식 투자를 위한 난이도 끝판왕에 해당되는데 이를 분석하는 방법을 저자의 경험으로 손쉽게 인도하기에 필자는 본 파트를 책에서 가장 가치있는 지식이 담긴장이라 말하고 싶다. 더불어 책의 곳곳에 숨어있는 경험에서 우러나온 통찰력과 노하우가 숨겨져 있어 인상깊었던 몇 구절을 아래와 같이 인용해 본다. 손익계산서 : 특정기간동안 거둔 수익과 비용을 나타낸 표 즉, 돈을 얼마나 잘 버는가? 매출 / 원가 / 매출총이익 / 관리비 / 경상비 / 영업이익 / 단기순이익 / 연구개발비 등 재무상태표 : 특정 시점에 보유한 자산, 부채, 자본 상태를 나타낸 표 즉, 현재 자산이 얼마인가? 자산 / 부채 / 자본 =&gt; 유동비율 / 부채비율 / 자사주매입을 반드시 파악 펀더멘탈(기업 현 상태를 점검할 수 있는 객관적 지표) PER / 매출성장률 / 이익성장률 / 부채비율 / 유동비율 패션 브랜드 미래주가 확인 방법 주변 여성 10명에게 그 브랜드를 좋아하는지, 구매한 적이 있는지 묻기 삽과 곡괭이 전략 비트코인의 광풍 뒤 큰돈을 번 기업은 Nvidia 미래의 유튜버인 어린이들의 선택을 받는 프리미어 프로는 파이널 컷보다 유리한 위치를 점하고 있다. 워린버핏이 죽기 전 재산 관리에게 당부할 말 “재산의 10%는 단기정부채권에, 90%는 수수료가 제일 저렴한 S&amp;P 500 인덱스 펀드에…” 마무리 &amp; 책에서 추천하는 주요사이트 URL 위에서 언급한 부분외에도 저자는 2020년 주식시장의 키워드인 스트리밍과 헬스케어에 대하여 언급한다. 특히, 헬스케어 부분은 의료보험과 관련된 현 미국의 정치 주소와도 밀접한 관련이 있어 흥미있게 읽었으며 그동안 몰랐던 국제적인 경제상식을 함양할 기회를 얻을 수 있었다. 더불어 필자가 관심있는 미국의 IT 핵심 기업을 분석하여 세부정인 시가총액, 배당수익률, 주가변화, 기업의 장단석 분석을 한눈에 파악할 수 있으며 저자가 우량 종목을 선정하는 기준을 얻을 수 있다는 장점이 있다. 즉, 주가는 언제든 변화할 수 있는 부분이므로 종목 하나하나에 연연하기 보다 종목을 분석하는 안목을 키우는 것이 중요하다고 생각했다. 책의 후반부에는 미국 시장의 숨겨진 보석들이라는 타이틀로 저평가된 우량주를 소개하고 있으며, 대표 ETF 라이브러리를 섹터별로 분석하는데 이 파트에서 미국 산업구조의 전반을 이해하는데 있어 큰 도움이 되었다. 부록에서는 주식거래의 구체적인 방법과 아래 그림과 같이 세금을 줄이는 꿀팁을 공유하면서 책을 마무리한다. 끝으로 책의 저자가 추천하는 유용한 사이트를 정리해보며 본 도서의 내용 리뷰를 마친다. 디비던드닷컴 : 미국 배당주, 배당세부지표 제공 SEC : 미국기업의 사업보고서 제공(또는 각종 미국기업의 IR 페이지) FRED : 미국 경제지표 데이터베이스 잭스 : 스탯분석 시킹알파 : 스토리분석 피어인사이트 : 가트너 운영 IT기업 리뷰 인베스팅(Investing) : 스마트폰 앱. 한글지원, 미국 주식의 가격을 실시간 무료 제공 책소개 Link 저자의 탈잉 강의 누가 읽어야 하는가? 미국의 국가, 산업, 기업 측면에서의 경제 지식을 재밌게 정리하고 싶은 분. 미국 주식 예비 투자자. 주식과 투자의 전반적 원리에 대한 이해가 필요한 분. 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 미국 주식 시장의 개요 (1 ~ 3장) 미국 주식 투자이유 미국 주식시장의 기초 미국 ETF의 종류 2. 가치투자와 나만의 투자전략 (4 ~ 6장) 나만의 포트폴리오 및 투자전략 세우기 재무제표 &amp; 사업보고서 읽는 법 투자에 필요한 도구 활용법 3. 미국시장 분석 및 실전(7 ~ 10장, 부록) 스트리밍 &amp; 헬스케어 미국 핵심 IT기업 리뷰 미국 주요 ETF 라이브러리 숨겨진 보석들 부록 : 거래절차, 세금총정리, 투자참고 사이트 목록 등 요약하며… 세계 경제위기 및 미-중 무역 분쟁, IMF 등 각종 경제 위기는 슬기롭게 대처할 경우 되려 기회가 되기도 한다. 전화위복의 기회가 될 수 있는 한가지 방법이 바로 달러 혹은 미국 주식에 대한 투자라고 생각한다. 수익 창출과 부도 좋지만 현재의 자산가치를 안전하게 보존하기 위한 수단으로서도 의미가 있을것이다. 시중에 미국 주식을 다루는 책들은 다양하게 나와 있지만 현 시장의 트렌드를 다루는 도서나 개인의 경험을 바탕으로 구체적인 시행착오를 진솔하게 다루는 책은 흔치 않다. 저자의 좌충우돌 시행착오를 일목요연하고 알기 쉽게 전달한 점이 이 책의 매력이라고 생각한다. 머리말에서 저자가 언급한 바와 같이 세월이 흘러도 두고두고 볼 수 있는 책을 목표로 만들어진지라 경제 지식에 대한 체계적인 정리가 일품이다. 다른 경제학 도서가 이론에 충실한 나머지 가독성을 잃곤 하는데, 본 도서는 현실에서 벌어지고 맞닥드리는 사례 및 현상을 위주로 경제를 설명하고 있어 빠른 이해가 가능하다. 특히 가치 투자를 위한 사업보고서 분석은 이 책의 백미이며 그 외에도 현 미국 시장의 기업분석 정보 및 구체적인 투자방법, 세금문제에 대한 Tip을 다룸으로써 실전적인 투자 가이드를 제시해준다. 약간의 아쉬움이 있다면 미국 주식을 최초로 투자하는 독자들에게 어떤 사이트에 접속하여 어떤 버튼을 누르는 등의 상세하고 구체적인 따라하기 코너가 없다는 것이다. 미국 주식은 커녕 한국 주식시장도 경험하지 못한 독자분들께는 다소 난이도가 느껴질 수 있는 부분이나, 긴 세월의 관점에서 보면 그런 파트는 한번 익히면 다시 볼 필요없는 일회성 지식이기에 저자가 일부러 제외시키지 않았을까 추측해본다. 미국 주식 초보자, 각종 경제 상황 및 거시 지표하에 주식에 대한 가치 투자를 하고 싶은 투자자라면 꼭 일독을 권한다. &lt;비제이퍼블릭 출판사&gt; 한 분야의 실전적 절대고수가 그간 쌓아온 지식과 경험을 바탕으로 내공의 정수를 담은 장인의 향기가 나는 책을 출판하는 회사입니다. 참신한 해외 번역서, 최신 트렌트를 겨냥하는 양질의 서적을 자주 출간하여 필자의 책장에도 어느덧 십여권의 책이 꼽혀있네요. 특히, IT 기술을 향상시키는데 있어 제자리 걸음이거나 시간투자 대비 내공이 채워지지 않는 공허함이 생긴다면 비제이퍼블릭 출판사의 책 목록을 한번 살펴보시기 바랍니다. 비제이퍼블릭 바로가기",
    "tags": "review book us stock starter",
    "url": "/review/2019/12/27/review-book-us-stock/"
  },{
    "title": "[리뷰] Do it! 지옥에서 온 문서관리자 깃&amp;깃허브 입문",
    "text": "개요 본 리뷰는 이지스퍼블리싱 출판사 \"Do it! 지옥에서 온 문서관리자 깃&amp;깃허브 입문(이고잉,고경희 저)\"을 읽고 지식을 정리한 글입니다. 목차 Git은 뭐고? GitHub는 또 뭐여? Git과 친해지기 어려운 이유 이 책을 한마디로 표현하면 뚫어뻥 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… Git은 뭐고? GitHub는 또 뭐여? 아래와 같은 곤란한 상황… 당신도 분명 겪어보셨을 것이다. “당신이 무엇을 상상하든 이 파일은 최종파일이 아니다. 설사 최종파일이 나왔다 치자. 다음은 또 무슨일이 기다리고 있을까? 기껏 회사에서 수정했건만.. 집에서 고치려면 USB로 복사해야 하나? 열심히 수정했는데 다른 사람이 손데면서 다 날라갔다! 보기만 해도 짜증나는 이 문제들을 Git은 지옥이라고 표현한다. 그렇다면 해결책은 무엇인가? 책 제목에서도 알 수 있듯이 바로 지옥에서 온 문서관리자 깃&amp;깃허브이다. 쉽게 정리하자면 Git은 분산(여러명이 수정할 수 있다.)버전(최최..종을 알아서 관리해준다.)관리시스템이며, GitHub는 Git으로 생산된 산출물이 저장되는 Git저장소라고 할 수 있겠다. 실용적인 관점에서 접근하자면 지역저장소를 관리하기 위한 도구가 Git이며, 원격저장소의 집합체가 GitHub이다. 위 문제들 때문에 골치가 아프셨다면, 그리고 반드시 해결하고 싶다면 Git&amp;GitHub는 필수다! Git과 친해지기 어려운 이유 위에서 설명한 지옥을 피하기 위해 반드시 Git&amp;GitHub을 사용하겠다는 다짐은 누구나 할 수 있지만, Git&amp;GitHub과 그 친분을 이어나가는 것은 아무나 할 수 없다. 그 이유로 필자가 경험했던 내용을 위주로 아래와 같이 요약해보았다. Git Bash vs Git GUI Git을 사용하는 방법은 크게 2가지가 있다. Bash는 커맨드라인 모드로 텍스트 기반의 명령어를 한줄식 타이핑하면서 Git을 사용하는 방법이고, GUI는 화면을 마우스로 제어하여 Git을 사용하는 방법이다. 처음에는 물론 GUI가 편하다. 다만 갈수록 복잡해지는 기능을 숙달하기에는 직관적이지 않고 사용하기 어려워진다. 태생이 리눅스 버전을 관리하기 위한 용도로 개발되었기 때문에 Bash 모드에 적응하는 것이 Git에 친숙해질 수 있는 길이다. 다만 그렇게 커맨드 라인으로 타이핑하는 부분 때문에 초반에 낯설고 어려운 진입장벽이 되기도 한다. 리눅스에 대한 사전지식의 필요성 앞서 설명한 바와 같이 Git Bash는 커맨드라인 텍스트 기반의 리눅스 명령어 방식으로 동작한다. 때문에 기본적인 리눅스 명령어에 숙달되지 않으면 버전관리가 어려워지며 다양하고 편리한 기능을 활용할 수 없다. Add - Commit - Push 매커니즘 그동안의 문서 편집기는 저장버튼만 클릭할 수 있으면 되었다. 간혹 되돌리고 싶은 경우 Ctrl+z 단축키를 이용하곤 했지만 겨우 커맨드는 2종류에 지나지 않았다. 그런데 저장만 하면 됐지 Add - Commit - Push 3종 세트는 귀찮고 어렵게 왜 계속 해야하는지 도통 이해가 안된다. 브랜치는 또 뭐냐? 나뭇가지냐? 갑자기 난데없이 나뭇가지가 나온다. 뭔가 들여다보니 수정내용을 커밋한 일련의 모음을 의미하는데 이게 나뭇가지처럼 갈라지기도 하고, 심지어는 합쳐지기도 한다. 대략 여기서부터 멘붕이온다.. 깃허브와 원격저장소 지역저장소 하나도 간신히 뗐는데, 이젠 지역저장소를 원격저장소와 연결하란다. 그것도 모자라서 원격저장소에 Push할 때마다 무슨 인증을 계속 하라는데 싫으면 SSH를 쓰란다. 리눅스도 몰라서 해메이는데 SSH는 또 뭐냐.. 이젠 보안도 알아야 하나? 회사 PC와 집 노트북에서도 동일하게 작업할 수 있다던데.. 그러려고 Git을 쓴건데 생각보다 어렵다. 매일 같은 컴퓨터에서 Commit이나 할 줄 알았지.. Git Pull도 다른 PC에선 필수라는 것을이 파악하는데 제법 오랜시간이 걸린다. 거기까진 좋은데 하다보면 파일이 서로 꼬여 Conflict라는 놈이 발생하는데.. 이거 해결이 골때리게 어렵다. 다 알고나니 이젠 남들과 동기화하란다. OTL 날이 갈수록 복잡해진다. 팀원들 각자 별도의 브랜치를 만들기도 하고 승인이 끝나면 팀장의 master 브랜치에 병합도 한다. 기껏 병합했는데 일정 시점이 지나서 또 분기를 한다. 그렇게 분기와 병합을 밥먹듯이 한다. 근데 그 과정에서의 충돌 그리고 심지어 되돌리기도 한다. 괴로워 보이는 일련의 과정이지만 돌이켜보면 생각보다 어렵지 않은 작업이었다. 그만큼 Git이 상당히 직관적으로 개발되었기 때문이다. 문제는 얼마나 쉽고 빠르게 익히느냐 정도인데 현재 필자가 아는 최고 좋은 솔루션은 단연 현재 소개하고 있는 본 도서이다. 이 책을 한마디로 표현하면 뚫어뻥 앞의 챕터에서 설명했던 Git과 친해지기 어려웠던 일련의 시나리오들은 이 책에서 매우 쉽고 간단한 예제를 통해 속시원하게 뚫어준다. 이미 Git의 대부분을 알아버린 이 시점에 이렇게 쉽고 재미있는 책을 늦게 만난것이 야속하지만, 그래도 그간의 Git 사용법을 돌이키고 간혹 몰랐던 Tip을 익힐 수도 있었다. 단연컨데 이 책은 Git을 가장 빨리 익힐 수 있는 방법이다. 이 책의 백미는 초보자들이 겪는 시행착오와 어려움을 일종의 타임라인 순으로 책을 구성하였다는 점이다. 특정 챕터를 익히고 Git을 사용하다보면 궁금점이나 한계에 봉착하는데 대부분의 문제점은 바로 다음장을 펴면 해답이 있을 정도로 놀랍게도 그 흐름이 일치한다. 아래 그림부터 배움의 순서를 중요시 하는 저자의 조언이 등장한다. Git 명령어별로 복잡한 상태에 대한 개념을 명확하게 해주는 점도 이 책의 장점이라고 할 수 있다. 아래 그림은 git status 명령어의 수행 결과에 따라 파일 상태가 어떤 단계에 속하는지 전체 개념을 정리해주는 도식도이다. 집요할 정도로 쉽고 상세한 설명은 마치 독자에게 아무리 무식해도 절대 이 책을 덮지 못하게 만들어 주마.라고 호언장담하는 느낌이다. 그렇게 설명했는데도 어려워할까 두려웠던지 그림의 우측 상단 도움말이 어려운 내용을 반복하여 설명해준다. 여기서 끝이 아니고 확인사살이 또 있다. 각 장을 마치고 나면 손글씨를 쓰게 한다. 마치 아마존의 CEO 베조스가 보고 혹은 회의 석상에서 전자문서가 아닌 손으로 쓴 글씨를 요구하는 느낌이라고나 할까? 실제로 손글씨가 오랜 기억력을 향상시킨다는 연구결과가 있다. 그래서인지 그림과 같은 통과의례를 마치고 나면 며칠 뒤에도 기억이 생생하다. 위 챕터에서 언급했던 Git과 친해지기 어려운 이유 중 하나인 충돌난 문서를 해결하는 부분이다. 대부분의 책 또는 블로그에서 대충 말로만 표현해서 어떻게 해결해야 하는지 막막했었던 경험이 있다. 그런데 이 책은 다르다! 필요한 부분만 정확하게 붉은색 박스로 하이라이트 했고, 유치원생에게 설명한다는 느낌을 받을 정도로 최대한 구체적으로 설명한다. 마찬가지로 회사 컴퓨터에서 내려받아 작업하는 부분이다. git pull이라는 명령어의 핵심 개념을 짚고, 실제 적용한 화면을 통해 실습과 비교할 수 있게 해주고, 확인사살로 도식화를 통해 개념을 정립하게 해준다. 혼히 풀리, 풀리퀘라고 말하는 pull request에 대한 설명이다. 오픈 소스에서 컨트리뷰터로 활동하기 위한 필수 과정이다. 사실 개인적으로 풀리퀘를 할 수 있다면 Git 관한 한 중수 ~ 고수는 되지 않을까 생각이 든다. 이 명령어는 책의 후반부에 설명이 되어있는데 역시 독자 시행착오의 타임라인을 철저히 고수하여 밀착관계를 유지한다. 마지막으로 필자가 지금 운영중인 블로그-theorydb와 같은 Github Pages를 활용한 개인 블로그 만들기, Visual Studio Code와 Git의 연동 등 일상생활에 유익한 Tip을 소개하며 책이 마무리된다. 쓸데없는 군더더기 모두 제거하고 핵심만을 밀착 지도하는 최고의 Git 입문서로, Git을 정복하기에 많은 난관에 부딪혀 포기한 분이 계시다면 이번 기회에 100% 승리하시길 권한다. 책소개 Link Do it!공부단 상시모집 중 누가 읽어야 하는가? Git&amp;GitHub을 사용하고자 하는 모든 분. Git을 정복하기에 많은 난관에 부딪혀 포기한 분. 사내 깃헙을 도입하여 새롭게 Git을 익혀야 하는 IT 개발자 등. 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 깃에서 브랜치까지 (1 ~ 3장) 깃 &amp; 설치 &amp; 리눅스 익히기 저장소 생성, 버전 만들기, 커밋, 파일상태 확인, 되돌리기 브랜치, 정보확인, 병합, 관리 2. 깃허브(4 ~ 5장) 원격저장소, 깃허브, 지역저장소와의 연결, 원격저장소 pull&amp;push, SSH 여러컴퓨터에서의 동기화 협업에서 브랜치 활용 3. 기타 Tip(6장, 꿀팁) 프로필관리, Readme, 오픈소스 프로젝트 참여하기 Github Pages를 활용한 개인 블로그 구축 Visual Studio Code와 Git 연동 등 요약하며… 위에서 언급한 바와 같이 이 책을 한마디로 표현하자면 Git&amp;GitHub를 익히기 위한 최고의 솔루션이다. 본 도서의 구성이 독자 시행착오의 타임라인 순서를 철저히 고수하며 독자와의 밀착관계를 형성하기 때문이다. 마치 선생님이 옆에 있는 것처럼 궁금한 질문에 대한 답이 바로바로 나와주니 흥미와 몰입도가 저절로 향상된다. 이런 구성이 신기하여 알아보니 저자분이 가독성 좋고 전달력이 뛰어난 책을 집필하시기로 유명한 분이시더라. Git을 익히기에 군더더기는 모두 제거하고 핵심만을 확인 사살하며 뼈 속 깊이 익힐 수 있게 만든 몇가지의 장치가 있다. 굳이 이런 구성까지 눈치채지 못하더라도 은 엄청 빨리 읽혀지는데 내용은 쉬이 잊혀지지 않는 의아한 경험을 하게 될 것이다. 책의 단점은 딱히 찾을 길 없고 입문서를 넘어선 중고급자를 위한 심화 후속편이 나왔으면 하는 바램이다. Git&amp;GitHub이 어려워 손들고 포기한 분이 계시다면, 속는셈 치고 본 도서로 다시 도전하시길 권유드린다. 100% 성공을 보장한다. &lt;이지스퍼블리싱 출판사&gt; 출판사 이름에서도 알 수 있듯이 초보자도 읽기쉬운(Easy) 서적을 출판하는 회사입니다. IT개발자라면 누구나 한번쯤은 접했을 Do it! 시리즈는 따라하기만 해도 걸작을 만들 수 있기로 정평이 나 있습니다. 온갖 서적을 접해도 해결하지 못했다면 마지막으로 이지스퍼블리싱 도서를 읽어보세요. 그동안 이해하지 못했던 기술과 개념 조각의 퍼즐이 완성되는 것을 느끼실 수 있습니다. Do it! 스터디 룸 카페에 가입하여 공부단으로 활동하시면 책 1권을 선물로 받을 수 있습니다. 커피 몇잔의 가격으로 책 한권이 주는 다양한 실속 가치! 이지스퍼블리싱의 매력입니다. 이지스퍼블리싱 바로가기",
    "tags": "review book git github introduction",
    "url": "/review/2019/12/26/review-book-git-github/"
  },{
    "title": "[리뷰] 미술관에 GAN 딥러닝 실전 프로젝트",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"미술관에 GAN 딥러닝 실전 프로젝트(데이비드 포스터 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 생성 모델링이란? 이해를 돕는 사례들 딥러닝 숲에서 VAE를 거쳐 GAN까지 이 책의 백미 : 그리기, 글쓰기, 작곡하기, 게임하기 이제 겨우 RNN, CNN 좀 알게 되었는데 어텐션은 또 뭐야? 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 생성 모델링이란? 판별 모델링에만 익숙했던 대부분의 머신러닝 엔지니어에게 생성 모델링은 처음 접하는 순간부터 직관적으로 와 닿지는 않는생소한 개념이다. 확률과 통계는 이제 다 배웠다고 안심하고 책을 덮는 순간부터 다른 악마로 나타나 처음부터 다시 공부해야 한다더니만… 생성 모델링이 어렵게 느껴지는 이유 중 하나도 확률적 랜덤요소를 활용하기 때문이다. 조건부확률 - 베이즈정리 - 나이브베이즈라는 그 흔한 테크트리가 다시 등장한다. 본 도서에서는 아래와 같이 판별모델링과의 비교를 통해 생성 모델링에 대한 이해도를 높혀준다. 판별 모델링 : p(y|x) 샘플 x가 레이블(카테고리) y일 확률을 추정. 예 : 추천시스템, 이메일의 긍정/부정 추정, 사진을 통한 녹내장 발생확률 추정 등 생성 모델링 : p(x|y) 레이블(카테고리) y를 통해 샘플x일 확률을 추정. 즉, 원본 데이터셋에 속할 가능성이 높은 픽셀을 출력하는 모델링. 확률적 랜덤요소를 고려하여 샘플링한 데이터셋을 생성하는 방법. 예 : GAN 생성 모델링이 의미가 있는 이유는 판별 모델링이 해낼 수 없는 AI 특성을 가지고 있기 때문이다. 강화학습의 시행착오, 사람의 인지능력을 모방하는데 핵심적인 도구로 활용된다. 그렇다면 도대체 어떻게 생성한다는 것인가? 본 도서에서는 아래 그림과 같이 간단한 생성 모델링 프레임워크 예제를 통해 P model에서 포인트 A, B, C를 생성하는 방법과 성공여부를 측정하는 방식을 알려준다. 더불어 핵심 개념을 파악하기 용이하게 끔 표본공간, 확률밀도함수, 모수모델, 가능도, 최대가능도추정 등 핵심 확률 개념을 다시금 짚어준다. 이해를 돕는 사례들 이 책이 가지는 흥미로운 부분은 각 장별로 이해를 돕기위해 일상생활과 유사한 사례를 예로 든다는 점이다. 예를 들면 위에서 설명한 생성 모델링을 이해시키기 위해 아래 그림과 같은 로들행성과 픽셀행성에서의 새로운 패션 스타일을 창조하기 위한 방법을 마치 소설과 같이 설명해준다. 소설은 수학적 지식이 전무한 분들이나 문과출신 분들도 이해하는데 무리가 없기에 이해되지 않는 수식이나 용어들과 싸우는 대신 개념과 활용 용도를 파악하는데 있어 상당히 효과적인 방법이라고 생각한다. GAN을 접하는데 있어 이렇게 쉬운 방법으로 접근하는 책은 처음인 것 같다. 먼저 로들행성에서 위 그림과 같은 50개의 패션 샘플을 바탕으로 고수준 특성(머리모양, 색깔, 안경여부, 옷 등)만을 바탕으로 가법 평활화 등을 활용하여 새로운 패션을 생성하는 방법을 예로 든다. 반면 픽셀행성 예제에서는 고수준 특성을 포함하여 저수준 특성(픽셀의 가로 x 세로 갯수, RGB)까지 고려하여 표현학습과 비선형 매니폴드의 개념을 학습한다. 로들행성이 일반적인 데이터사이언스의 관점에서 접근한다면, 픽셀행성은 보다 심화된 딥러닝의 시각에서 접근하는데 두 예제만큼 딥러닝에서 GAN으로 매끄럽게 설명을 이어나가는 예제는 찾기 힘든것 같다. 상세히 정리하고 싶지만 리뷰 취지에는 벗어나기에 이 즈음에서 마무리하며 더 궁금하신 분들은 책을 참고하시기 바란다. 딥러닝 숲에서 VAE를 거쳐 GAN까지 딥러닝을 학습하다보면 자칫 활성화함수, 배치 정규화, 드롭아웃, 경사하강법, 오차역전파법 등 딥러닝을 이루는 세부 요소와 원리에 깊이 빠져들어 딥러닝의 숲을 이해하지 못하고 주화입마에 빠지는 경우가 종종 발생한다. 나아가 월드모델과 같은 최신 논문 혹은 기술이 등장하면 현재 학습하고 있는 딥러닝 좌표를 잃어버리고 헤매이게 된다. 무수히 많은 수식과 기본개념, 학습속도로 도저히 쫓아갈 수 없는 쏟아지는 최신 논문과 연구결과 속에서 본 도서는 현재 학습하고 있는 전체를 아우를 수 있는 큰 그림으로 학습중인 내용이 어느 위치에 해당하는지 명확한 좌표를 제시해준다. 2 ~ 4장에서 소개되는 지식들은 GAN을 구현하기 위해 반드시 알아야 할 필수 개념들이다. 먼저 GAN에서 활용되는 딥러닝의 핵심 기술들과 개념들을 자세히 알아본다. 아래 그림은 간단한 예제실습을 통해 알아본 CNN에 관한 설명이다. 더불어 딥러닝의 필수 개념중 하나인 오토인코더(AE)를 비롯 그 한계를 뛰어넘기위해 사용된 변이형 오토인코더(VAE)에 대해 학습한다. 아래 그림은 VAE에 대한 소개 중 일부이다. VAE까지 학습하고나면 생성모델링에 대해 제법 가시적인 개념이 형성된다. GAN의 핵심개념부터 비교적 최신 연구결과인 WGAN-GP까지 정리하면 비로소 GAN의 기술들에 대한 큰 그림이 보이기 시작한다. 이렇듯 책의 구성이 전반적으로 Top-Down의 구조를 체계적으로 그려준다. 현존하는 최신기술들을 한눈에 정리해줌으로써 앞으로의 방향을 잃지 않게 해준다. 방향을 잃지않게 큰 그림과 개념을 지속적으로 잡아준다는 점. 바로 이 점이 본 도서의 뛰어난 매력중 하나이다. 이 책의 백미 : 그리기, 글쓰기, 작곡하기, 게임하기 이로써 딥러닝부터 GAN에 이르까지 핵심개념과 기초를 탄탄히 학습하였다. 이제 이 책의 백미인 주옥같은 감성넘치는 예제들을 실습하며 GAN의 실체를 가시적으로 살펴볼 단계이다. 그리기 역시나 초반부는 이해를 돕기위한 사례 소설이 또 등장한다. 사과를 파는 그래니 스미스와 오렌지를 파는 플로리다의 경쟁을 통해 CycleGAN의 핵심원리를 설명한다. CycleGAN을 통해 마치 고흐와 같은 아티스트가 시진의 풍경을 보고 그린듯한 결과를 생성할 수 있다. 더불어 뉴럴 스타일 트랜스퍼 기법을 통해 특정이미지의 스타일을 원본 이미지로 옮기는 방법을 구현한다. CycleGAN이 특정 화가의 기풍을 학습한 결과라면, 뉴럴 스타일 트랜스퍼의 경우 특정 작품을 학습한다고 비유해야 할까? 쓰기 한 교도관이 죄수들을 이용하여 자신의 단편소설을 쓰게 하는 흥미로운 예제로 출발한다. LSTM, GRU 등의 순환층을 활용하여 특정 글의 스타일을 흉내낸 텍스트 시퀀스를 생성하는 방법에 대해 배운다. 작곡하기 바흐의 첼로모음곡을 활용하여 옥타브, 키와 같은 개념을 학습한다. 어텐션 기법을 활용하여 적층 LSTM, MuseGAN의 기술을 실습한다. 게임하기 오픈AI Gym 라이브러리를 활용하여 자동차 경주를 학습한다. 아래 그림과 같은 월드모델을 활용하는데 VAE 생성모델에 강화학습을 접목한 방식이다. 개인적으로는 이 실습이 가장 흥미로웠고 GAN과 Reinforcement Learning을 동시에 학습할 수 있어서 가장 배운것이 많은 챕터였다. 생성모델을 의사환경으로 사용하여 전략 정책을 반복하는 기법인데 예전에 알파고와 유사한 것을 만들다가 어려운 난이도에 봉착했던 정책망에 대한 해결책을 얻을 수 있어서 뿌듯했다. 이제 겨우 RNN, CNN 좀 알게 되었는데 어텐션은 또 뭐야? 오래 전도 아닌 고작 2019년 올해의 일이다. NLP, Computing Vision을 학습하며 이제 좀 CNN, LSTM, RNN 기법도 이해가 가고 딥러닝이 실용적인 측면에서 어떻게 활용되는지 알만해질 때 즈음 BERT가 등장했다. 아니 정확히는 어텐션 기반의 기법들이 등장하기 시작했다. 실제로는 더 이전에 등장했겠지만 적어도 필자의 레이더망에는 그 즈음에 포착되었다. 문제는 기껏 이전 기술들을 이제는 좀 알것 같아 실무에 적용 좀 해보려는데 BERT를 접하니 이전 기술들을 쓸 일이 없어졌다는 것이다. 아예 모르는것보다는 낫고 개념적으로도 도움이 되는것도 사실이지만 엔지니어링 측면에서는 좀 황당할 정도로 기존 레거시들을 활용할 일이 별로 없었다. 물론 다시 배우면 된다. 그런데 이젠 수없이 쏟아지는 논문과 연구결과 속에 도대체 지금 어떤 기술을 눈여겨봐야 하는건지 어떤 기술을 실무에 적용해야 하는건지… 미로속에 빠진 느낌이었다. 살면서 요즘처럼 열심히 공부를 한 적도 없는것 같은데 겨우 최신 기술의 산더미 속에 단어조차 생소한 기법이 이렇게 많다니.. 기술의 부분 부분들은 요즘 잘 활성화된 페이스북 등의 커뮤니티, 오픈채팅방 또는 외국 커뮤니티나 Quora 등의 활동을 통해 물어보면 대부분 친절하게 답을 해준다. 그런데 거시적인 숲과 현재 내가 학습하는 기술의 위치를 알고 싶다면? 그건 정말 어렵다. 다들 필자 못지 않게 열심히 공부중이시라 시간들도 부족하시고 무엇보다 필자의 학습수준을 파악하여 필자의 현재위치를 알려줄만큼 한가한 분은 없다. 학습자의 입장에서 무례한 질문이라는 생각도 든다. 본 도서에서 개인적으로 가장 마음에 들었던 부분이 후반부 생성 모델링의 역사를 일목요연하게 정리해준 부분이다. 다뤄본적이 없던 GPT-2, MuseNet 등이 어텐션 기반의 모듈이라는 것도 새롭게 알 수 있었다. BERT 하나만 들이파도 시간과 정력이 부족한 나날이기에 다른것에 집중하기는 쉽지 않음을 인정하지만 적어도 근래에 핫한 최신기술이 어떤것이 등장하고 있고 그게 대충 뭔지는 알아야 내 기술의 현위치를 알 수 있고 이런 노력이 가치가 있는것인지 판단할 수 있을 것이다. 이런 측면에서도 이 책은 큰 가치를 발휘한다. 정리하며 본 도서와 관련된 정보를 아래 링크로 남긴다. 책소개 GitHub 소스코드 누가 읽어야 하는가? GAN 및 강화학습(Reinforcement Learning)에 관심이 많은 분 AI관련 최신 논문, 연구, 기술을 숲의안목으로 체계적으로 정리하고 싶은 분 그 외 AI, 딥러닝 등에 관심이 많은 연구자, 프로그래머 등 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 생성 모델링, 딥러닝, VAE, GAN (Part1) 생성 모델링과 확률의 개념, 복잡도를 해결하기 위한 딥러닝의 활용 생성모델과 관련된 개념에 특화된 딥러닝의 전반적인 지식 재정립 오토인코더의 한계점 분석, 변이형 오토인코더를 통한 무작위성 주입 및 잠재공간 분포방식 제한 기법 등 애니멀 GAN 예제(생성자가 판별자를 속이기 위한 순환 방법)부터 WGAN-GP까지 GAN 기법 총정리 2. GAN을 활용한 실습예제(Part2) 그리기, 쓰기, 작곡하기, 게임하기 실습 GAN외에도 강화학습(Reinforcement Learning), MuseGAN, 적층LSTM, RNN, CNN 등 딥러닝의 핵심기법을 거의 모두 배울 수 있다. 3. 생성모델링의 미래(9~10장) 2019년 최신기술까지 생성 모델링의 발전과 관련된 핵심 기술, 연구를 한눈에 살펴본다. 결론(10장)에서는 GAN을 넘어선 AI에 대한 저자의 통찰력이 담긴 고민도 엿볼 수 있다. 요약하며… 2019년 한해가 어느덧 저물어간다. 2019년 한해동안 GAN, Reinforcement Learning으로 대표되는 AI기술들과 연구 결과는 끊임없이 쏟아졌고 축적되는 지식의 Volume은 점점 가속화되고 있다. BERT와 같은 기술 하나에 집중하기도 어려운데 정신을 차리고 주위를 둘러보면 MuseNet이니 GPT-2이니 주옥같은 기술들이 이미 등장하여 상용화 되어간다. 넘쳐나는 기술의 축제이자 축복이라는 향연을 즐기며 행복한 반면 대부분 필자와 같은 평범한 사람들에게는 조급함이 느껴질 수도 있다. 그만큼 학습해야 할 과제들은 많아지고 자칫 항해의 좌표를 잃고 방황하기도 쉬워진다. 불과 몇달 전부터 열심히 파고들던 기술이 현 시점에는 큰 쓸모가 없어지는 절망감에 빠질수도 있다. 논문을 처음쓸 때 보통 연구주제의 관련 지식들을 집대성한다. 지구상에 등장한 모든 관련 기술들을 집대성 하기 전 일종의 목차와 같은 기능을 담당하는 소중한 자료가 Review 논문인데, 이 책을 한마디로 표현하자면 딥러닝 기술의 Review 논문이라 말하고 싶다. 쏟아져나오는 최신기술들을 숲의 안목으로 일목요연하게 정리하며, 자칫 원론과 개념에 치중하여 좌표를 잃지 않도록 감성충만한 주옥같은 예제들을 통해 빠른 시간내에 딥러닝의 현 주소를 제시해준다. 딥러닝과 최신 기술들의 원론을 가시적으로 이끌어내는 점이 이 책의 별미이다. 때문에 딥러닝 관련 연구자, 엔지니어는 물론 사업가, 기획자, 관련 사업의 대표자들이 빠른시간 내에 딥러닝의 모든 기술을 아우르고 싶다면 이 책은 필수도서이다. 판별 모델링의 거의 모든 역사가 담긴 패턴인식과 머신러닝(비숍 저)나 NIPS 학회에서 GAN을 처음 선보인 이안 굿펠로 님의 심층학습과 같은 두말할 나위 없는 최고의 기본서들도 중요하지만, 쏟아지는 기술의 폭풍속에 현 주소를 놓치지 않을 수 있는 GPS와 같은 책도 제너럴리스트를 위해 반드시 필요한 책이다. 예쁜 색상과 재질로 디자인되어 들고만 다녀도 감성 충만이다. 디자인 못지않은 감수성 있는 예제들을 구현해 보고 저변에 깊이 축적된 딥러닝 최신기술들의 향연을 즐기다보면 어느덧 인생의 목적이 어렴풋이 비춰질 정도로 뿌듯해지는 지금의 시간들에 감사하게 된다. AI와 관련된 분이라면 커피한잔 즐기시며 이 책이 전해주는 행복을 느껴보시길 바란다. &lt;한빛미디어 출판사&gt; 믿고보는 “한빛미디어 출판사”. IT분야에서 독보적인 양질의 도서를 출판하는 회사입니다. “나는 프로그래머다” 팟캐스트 후원, DevGround2019 행사, 리뷰어 모집, 다양한 학습 지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT분야에 관심 있으시다면 한빛미디어의 책으로 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review book gan vae deep learning reinforcement",
    "url": "/review/2019/12/15/review-book-GAN/"
  },{
    "title": "[리뷰] 파이썬에 참 좋은 PyCharm",
    "text": "개요 본 리뷰는 비제이퍼블릭 출판사 \"파이썬에 참 좋은 PyCharm(테리엇 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 제트브레인(Jetbrains)과 파이참(PyCharm) Pycharm 체계적인 기능소개 기본서보다 더 뛰어난 (실용) 기본서 다시 고급기능으로… 화룡정점 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 제트브레인(Jetbrains)과 파이참(PyCharm) 프로그래머라면 대부분 파이참, 웹스톰, 인텔리제이, 데이터그립과 같은 IDE툴을 한번쯤은 다뤘거나 최소한 들어보기라도 했을 것이다. 이 툴들에게 공통점이 있다면 제트브레인. 즉, 제조사가 동일하다는 것이다. 필자는 프로그램 언어 중 Java를 가장많이 다뤄봤는데 그때 이 회사의 존재를 알게 되었다. 인텔리제이라는 매력적인 툴을 다뤘기 때문이다. 이클립스도 JAVA를 다루기에 훌륭한 도구이지만, 인텔리제이의 강력한 기능에 빠져든 사람은 쉽게 이클립스로 돌아오기 어렵다. 개인차는 있겠지만 개발자 커뮤니티 또는 주위의 개발자 동료들은 생산성이 2~5배 정도는 되는것 같다고 칭찬일색이다. 그동안 다뤘던 가장 매력적인 IDE Tool을 꼽으라면 Visual Studio를 주저앉고 꼽았었는데 제트브레인 社 제품을 사용하면서 순위가 바뀌게 되었다. 장인정신의 냄새가 가득담긴 기가막힌 IDE툴들을 개발하는 멋진 회사이다. 아쉬운 점이 있다면 무료인 Community 버전엔 다소 기능제약이 있어 사용자 층이 두텁지 않다는 점과 그로인해 한글화가 부족하여 참조할만한 레퍼런스가 많지 않다는 점이다. 이렇게 훌륭한 툴에도 Trade-Off는 존재하는 씁쓸한 현실. 그 와중에 지금 리뷰하는 “파이썬에 참 좋은 PyCharm” 책을 만나고 얼마나 반가웠는지 모른다. 인텔리제이는 나름 역사가 깊어 좋은 레퍼런스도 많았지만 Pycharm은 상대적으로 역사가 짧아서인지 좋은 한글 레퍼런스를 찾기가 어렵다. 마침 저자 테리엇님이 먼저 사용하신 좋은 팁들을 체계적으로 구성해 전달하신 덕에 이번 기회에 파이참의 활용 능력을 더욱 높일수 있었다. Python을 다루는 분들이라면 누구나 IDE의 아쉬움에 대해 고민이 많으셨을텐데 이 책을 계기로 Pycharm의 세계에 푹 빠져보시는 것은 어떠실런지.. Pycharm 체계적인 기능소개 본 도서에서는 파이참의 기능을 난이도에 따라 체계적으로 구성하였다. 먼저 파이썬을 설치하는 방법에 대해 학습한 후 아래 그림과 같이 파이참을 설치하는 방법을 자세히 다룬다. 설치단계에서부터 주저하는 독자가 없도록 설치과정은 자세하고 친절하게 안내되어있다. 설치가 완료되면 아래 그림과 같이 파이참의 IDE의 UI 레이아웃, 컴포넌트별 구성요소를 자세히 설명한다. 이어서 자주 활용하는 기능 또는 추천할만한 설정을 통해 파이참에 친숙해지도록 잘 유도한다. 아래 그림은 파이썬의 가상환경을 설정하는 방법으로 자주 활용하는 기능 중 하나의 예시이다. 이렇게 파이참에 친숙해진 후 파이썬의 기본 맛보기에 들어간다. 파이썬의 기초조차 모르는 분들이 파이참에 익숙해지는 것은 조금 두려운 일일 수 있다. 이렇게 단계별로 쉽게 다음 심화단계에 적응할 수 있도록 선수지식을 짜임새 있게 배치한 점이 이 책의 매력이다. 어렵지 않은 입출력, 함수, 자료형, 조건문, 반복문, 주석, 예외처리, 패키지 등을 간단한 예제를 작성하며 파이썬을 알려준다. 단순히 파이썬만 알려주는 것이 아니다. 매번 파이참 도구를 이용해서 예제를 개발하며 그때마다 생산성을 높일 수 있는 파이참의 기능 혹은 단축키를 설명해준다. 파이썬 초보자에게는 두마리 토끼를, 설사 파이썬 능숙자라 할지라도 파이참이 주는 개발 생산성이 얼마나 큰지 비교해볼 수 있는 기회다. 책의 가독성과 짜임새가 훌륭하여 여기까지 진행하는데 겨우 2시간이면 충분했다. 여기까지 진행하면 파이참이 매우 만만해진다. 왠만한 IDE보다 훨씬 파이썬 코드를 능숙하게 다루는 레벨에 오른다. 그리고 다음에 소개할 5장에서 이책의 백미가 등장한다. 기본서보다 더 뛰어난 (실용) 기본서 언어의 기초부터 자세하게 설명하는 여느 기본서와는 달리 이 책의 5장은 파이썬이 어떤 분야에서 강점을 가지는지 알 수 있게끔 매우 재밌는 예제들로 시작한다. 뭐하나 재미없거나 신기하지 않은 것이 없다. 예를들어 아래 그림과 같이 처음부터 MS 클라우드인 Azure에 가입하여 AI API를 활용하는 예제가 등장한다. 로컬 PC에서 머신러닝 혹은 딥러닝 예제를 개발한 분들은 많겠지만 클라우드 API를 활용해서 개발한 분은 흔하진 않을 것이다. 파이썬에 API를 접목한 개발을 배우는 것을 넘어서서 AI API 수준이 얼마나 발전했는지 가늠하는 계기가 되고, 나아가 클라우드를 처음 쓰는 분이라면 신세계를 경험할 수 있다. 아래 그림은 zAI 패키지를 활용하여 특정 인물의 사진을 피카소 그림을 학습한 화풍으로 변환한 예제이다. 데이터 사이언스를 위한 언어답게 의사결정나무를 통한 머신러닝 예제도 다룬다. 주피터 노트북만 다뤘던 분들도 파이참의 또 다른 매력을 느낄 수 있는 계기가 된다. 아래 사진은 Graphviz 라이브러리를 활용하여 의사결정나무를 도식화한 모습이다. 그 외에도 매우 재밌는 파이썬의 매력을 파이참으로 구현한다. 하루를 온전히 투자할 수 있다면 파이썬으로 이 모든것을 엄청나게 빠르게 해낼 수 있다는 것에 큰 재미를 느끼게 될 것이고, 그 재미에 빠져드는 과정에서 파이참이라는 도구가 내 몸처럼 편안하게 착용되는 느낌을 얻을 수 있다는 점에 이 책에 높은 점수를 주고싶다. 추가로 구현하는 재미있는 예제를 간략히 소개하면 다음과 같다. AI API를 활용한 사진을 텍스트로 변환 영어를 한글로 번역 후 음성으로 변환 파파고 API를 활용한 PDF 문서번역 Selenium 및 BeautifulSoup을 활용한 웹브라우저 자동화 및 크롤링 암기하고 싶은 내용을 메일로 받기 GUI 기능을 활용한 만능사전 만들기 오픈 API를 이용한 데이터 수집 및 분석 금융데이터 수집, 분석, 시각화 MatPlotLib으로 타임 타이머 만들기 의사결정나무로 메이저리그 경기결과 예측하기 웹대시보드 만들기 열거한 주제만봐도 만들어보고 싶은 욕구가 샘솟았다. 그래서 책도 굉장히 빨리 읽을 수 있었고, 파이썬의 실용성에 다시금 감탄했으며, 파이참의 생산성에 또 한번 놀랐다. 그 과정에서 파이참은 더욱 능숙해졌다. 다시 고급기능으로… 화룡정점 어떤 IDE툴일지라도 생산성을 최고로 높이는 Tip중 최고를 꼽으라면 디버깅이라고 말하고 싶다. 위에서 소개한바와 같이 재밌는 예제를 구현하며 흠뻑빠졌다 나오면 이제 익숙해진 파이참에 대한 자신감이 생기고 자연스럽게 다음 스텝에의 적응이 쉬워진다. 디버깅은 IDE툴을 제대로 다루기 위해선 빼놓을 수 없는 기능인 바 자연스럽게 익힐 수 있도록 짜임새있는 구성으로 배려한다. 더불어 코드 탐색을 손쉽게 다룰 수 있는 방법, 코드 작성 Tip, 그 외의 기능에 대해 소개한다. 아래 그림은 다른 프로그램에서 파일열기, 탐색바로 이동하기, 함수단위로 이동하기 등 다양한 Tip에 대한 기술이다. 아래 사진은 DB Navigator 플러그인을 설치하여 활용하는 방법을 제시한다. 이처럼 파이참의 본 기능을 충분히 다진 후 다양한 확장기능을 활용할 수 있도록 파이참에 대해 깊이있게 다룬다. 책소개 Link Github Link 누가 읽어야 하는가? 다른 IDE 환경에 불편이 많아 파이참의 매력을 알고 싶은 분. 파이썬의 매력을 제대로 알고싶은 파이썬 입문자. 파이썬의 깊이보다는 넓이를 폭넓게 파악하고 싶은 기획자, 경영자, 문과생 등 기타 파이썬, AI, 데이터분석에 관심이 있는 분. 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 파이썬/파이참 설치 및 기본연습 (1 ~ 4장) 파이썬과 파이참 설치하기 파이참 프로젝트, 레아이웃, 초기설정 익히기 파이썬의 기초학습 입출력, 함수, 자료형, 조건문, 반복문, 주석, 예외처리, 패키지 등 2. 재미있는 예제구현(5장) AI API, 오픈 API, 수집, 크롤링 활용 GUI 및 시각화, 자동화 및 웹개발 의사결정나무 머신러닝 예제 등 3. 파이참 심화(6장) 디버깅 및 확장기능 활용 코드 탐색 및 작성 Tip 등 요약하며… 제트브레인사의 또다른 명작 IDE 파이참의 활용방법을 다룬 현시점 국내 유일무이한 도서이다. 처음부터 끝까지 독자가 어려워 나가떨어지는일이 없도록 흥미의 레벨을 서서히 올려가는 가독성있고 짜임새 있는 구성이 이 책의 백미이다. 더불어 초보자 또는 파이썬 입문자들도 파이썬을 활용하여 어떤 재밌고 편리한 기능을 구현할 수 있는지 예제를 통해 실습해보면서 스스로의 창의성에 파이썬을 접목시켜볼 수 있는 좋은 계기를 얻게된다는 점이 큰 장점이라고 할 수 있다. 파이참은 다양하고 파워풀한 기능을 제공하는 대신 초반에 익숙해지기 약간 까다로운 진입장벽이 있는 편인데 재미있는 예제에 푹빠져 하나씩 팁을 배우다보면 어느새 책을 덮을때 즈음 파이참이 수족처럼 느껴지는 신기한 현상을 경험할 수 있다. 저자는 독자로 하여금 흥미를 잃지않고 숨쉬듯 자연스럽게 훌륭한 IDE를 익힐 수 있게 인도해주는 좋은 선생님 역할을 충실히 수행해냈다 생각한다. 물론 GUI IDE를 다루는 책의 특성 상 컬러판이었다면 가독성을 보다 높여주었을텐데 라는 아쉬운 마음도 있다. 대신 가독성이 좋은 책이라 흑백으로 봐도 큰 불편함은 없다. 간만에 프로그램을 처음 배울때 처럼 호기심과 재미만으로, 공부보다는 취미생활하는 느낌을 주었던 책이다. 더불어 일상생활에 사진이나 데이터를 관리하고 시각화하는데 좋은 영감을 얻기도 해 상당히 만족스럽게 읽었다. 파이참을 몇년 간 다뤘음에도 알지못했던 Tip을 얻고 다시금 정리할 기회를 얻었음에 감사드리고, 파이참에 관심이 있거나 다루기 어려웠던 독자분들은 반드시 읽어보시길 추천드린다. 파이참을 익히는 가장 빠른 지름길이라고 생각한다. &lt;비제이퍼블릭 출판사&gt; 책 한권만으로도 실무를 수행할 수 있을 정도의 실용성, 기존 출간된 책에서 다뤄지지 않았던 가려웠던 구석을 시원하게 긁어주는 참신함, 피상적인 접근으로 그런가보다 넘어가기 쉬운 지식에 대한 깊이있는 고찰을 담은 장인 냄새가 나는 출판사입니다. 더불어 참신한 해외 번역서, 최신 트렌트를 겨냥하는 양질의 서적을 자주 출간하여 필자의 책장에도 어느덧 십여권의 책이 꼽혀있네요. IT 실력을 향상시키는데 있어 뭔가 허전하거나 시간은 투자대비 내공이 채워지지 않는 공허함이 생긴다면 비제이퍼블릭 출판사의 책 목록을 살펴보시기 바랍니다. 비제이퍼블릭 바로가기",
    "tags": "review book python pycharm",
    "url": "/review/2019/11/22/review-book-pycharm/"
  },{
    "title": "[리뷰] 다이내믹 프로그래밍 완전정복",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"다이내믹 프로그래밍 완전정복(미나크시, 카말 라와트 저)\"을 읽고 얻은 지식을 정리한 글입니다. 목차 무서운 재귀, 더 무서운 Dynamic Programming(DP) 다이내믹 프로그래밍을 위한 본 도서의 장점 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 무서운 재귀, 더 무서운 Dynamic Programming(DP) 프로그래머 혹은 컴퓨터 공학도라면 누구나 이 무서운 단어들을 들어봤을 것이다. 대부분의 프로그래머는 이런 용어가 세상에 존재하는구나 하는 정도로 넘어가고 잊어버린다. 이 단어들은 프로그래머에게 왜 무서운 단어가 되었을까? 먼저, 본 도서의 장점을 전달하기 위해 필자가 겪었던 다이내믹 프로그래밍 경험을 말씀드리고자 한다. 이름부터가 직관적이지 않다. 본 도서 서문 “옮긴이의 말”에서 역자는 Dynamic Programming을 “동적계획법”이 아닌 “다이내믹 프로그래밍”으로 음차하겠다고 소개한다. 자칫 독자가 프로그래밍 방법론으로 혼동할 수 있기 때문이다. 역자가 우려하듯 우리말 번역과정에서 진의가 꽤 소실된다. 리처드 벨만은 그의 자서전, “태풍의 눈”에서 Dynamic Programming의 어원을 다음과 같이 설명한다. ‘의사 결정 프로세스’라는 이름을 사용했지만, 여기에서 ‘프로세스(Process)’라는 단어를 사용하는데 여러가지 차질이 생겨버리고 만 것이다. 그래서 나는 사람들이 알지 못하게 ‘계획법(Programming)’이라는 단어를 붙였다. 또한 나는 이 프로세스가 다단계로 이루어져 있으며, 시가변적(time-varing)이며, ‘동적(Dynamic)’이다라는 개념(idea)이 전달되길 원했다. 이 단어야말로 내가 연구하는 알고리즘의 성질을 정확하게 짚어내었고, 게다가 윌슨에게도 피해를 입히지 않으며 공군도 이 단어에선 꼬투리를 잡지 못했으니 그야말로 일석이조의 효과를 누린 것이다. 벨만이 처한 상황에는 당시 모종의 정치적인(?) 이유가(상급자, 연구비 등) 개입되며 직관성이 흐려진 듯 하다. 그래서 필자는 대다수의 해석에 따라 다음과 같은 뉘앙스로 이해한다. 다이나믹(Dynamic) : 동적 메모리(시간에 따라 변하는 메모리) 프로그래밍(Programming) : 다단계(큰 문제 안에 작은 문제들이 중첩된)로 이루어진 문제 풀이계획 결론 : 시간에 따라 변하는 데이터를 고려하여, 큰 문제를 작은 문제로 쪼개서 푸는 방식 실무에서 좀처럼 쓰이지 않는다. 여러분은 지금까지 재귀 혹은 다이내믹 프로그래밍을 몇번이나 활용하셨는지? 필자 역시 대학원 혹은 취업 면접이 있을때만 책으로 접했고, 실무에서 사용한 경험은 많지 않다. 가장 쉬운 경험을 예로 들자면 즐겨찾기 관리를 위한 App을 개발하면서 즐겨찾기 폴더를 순회하여 폴더별 저장된 URL을 가져오는 용도로 활용한 적이 있다. 시간복잡도와 찰떡궁합이다. 위에서 언급한 즐겨찾기 관리 App의 경우를 보자. 테스트를 진행하며 폴더 Depth를 100으로 늘렸더니 생각외로 꽤 오랜 수행시간이 소모되었다. 재귀함수를 이용하여 매우 소량의 코드로 문제를 해결한데다 기억이 가물가물한 재귀를 제법 빠르게 구현했기에 나름 속으로 ‘아직 살아있구만!’라고 자화자찬하던 와중에 약간 충격을 받았다. 다른 관련코드들 역시 시간을 잡아먹을만한 부분이 보이지 않아서 결국 알고리즘 책을 간만에 펼쳤다. 문제는 의외로 간단했다. 재귀 함수의 시간복잡도가 지수시간을 잡아먹고 있었던 것이다. 알고리즘의 기초 내공의 중요함을 다시금 깨닫게 된 순간이었다. 메모리 구조와 밀접한 관련이 있다.(공간복잡도) 경력 2년차 초보때 벌어진 일이라 혼자만의 추측이 시작되었다. “결국 재귀를 버려야 하나..? 이래서 사람들이 재귀를 안쓰는 거구만..!”등등…결국 Loop를 이용해서 다시 구현할까 생각했는데 왠지 지는 느낌이 들어 싫었다. 결국 다시 알고리즘 책을 펴보았다. 역시나 선배들이 해결해 온 역사를 통해 힌트를 얻을 수 있었는데 메모이제이션(Memoization)라는 캐시 기능을 활용하여 재귀 함수가 호출될 때 시간복잡도를 O(N)으로 줄일 수 있었다. 배열 하나 썼을뿐인데 이렇게 속도가 빨라지다니! 조금 더 흥미가 생기기 시작했다. 메모리 구조를 분석하며 얻은 또 하나의 수확은 Stack 시각화를 통한 개념 명확화였다. 메모리 및 스택을 그림으로 그리고 활성레코드 함수 변화를 정리해보니 어렴풋했던 재귀 호출의 동작방식이 명확하게 이해되는 것이었다. 당시 정립한 개념 덕분에 지금도 DP를 활용하여 개발할 때 머리 속 스택그림의 도움을 많이 받는다. 일상속의 직관과 거리가 멀어 전략이 필요하다. 꼬리에 꼬리를 끝없이 물어가는 재귀 호출을 구현 시 명확한 기준이 없다면 재귀적 사고의 악순환(?)이라는 주화입마에 빠지고 만다. 머리가 복잡해지면서 뇌는 본능적으로 재귀를 피하려고 한다. 따라서 가장 중요한 것은 뇌에게 탈출구를 안내할 수 있는 종료조건과 작은 문제에 대한 명확한 정의이다. 재귀가 보통 Top-Down 방식을 이용하는 것과 달리(Top-Down 방식은 이름에서 유추할 수 있듯이 보다 큰 인자에서 작은 인자로의 재귀 호출을 반복한다. 따라서 동일한 인자를 가지는 함수가 매번 수행되어 성능상 치명적인 약점을 가지게 된다. 대신 코드의 가독성이 높다.) DP에서는 Bottom-Up방식을 이용한다. 재귀에 비해 크게 어려울 것이 없는것이 함수대신 변수(배열 등)의 이미 연산된 작은 값들을 활용하여 큰 값들을 반복적으로 채워나가는 방식이다. 덕분에 동일값에 대한 연산을 피하여 성능을 높일 수 있는 장점이 있다. 재귀와 마찬가지로 DP를 적용할 수 있는 문제인지 어떻게 세부적으로 구현할 지 등에 대한 전략이 존재한다. 면접과 실무를 넘어서서… DP 자체를 명확히 이해하는 것도 중요하지만 공학분야에 있어서만큼은 언제, 어디에 적용할 수 있는지를 아는 것이 더 중요하다. (참고 : WHAT « WHEN &amp; WHERE) DP는 언제 어디에 적용할 수 있을까? 부분적으로는 O(n^3) 등 다항식 수준의 시간복잡도를 O(n*logn) 등의 로그 수준으로 줄여 성능을 높일경우 DP를 활용할 수 있을지 판단해야 한다. 더불어 한단계 수준을 넘어선 전혀 다른 영역에의 응용이 가능하다. 예를들면 강화학습이 있다. 강화학습은 각 Step별 Action을 취하는 문제를 모델이 없는(Model-free)상태에서 MDP(Markov Decision Process)를 활용하여 풀어나가는 방법이다. 때문에 Model의 Environment에 해당하는 Reward, State Transition Probability등을 최적화하기 위해 Learning(학습)을 해 나간다. DP와의 접점이 느껴지시지 않는지? DP는 Model을 완벽히 안다는 전제하에(Model-based) Bellman Equation을 풀어 Environment를 구하는 방식이다. 그래서 Planning이라고 부르며 이를 보완하여 Learning을 통해 Environment를 최적의 상태로 찾아가는 것 즉, 강화학습 알고리즘을 만들게 된 것이다. 때문에 DP의 개념 및 활용방안을 정확히 모른다면 강화학습에 대한 이해는 물론이고 보다 나은 방법을 찾기가 거의 불가능할 것이다. 더불어 문자열 연산을 다루는 NLP에 있어서도 DP의 문제 해결방식은 큰 도움을 준다. DP에 대해 더 설명하고 싶지만 필자의 짧은 지식으로는 여기까지다. 하지만 경제학 등 DP의 활용도는 무궁무진할 것이고 어떻게 다른 지식과 융합, 보완하느냐에 따라 멋진 걸작이 나올지도 모른다. 필자가 경험한 이 일련의 과정에 비추어 본 도서가 어떤 장점을 갖는지 다음장에서 간단히 다뤄보고자 한다. 다이내믹 프로그래밍을 위한 본 도서의 장점 앞장에서 다이내믹 프로그래밍 경험 및 스스로 학습해왔던 과정을 간략하게 설명하였지만, 사실 그 간략함이 몇 년간 다이내믹 프로그래밍과 관련하여 학습한 지식의 거의 전부이다. 본 도서를 읽으며 놀라웠던 점은 필자가 겪었던 시행착오나 전략이 고스란히 담겨있다는 것이다. 필자의 지식이 고수들에 비할바 못하는것을 알면서도 꼭 이런 양서를 만나면 나만알고 있었을 듯한 밑천이 외부에 다 털린 느낌이 들어 배가 아프다. 하수라서 그런것일까? 다이내믹 프로그래밍 자체를 적용하기 위해 몇일간 고민했던 흔적은 아이러니하게도 자연어로 정리하면 고작 한줄 정도 담긴다. 즉, 다이내믹 프로그래밍을 자연어로 정리하면 설명력이 크게 떨어진다. 미묘한 기법과 뉘앙스를 전달하기 위한 사고과정에 대한 뚜렷한 설명이 거의 불가능하다는 것이다. 때문에 본 도서 또한 전략과 이론에 관련된 내용이 매우 짧다. 거의 대부분의 페이지는 예제와 예제의 설명, 시각적 설명이 차지하고 있다. 많은 예제를 바탕으로 사고과정을 공유하는 것이 거의 유일한 해결책이라는 생각이 드는데 본 도서가 그런 접근법을 통해 다이내믹 프로그래밍 예제를 같이 풀어보고 알기쉽게 설명해줌으로써 주화입마에 빠질 우려를 줄여준다. 명쾌한 전략제시 앞서 설명한 바와 같이 자연어로 다이내믹 프로그래밍이라는 문제풀이 접근방식을 설명하긴 보통 어려운 일이 아니다. 대신 기억하기 쉬운 핵심 전략을 머리속에 두고 접근하는 것과 대책없이 프로그래밍을 시작하는 것은 큰 차이가 있다. 아래 그림은 다이내믹 프로그래밍과 메모이제이션 그리고 재귀 호출에 대한 핵심 전략을 기술한 페이지이다. 사고과정의 이해를 돕는 시각화 다이내믹 프로그래밍은 예제와 실전을 통한 사고과정의 고민의 깊이가 어느 정도인지에 따라 그 활용 능력이 갈린다. 사고과정이 일상의 직관과는 동떨어져있어 쉽게 포기하기 쉬운데 절대 포기하지 않도록 저자, 역자의 고민의 흔적이 설명에 녹아있다. 더불어 아래 그림과 같이 직관적인 이해를 돕는 시각화를 통해 이해도를 크게 높여준다. 원리를 바탕으로 한 전달력, 중간중간 깨알같은 선수지식의 소개 기저에 숨어있는 원리를 설명하지 않고 소스코드의 주석과 결과만으로는 다이내믹의 정수를 얻기 힘들다. 기본 원리를 절대 놓치지 않으려는 시도가 이 서적의 또 다른 매력이다. 예를들면 다이내믹 프로그래밍이 가지는 장점을 소개하기 위해, 또 이해도를 높이기 위해 메모리 구조를 설명한다. 이를 통해 공간복잡도의 계산이 훨씬 쉬워지고 다이내믹 프로그래밍이 얻게되는 시간, 공간복잡도 성능향상이 어느정도인지 개념적으로 와 닿도록 도와준다. 아래 그림은 메모리 구조 및 스택영역에서의 재귀함수의 활성레코드를 보여줌으로써 머리속에 스택의 동작방식을 이해하고 있는것이 얼마나 다이내믹 프로그래밍에 대한 이해도를 높여주는지 알 수 있게 해준다. 다이내믹 프로그래밍과 관련된 거의 모든 예제 본 도서에 소개된 재귀 및 다이내믹 프로그래밍의 관련 예제는 무려 20개가 넘는다. 그 정도면 거의 왠만한 실무를 커버할 수 있는 수준이 아닌가 싶다. 제대로 이해를 못했다면 예제의 감각이라도 충분히 잡아 반드시 활용할 수 있게 해주려는 저자의 의지가 돋보인다. 아래 그림은 필자가 재미있게 풀어본 예제인 “문자열 인터리빙 확인” 문제인데 사고과정을 명확하게 시각화하여 이해를 돕는다. 더불어 아래 “거스름돈 최적화” 문제와 같이 DP와 유사한 탐욕 알고리즘과의 비교도 시도한다. 탐욕 알고리즘이 반드시 최적해가 아닌 케이스를 설명하며 비교 과정을 통해 DP 특성에 대한 이해를 더욱 높여준다. 본 도서의 모든 소스코드는 C와 Python 두종류의 언어로 제공된다. 두 언어를 모두 활용한다면 보다 이해도를 높일 수 있다. 기타 끝으로 본 도서를 학습하며 도움이 될만한 유용한 참고자료를 아래 링크로 남긴다. 책소개 Link GitHub 소스코드 C언어 개발환경 구성 - 인생초보자님의 블로그 누가 읽어야 하는가? 프로그래머 누구나(특히 면접시험을 앞둔 프로그래머) 재귀호출과 다이내믹 프로그래밍에 정면 도전하고 싶은 분 AI분야의 프로그래머(특히 강화학습, NLP에 많은 도움이 됨) 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 재귀호출과 메모리, 활용전략(Part1) 재귀 접근전략, 재귀 호출과 메모리 최적의 하위구조, 하위 문제의 반복 계산, 메모이제이션(메모전략) 예제 : sum(n), 점화식, 하노이탑, 피보나치, 역사이 최소 비용 등 2. 다이내믹 프로그래밍(Part2) Top-Down 및 Bottom-Up 접근방식의 차이 다이내믹 프로그래밍의 적용을 위한 전략 예제 : 부분 문자열, 계승함수, 이진트리, 행렬 최소이동비용, 타일공터 채우기, 경우의수, 연속부분 배열의 최댓값 등 3. 실전연습(Part3) 최소교정비용문제, 직사각형의 총 경로수, 문자열 인터리빙, 부분집합의 합 최장공통부분수열, 거스름돈 최적화, 철근자르기, 0 -1 배낭, 최장회문부분수열, 달결낙하퍼즐 등 부록 : 시간 및 공간복잡도, 코딜리티(온라인 코딩 테스트) 활용법 요약하며… 다이내믹 프로그래밍이 어려운 가장 큰 이유는 일반적인 직관과는 다른 사고방식을 필요로 한다는 점이다. 특히 사고과정을 면대면이 아닌 책으로 기술한다는 것은 더욱 어려운 일일 것이다. 이런 어려움을 해결하고자 본 도서는 명확한 전략, 사고과정에 대한 깊은 설명, 시각화를 이용한 사고과정의 보조, 원리를 중시한 핵심 개념 설명을 활용하여 DP에 대한 이해도를 극대화 시켜준다. 왜 이 책이 실리콘밸리의 우수한 IT인력을 공급하는 인도 그리고 해외 시장에서 베스트 셀러에 올랐는지 알 수 있는 부분이다. 아쉬운 점이 하나 있다면 독자로 하여금 DP에 대한 이해를 포기하지 않도록 책 중간중간에 선수 지식이 종종 소개되는데 어느정도 내공이 찬 프로그래머라면 재귀 및 DP에만 집중하기에 약간 산만한 느낌을 받을 수 있겠다는 생각이 들었다. 하지만 초중급자를 위해서는 이만큼 친절할 수가 없다. 아울러 C, Python 두가지 언어로 예제를 작성한 바 포인터의 유무, 객체지향의 유무 등 언어 특성에 따라 가려지기 쉬운 DP의 개념을 두 언어로 구현, 비교해봄으로써 이해를 명확하게 할 수 있다는 장점이 있다. 더불어 두 언어 자체에 대한 이해도가 높아지는 것은 보너스다. 예쁜 색상으로 디자인되고 무겁지 않아 들고 다니면 왠지 뿌듯한 감성이 충만된다. 강화학습 또는 NLP를 학습하며 DP의 명확한 개념을 잡고 싶은 분, 알고리즘 코딩 테스트를 앞둔 분, DP를 뽀개 완전히 두려움을 없애고 싶은 분께 꼭 일독을 권한다. &lt;한빛미디어 출판사&gt; 믿고보는 “한빛미디어 출판사”. IT분야에서 독보적인 양질의 도서를 출판하는 회사입니다. “나는 프로그래머다” 팟캐스트 후원, DevGround2019 행사, 리뷰어 모집, 다양한 학습 지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT분야에 관심 있으시다면 한빛미디어의 책으로 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review book dynamic programming",
    "url": "/review/2019/11/11/review-book-dynamic-programming/"
  },{
    "title": "[리뷰] 파이썬과 NumPy로 배우는 선형대수",
    "text": "개요 본 리뷰는 비제이퍼블릭 출판사 \"파이썬과 NumPy로 배우는 선형대수(이정주 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 도대체 보스턴 주택가격 예측이 선형대수랑 뭔 상관이야? 선형대수를 쉽게 익히기 위한 본 도서의 장점 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 도대체 보스턴 주택가격 예측이 선형대수랑 뭔 상관이야? 책 소개에 앞서 잠시 선형대수의 맛을 보여드리고자 한다. 선형대수의 맛이라도 보지 않고는 그저 고1때 배웠던 행렬 정도로 피상적인 주입식 개념만 가진 채 앞으로도 그때처럼 선형대수를 마주하게 될 것이다. 그렇게 배운 지식은 쉽게 잊혀지고 잊혀지지 않아도 언제 어디에 써야 하는지 알수가 없다. 더불어 이 책이 왜 필요한 것인지도 제대로 알 수 없다. 아마 이 글을 읽고 계신 대부분의 독자분들은 선형대수 자체를 배우기 위한 목적보다는 데이터 사이언스, 머신러닝, 딥러닝 예제를 구현하다 거꾸로 선형대수라는 기초를 튼튼히 하고자 오신 분들이 대다수일 것이다. 선형대수 책을 펴면 역행렬, 가우스 소거법, 단위행렬과 같은 생소한 수학 단어들이 즐비한데 보스턴 주택가격 예측 예제에는 저런 단어들이 나오지 않는다. 본 포스팅은 그 먼 간격을 좁히고 그 과정에 이 책이 얼마나 도움이 되는지를 알리는 데에 있다. 먼저 선형대수를 왜 알아야하는지 설명하면서 그 간격을 좁혀보고자 한다. 가우스소거법과 역행렬 연립방정식 : 수포자가 아니라면 “그래 이 정도는 알고 있지.”라고 생각하실 것이다. 1식 : 2x + 3y = 7 2식 : x + y = 3 그렇다. 미지수가 2개이고 방정식도 2개이며 x, y를 각각 구하는 문제이다. 기억이 나시는지? 가우스소거법 : “어디서 많이 들어봤는데..” 그렇다. 우리는 주입식 교육으로 중고교 시절 분명히 이것을 들었다. 먹고 사느라 잊혀졌지만 사실 별건(?) 아니다. (2,3) * (x) = (7) (1,1) * (y) = (3) 위 식을 이렇게 행렬로 변환한 후 역행렬과 단위행렬을 활용해 해를 구하는 방법이다. 여기서 우리가 알던 수식들이 다른 차원의 세계 즉, 행렬의 형태로 변환이 가능하고 심지어 해도 구할 수 있다는 것을 알게된다. 역행렬 : “분명 배웠는데 뭔진 모르겠네..” 역시 별것아니다. 행렬 A에 곱하면 결과가 단위행렬이 되는 행렬이다. 그럼 이건 왜 중요하냐? 역행렬이 존재하면 연립방정식의 해가 존재하는지 알 수 있다. 더불어 고유값, 고유벡터 계산시 활용된다. 벡터, 벡터공간, 선형사상 벡터 : (2,3)과 같이 그저 1차원의 행렬이다. 가로, 세로 둘중에 하나만 있는 행렬이라고 생각하면 된다. 이걸 좌표에 그려보자. 그럼 x축이 1이고 y축이 2인 선이 만나는 곳에 점이 찍힐것이고 이것을 어떤점과 이으면 그것이 벡터다. 어? 그런데 위의 연립방정식 1식을 x,y축 좌표에 그리면 비슷한 모양의 직선이 된다. 모양이 비슷하지 않은가? 여기서 우리는 \"벡터=함수\"라는 것을 착안할 수 있다. 벡터공간 : 2개의 벡터가 만나면 생기는 공간. 여기서 우리는 벡터 2개로 2차원의 평면 세계를 다룰 수 있음에 주목해야 한다. 선형사상 : (2,3)과 같은 벡터에 10을 곱하면 (20,30)으로 변한다. 즉, 벡터라는 함수에 10을 곱하는 함수를 곱할 수 있다. 시각화와 GPU 시각화 우리는 머신러닝을 다룰 때 시각화에 크게 의존한다. 수식 자체보다는 시각적 요소가 우리에게 직관을 쉽게 전달해주기 때문이다. SVM 분류를 다룰 때 초평면을 사용한다. 수식으로 보면 난해하겠지만 시각화를 이용하면 분류의 성공여부를 쉽게 파악할 수 있다. 수식으로만 알고 있던 것들이 벡터로 표현 가능함을 알았고 벡터로 그림을 그릴 수 있음을 위에서 설명했다. 이제 시각화 도구를 구현하려면 위에서 설명한 선형대수를 활용하면 가능하다는 것을 알게 되었다. GPU GPU는 대량의 실수 연산처리를 병렬로 수행할 수 있다. 가우스소거법에서 든 예처럼 행렬로 데이터를 전달하면 GPU의 자원을 매우 효율적으로 활용할 수 있게 된다. 컴퓨터그래픽스와 이미지 전처리 데이터 사이언스와 별개로 특히 게임분야에서는 렌더링, 물리엔진 등 수학적 기법이 활용되고 있다. 2D, 3D 등 그래픽스의 구현을 위해서 위에서 설명한 선형대수가 활용된다. 딥러닝에 있어서도 이미지 전처리 과정에서 벡터를 중심으로 한 선형대수가 활용된다. 드랍아웃은 물론 이미지 회전, 이동, 확대/축소, 스케일 변환에 있어서 선형대수로 거의 모든 처리가 가능하다. 즉, 수식을 행렬로 바꾸어 GPU에 전달이 가능해진다. 최소제곱법과 딥러닝 최소제곱법은 익숙하실 것이다. 보스턴 주택가격 예측 예제 코딩하면서 시각화를 통해 선형회귀 모델의 결과로 산점도 사이의 정중앙을 통과하는 직선을 우리는 보았다. 그런데 이게 선형대수와 무슨 상관인가? 이제 간격을 좁힐때가 왔다. 연립방정식의 해는 방정식의 수와 미지수가 동일할 때 구할 수 있다. 하지만 방정식이 더 적고 해가 많다면 해가 무수히 존재하게 된다. 이때 해를 찾을순 없지만 정답에 가까운 근사해를 찾고 싶다면 오차제곱합이 가장 적은 하나의 선을 얻게 되는데 바로 이 기법이 최소제곱법이다. 즉, 위의 예시와 같이 수많은 연립방정식이 선형대수를 활용하여 빠르게 GPU에게 연산을 맡길 수 있게 된다. PCA(주성분분석) 주성분분석은 데이터 개수에 비해 차원이 클 경우에 대량의 차원에서 최대한 특성을 보존한채로 차원을 축소하는 기법이다. 이때 특성을 최대한 보존하기 위해 분산이 최대인 주성분을 찾게 되는데 이것이 고유벡터다. 글이 예상외로 너무 길어져 이 즈음에서 줄일까 한다. 선형대수는(다른 수학도 마찬가지) 적어도 공학에 적용됨에있어서 만큼은 WHAT &lt;&lt; WHEN &amp; WHERE임을 강조하고 싶다. 우리나라 교육의 문제이기도 하다. 우리는 WHAT을 암기로만 외웠고 그 암기로 계산을 빨리 푸는데에만 집착했다. 그런데 그렇게 배운 지식을 언제, 어디에 써야하는지는 아무도 모른다. 우리는 수학을 배워서 구멍가게에서 과자 사먹을때 계산할 때나 쓰지 언제쓰냐고 하고 있을 때, 미국에서는 선형대수와 미적분으로 경사하강법, 오차역전파법을 고안했고 그 결과 알파고를 만들 수 있었다. 선형대수를 쉽게 익히기 위한 본 도서의 장점 위 장에서 선형대수를 통해 우리 세상의 복잡한 수식을 연산 구조를 보존한 채 효율적으로 정형화 된 형태로 표현할 수 있음을 어렴풋하게나마 알게 되었다. 더불어 컴퓨터가 빠른 속도로 계산하기 쉬운 형태로 변환할 수 있다는 점, 그로인해 시각적 표현도 가능해 졌다는 점을 알게 되었다. 이제 그 구체적인 방법을 배울 차례인데 이 책에 그 해답이 담겨있다. 필자는 프로그래머이고 위에서 언급했듯 선형대수를 익히기 위해 WHAT보다는 WHEN, WHERE가 중요했고 그로인해 수학 기본서보다는 프로그래머 전용의 수학책을 선호한다. 이 책도 그 중 하나인데 다른책에 비해 매우 쉽게 선형대수에 접근할 수 있다는 장점이 있다. 딥러닝을 학습하다보면 배워야 할 것이 너무 많아 자칫 기본을 소홀히하고 지나가기 쉽다. 코딩 한줄 음미하여 그런가보다 하고 넘어가다보면 캐글 같은 문제를 접하거나 모델링을 시도할 때 어디서부터 시작해야 할지 막막해지곤 한다. 가끔 침착한 마음으로 그 본질을 되새기다보면 머리속 복잡했던 개념들이 말끔히 정리되어 불현듯 해답이 떠오르곤 하는데 이 책을 읽다가 그런 느낌을 자주 받을 수 있었다. 저자는 미처 선형대수에 접근하지도 못한 채 낙오하는 독자를 한명도 남기지 않고자 파이썬을 다루기 위한 사전 작업에서부터 꽤 신경을 쓴 것 같다. 파이썬의 기본문법은 물론이고 설치과정, Numpy까지 친절히 설명하며 리스트 등의 다른 자료구조와 무엇이 다르고 그로인해 어떤점을 유의해야 하는지 자세한 예제를 통해 전달한다. 그 과정에서 파이썬과 Numpy에 대한 상당한 자신감을 갖고 선형대수를 접하게 된다. 선형대수 역시 기본부터 천천히 다룬다. 인상적이었던 점은 모든 과정을 시각화하며 확인한다는 점이다. 처음엔 자칫 느린 진행처럼 보여 시간이 아깝진 않을까 우려가 되었으나 급할수록 돌아가라 했던가. 코드로 직접 구현하며 수식을 음미하고 시각화를 통해 눈으로 확인하는 확인사살까지 거치다보니 투자한 시간이 전혀 아깝지 않을정도로 머리속에 모호했던 개념들이 뚜렸하게 보이기 시작했다. 그리고 코드 구현 및 시각화 구현 과정에서 소요된 시간은 생각보다 길지 않았다. 예를들어 아래 그림은 역행렬이 존재하지 않아 연립방정식의 해가 존재하지 않는다는 예제를 다룬 페이지이다. 위에서 언급한 기본 개념을 코드 및 시각화 구현을 통해 확실히 이해함으로써 역행렬의 개념을 보다 확실히 잡을 수 있었다. 다음 그림은 최소제곱법을 도출하는 수식을 제시하고 코드 및 시각화를 통해 어떤 상황인지 명확히 인지시켜준다. 그동안 두루뭉실하게 외우고 넘어갔던 지식들을 제대로 이해하는 계기가 되었다. 이미지를 확대, 축소, 이동, 변환하는 예제를 직접 구현하여 시각화해보면 수식과 숫자에 숨어있는 또 다른 의미가 명확히 보이게 된다. 수식에 대한 가독성이 높아진다고나 할까? 시각화를 선택한 저자의 판단은 옳았다. 그렇게 선형대수에 대한 자신감이 끓어오를때 즈음 이미지에 적용한 응용 예제가 나온다. 이미지의 기하학적 변환, Planar Rectification 등의 예제를 통해 선형대수가 어떻게 공학에 활용되는지 보다 구체적인 인사이트를 얻을 수 있다. 마지막으로 저자 특유의 꼼꼼한 설명으로 인공신경망에 대해 정리해준다. 선형대수와 크게 관련이 없는 내용이지만 인공신경망의 구조를 처음 접했을때와 달리 선형대수의 시각에서 접근하며 보게되어 밀도 높은 학습효과를 준다. 전체적으로 수수하면서도 내실이 꽉찬 묘한 정이 가는 기술서이다. 책소개 Link 누가 읽어야 하는가? 데이터사이언스 선행학습으로 선형대수의 기본 개념을 탄탄히 잡고싶은 개발자. 선형대수를 프로그래밍으로 구현해보고 싶은 수학도. 선형대수의 기초 개념조차도 이해하기 쉽지 않으신 분. 기타 선형대수, Python, 이미지 전처리 등을 배우고 싶은 학생, 실무자, 경영자 등 책의 구성 및 요약 이 책은 크게 세부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 파이썬과 넘파이(1 ~ 2장) 파이썬 개발환경 셋팅, 데이터타입, 제어문, 함수, 모듈, 입출력 등 넘파이 배열, shape, 슬라이싱, 복사, 브로드캐스팅 등 2. 선형대수(3장) 시각화, 벡터, 행렬 선형결합, 벡터공간, 벡터와 행렬 간 곱셈 등 선형 연립방정식, 행렬식, 고유값, 고유벡터 등 3. 응용예제 및 인공신경망(4장 ~ 6장) 이미지의 기하학적 변환 호모그래피 행렬과 Planar Rectification 인공신경망의 기본구조(레이어, 활성화함수, 가중치, 모델학습, 피드포워드, 경사하강법, 역전파 등) 요약하며… 딥러닝을 학습하다보면 배워야 할 것이 너무 많아 시간에 쫓겨 자칫 기본을 소홀히하고 지나가기 쉽다. 이 책은 시간에 대한 부담은 잠시 접어두고 기본에 충실하게 만든다. 마치 여행을 한번 갔다오는듯한 착각을 불러일으키는 책이다. 파이썬의 기본문법은 물론이고 설치과정, Numpy까지 친절히 설명하며 다른 자료구조와의 차이점, 유의사항까지 상세한 예제를 통해 전달한다. 이 책의 백미는 모든 과정을 시각화하며 확인한다는 점이다. 코드로 직접 구현하며 수식을 음미하고 시각화를 통해 눈으로 확인사살까지 거치다보면 머리속에 모호했던 개념들이 뚜렷하게 보이기 시작한다. 선형대수의 이해를 위해 전 과정을 시각화 한 저자의 선택이 탁월했다고 생각하며, 덕분에 수식과 숫자에 숨어있는 또 다른 의미가 명확히 보이게 된다. 일종의 수식에 대한 가독성이 높아진다. 특히 선형대수에 고질적인 어려움이 있는 독자라면 꼭 일독을 권하고 싶다. 반면 아쉬운 점은 크게 없지만 컬러판이 아니라는 정도인데 상당한 지면수에 따른 비용을 감안할 때 어쩔 수 없는 선택이었다는 생각이 든다. 더불어 인공신경망 부분의 설명은 선형대수 중심으로 전개하고 기존 시중에 나온 서적과의 중복된 부분은 개념위주로 최소화 했다면 더욱 명작이 되지 않았을까하는 아쉬움이 남는다. 전반적으로 겉모습은 수수한데 내실은 꽉찬 장인의 냄새가 나는 묘한 정이 가는 기술서이다. 선형대수에 관심이 있는 사람이라면 일독을 권한다. 그동안 경험하지 못했던 선형대수에 대한 새로운 시각을 얻게 될 것이다. &lt;비제이퍼블릭 출판사&gt; 책 한권만으로도 실무를 수행할 수 있을 정도의 실용성, 기존 출간된 책에서 다뤄지지 않았던 가려웠던 구석을 시원하게 긁어주는 참신함, 피상적인 접근으로 그런가보다 넘어가기 쉬운 지식에 대한 깊이있는 고찰을 담은 장인 냄새가 나는 출판사입니다. 더불어 참신한 해외 번역서, 최신 트렌트를 겨냥하는 양질의 서적을 자주 출간하여 필자의 책장에도 어느덧 십여권의 책이 꼽혀있네요. IT 실력을 향상시키는데 있어 뭔가 허전하거나 시간은 투자대비 내공이 채워지지 않는 공허함이 생긴다면 비제이퍼블릭 출판사의 책 목록을 살펴보시기 바랍니다. 비제이퍼블릭 바로가기",
    "tags": "review book python numpy linear algebra",
    "url": "/review/2019/10/28/review-book-numpy-linear-algebra/"
  },{
    "title": "[리뷰] 파이썬을 활용한 머신러닝 쿡북",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"파이썬을 활용한 머신러닝 쿡북(크리스 알본 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 데이터사이언스의 8할은 전처리 전처리, SW 2.0, 통계 핵심개념 등 본 도서의 장점 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 데이터사이언스의 8할은 전처리 자! 이제 난 제법 보스턴 주택가격 예측도 돌려봤고, MNIST 데이터셋 손글씨 인식도 해봤고, 또 Iris 붓꽃 분류도 능히 해냈다. 근데 왜 현실로 돌아오면 난 아무것도 할 수가 없지? 이 책에 관심있는 독자분이라면 다들 비슷한 고민을 한번쯤 해보셨거나 마주하고 있는 중일 것이다. 필자 역시 마찬가지였고 지금은 나아가긴 했지만 새로운 유형의 데이터 셋을 만날 때마다 비슷한 고민에 빠진다. 머신러닝과 딥러닝을 마주하며 겪는 우리의 고민은 여러 유형이 존재하지만 당장 맞닥드리게 되는 크게 2가지 부류의 문제가 있다. 하나는 데이터 특성을 파악한 후 어떻게 해야 모델에 떠먹여 줄 수 있을지.. 즉, 전처리에 대한 고민이고, 다른 하나는 어느정도의 방법론은 찾아냈는데 언어의 벽 - Python, Julia, R, Swift - 등을 사용하면서 문법을 모르거나 또는 그 언어다운 기법을 사용할 줄 몰라 나만의 방식으로, 이상한 문법으로 빙빙 돌아가며 시간과 성능을 낭비하는 문제이다. 머신러닝 모델을 배워서 자신감도 생겼겠다, 내공을 튼튼히 하고자 머신러닝 관련 수학 개념도 잡았고, 코딩 더 매트릭스 같은 책으로 선형대수도 배웠고, 통계학의 기본기도 다잡으며 여기까지 왔는데 할 수 있는게 별로없다니.. 독립변수, 종속변수가 수치형인지 범주형인지 이진인지 숫자인지 등 데이터 성격에 따른 통계학적 개념도 어느정도 잡았는데 실무에서 마주친 데이터를 보고 머뭇거리게 될 수 밖에 없었던 경험. 그 무서운 경험에서 벗어나고 싶다면, 전처리에 대한 두려움을 없애고 싶다면, 필자가 아는 한 국내에선 이 책이 으뜸이라 말하고 싶다. 몸값 높은 데이터사이언티스트라면 현실에서 데이터를 마주했을 때 EDA와 시각화등을 통해 데이터의 생김새가 어떤지 감을 잡을것이다. 그리고 그간의 경험으로 축적된 내공을 통해 어떤 모델을 선택하고 어떤 평가지표를 적용할 지 견적이 나올 것이고, 모델이 최고의 성능을 발휘할 수 있도록 모델의 입맛에 맞게 전처리 작업에 돌입할 것이다. 그런 전문가가 아니라면 우리 대부분은 상당히 방대한 전처리의 범위에 압도당하고, 데이터의 특성에 따라 분석 기법도 천차만별이라는 사실에 좌절하게 된다. 실무에서 쉽게 맞닥드리게 되는 다음의 질문을 생각해보자. 이미지에 나타난 사물의 경계선을 표시하고 싶다. 그런데 어떻게 그리지? 기껏 학습에 사용할 사진을 열심히 찍었다. 그런데 빛에 반사가 되었네? 빛의 반사는 어떻게 지우지? 평가지표라고는 MSE, ROC Curve, 정확도, Score Method 등 기본 지식만 갖췄는데 나만의 지표는 어떻게 만들수 있을까? 방금 만든 로지스틱 회귀 모델의 분산을 줄이고 싶은데 이거 가능한건가? 성별 데이터 불균형이 심한데 이거 그대로 돌려도 과적합에 빠지진 않으려나? 성능에 문제는 없으려나? 명목변수를 특성 인코딩을 거쳐 수치로 바꿨는데 왜 원하는 결과대로 안나오지? 만약 위 질문 중 단 한번이라도 비슷한 고민을 하셨다면 이 책이 솔루션이라고 말하고 싶다. 저자는 수십 페이지로 작성된 안내글이 아닌 하나의 직관적인 질문으로 이 책을 소개한다. 1000개의 범주와 누락된 데이터가 있는 수치 특성, 불균형한 클래스로 이루어진 범주형 타깃 벡터가 담긴 JSON 파일을 다루기 위해 어떤 레시피를 사용해야 할까? 이 짧은 문장에 대한 답이, 그리고 유사한 레시피가 이 책에 씌여있다. 필자는 저자, 역자, 출판사와 금전적으로 관련이 없지만 이 책은 돈이 전혀 아깝지 않다. 그동안 데이터 사이언티스트의 꿈을 꾸며 열심히 삽질했던, 캐글 점수 좀 올려보겠다고 책 뒤져보며 축적해 온 엄청난 공을 들여왔던 나의 내공이 전부 녹아있다니… 눈 앞에 늦게 나타난 것이 밉기도 하고, 몰랐던 기법을 배울 수 있어 이쁘기도 한 묘한 매력을 뽑낸다. 실무 혹은 캐글 등의 경진대회를 접할수록 전처리의 중요성은 알겠는데 도대체 고수들의 내공이 담긴 책이 없는 것이 늘 불만이었다. 원서나 외국 레퍼런스는 어학 실력이 부족해 종종 한계에 부딪혔고, 국내에 발간된 번역서 중 전처리라는 제목이 포함된 신간을 기쁜 마음으로 구입하기도 했는데 데이터 수집 내용만 잔뜩있는 것을 확인하고 눈물 흘리며 덮었던 경험도 있다. 다행히 이번엔 진짜가 나타났다. 다들 전처리, 전처리 말은 많지만 고작 결측치와 이상치 처리만 주구장창 다루고 실전에서 겪은 경험들은 왜 이리도 없는지.. 덕분에 갈증이 엄청나게 해소되었다. 전처리, SW 2.0, 통계 핵심개념 등 본 도서의 장점 앞 장에서 소개한 바와 같이 이 책의 화룡정점은 전처리이다. 하지만 그걸로 끝이 아니다. 전처리에 대한 필요 이상의 이론을 걷어내고 실무에 필요한 딱 그 수준으로 빨리 급한불을 끄게 솔루션을 제시한다. 그 후 깔끔하고 핵심이 담긴 언제 사용하고 왜 사용하는지 설명을 제시한다. 마지막으로 핵심과 어긋난 삽질으로 지치거나 가독성이 떨어지지 않도록 (예를 들면 Open CV가 Python 3.6에서 지원이 안되는 줄 알고 지웠다가 설치하는 등 또는 사이킷런 0.20 등 최신버전의 등장으로 deprecate되기 직전의 코드를 유지보수하지 않도록 지켜주는 등) 역자가 후방을 든든하게 지켜준다. 요약하자면 책의 구성은 아래 그림과 같이 과제 - 해결 - 설명 - 참고 - 덧붙임(역자의 주)순서로 구성되어 있다. 책의 제목에 걸맞게 쿡북답게 구성되어있다. 덕분에 데이터의 기본 적재, 랭글링 기법을 기본으로 수치, 범주, 텍스트, 시계열, 이미지에 이르기까지 종류별로 거의 완벽하게 대응할 수 있는 기법들을 핵심개념 위주로 알려주어 문제 해결에 바로 적용할 수 있음은 물론 가독성을 올려주기에 학습효과가 좋다. 그렇게 탄탄해진 개념을 바탕으로 다양한 예제와 친절한 주석, 설명을 통해 스프트웨어 2.0의 구현에 도움을 주는 프로그래밍 스킬을 전수한다. 특히 R에서 탄생한 Pandas를 Pandas답게(axis 활용 등), Python을 Python답게(리스트 컴프리핸션의 등) 사용할 수 있는 기법을 전수한다. 특성추출 및 선택 파트에서는 주성분분석(PCA)에 대한 깔끔한 해결책 뿐만 아니라 데이터 종류, 분산 특성에 따라 베르누이, 카이제곱, RFECV 등 어떤 것을 사용해야 하는지 통계학 개념이 실무핵심 위주로 집대성 되어있고 어떤 상황에서 왜 써야 하는지 상세히 서술한다. 더불어 현존하는 인기 머신러닝 모델을 선형회귀부터 신경망까지 깊이있게 다루는데 하나같이 다른책에선 보기 힘든 모델 관련 실무에서 맞닥드리게되는 문제 및 해결책 중심으로 구성되어 있어 저자 내공의 깊이에 적잖이 놀랐다. 마지막으로 pkl, HDF5등의 포맷으로 학습된 모델을 저장하고 불러오는 방법의 전수를 끝으로 화룡정점을 장식한다. 누구나 좋은책을 만나면 숨겨놓고 나만보고 싶다는 생각이 든 경험이 있을 것이다. 본 도서가 그렇다. 내가 그동안 열심히 시간을 투자해가며 얻은 팁과 내공들을 왜 세상에 이렇게 쉽게 내놓는 거냐며 투덜거리면서도 몰랐던 부분을 배울땐 그렇게 고마울 수 없는 묘한 책이다. 멋진 저자와 역자의 환상적인 콜라보로 탄생한 이 명작이 국내에서 필자와 같이 고군분투중인 데이터 사이언티스트 지망생들께 큰 도움이 될 것이라 믿어 의심치 않는다. 책소개 Link 역자가 구현한 소스코드 GitHub Link 누가 읽어야 하는가? sw 2.0 프로그래머 : 자체 데이터 사이언스 솔루션 개발, Python의 고급기술 습득 데이터 애널리스트 : 부족했던 프로그래밍 스킬을 채우기에 좋다. 데이터 사이언티스트 : 모델링 핵심에 대한 실무 전반을 돌이켜 보고, 더 좋은 성능을 보장하는 설계에 도움이 될 것이다. 기타 머신러닝, 딥러닝, Python, 전처리 등을 배우고 싶은 학생, 실무자, 경영자 등 책의 구성 및 요약 이 책은 크게 네부분으로 구성되며, 각 파트에서 다루는 내용을 아래와 같이 요약해 보았다. 1. 벡터, 행렬, 배열, 데이터 적재 및 랭글링(1 ~ 3장) Numpy를 활용한 벡터, 행렬, 배열, 희소행렬, 인덱싱, 벡터연산, 기초통계량, 전치, 행렬식, 고유벡터, 역행렬 등 샘플 데이터셋 적재 및 생성, CSV/ Excel/ Json/ SQL로 부터의 적재방법 데이터프레임, 행선택, 치환, 기초통계, 이상치, 결측치, 중복제거, 열원소순회, apply일괄적용, 병합 등 2. 데이터 종류별 전처리 기법(4 ~ 8장) 데이터 종류(수치, 범주, 텍스트, 시계열, 이미지)에 따른 차별화된 실무 솔루션 제공 스케일변환, 군집과 KNN을 활용한 이상치 처리, 결측치 처리, 순서유무에 따른 인코딩, 불균형 해소 등 구두점, 불용어, HTML파싱, 어간추출, 품사태깅, BoW, TF-IDF 처리기법 등 시차특성, 이동시간 윈도우 사용법, 이미지 이진화, 배경제거, 경계선 감지 등 3. 특성 추출,선택 및 모델의 평가,선택(9장 ~ 12장) 주성분분석, 클래스분리, 행렬분해, 희소데이터 특성 줄이기, 분산 기준으로 수치 및 이진 처리, 상관관계 특성 다루기 등 교차검증, 기본회귀, 기본분류, 이진분류, 다중클래스분류, 사용자정의 평가지표 만들기 등 완전탐색, 랜덤탐색, 전처리, 병렬화, 알고리즘 특화 기법 활용등을 통한 최선의 모델 선택 기법 4. 머신러닝 모델(13장 ~ 20장) 선형회귀, 트리, KNN, 로지스틱회귀, SVM, 나이브베이즈, 군집, 신경망 등 모델별 실전에서 마주치는 실무적 해결기법 정리 훈련된 모델의 저장 및 복원 요약하며… 현존하는 국내 서적 중 데이터사이언스 전처리 실무를 다루는 끝판왕이라고 요약하고 싶다. 머신러닝 모델과 개념을 다루는 책은 많지만 실전에서 특히 전처리를 다루는 책은 정말 접하기 쉽지 않다. 실무에서 맞닥드리기 쉬운 200가지의 실전문제에 대한 레시피를 제공함으로써 능숙한 데이터사이언스들이 실무에서 발휘하는 내공을 얻을 수 있다는 점이 매력이다. 모델에 따른 전처리 능력은 현직자들의 몸값을 높이는 스킬이므로 좀처럼 공유되지도 않는데다, 전처리는 데이터 유형, 분포, 성격에 따라 다루는 기법이 천차만별이기에 집대성하기 결코 쉽지 않은 분야이다. 실무 및 캐글 등 경진대회에서 깊이있게 고민했던 거의 모든 문제에 대한 해법이 담겨있어 놀랄 수 밖에 없다. 더불어 파이썬 다운 프로그래밍 기법을 활용한 샘플 제시로 좀 더 효율적인 프로그래밍 기법을 익힐 수 있게 해주는 점 또한 백미이다. 개념적으로 접근하기에도 쉽지 않은 실무문제에 골머리를 썩으면서, 또 한편으로는 파이썬의 효율적인 기법을 몰라 Pandas가 데이터를 다루는 방식을 몰라 레퍼런스를 찾고, 구글링하고, 커뮤니티에 질문을 올려 답이 올라오기만을 바라는 상황에 처해 본 프로그래머라면 이 책에서 제시하는 데이터사이언스에 특화된 프로그래밍 기법이 실무에 적용하기 얼마나 편리하게 정리되어있는지 감탄하게 될 것이다. 아울러 때로는 실무에 필요한 요소를 바로 찾아내기 어려운 방대한 통계의 바다에서 실무 문제에 어떤 기술을 어떤 상황에서 빠르게 적용해야 하는지 바로 제시함으로써 실무에 특화된 통계적 시각을 함양하게 해준다. 컬러판으로 출간되어 코드에 대한 가독성도 훌륭하여 아쉬운 점이 거의 없다. 굳이 찾자면 제목에 임팩트가 너무없기에 다른 서적과 중복된 내용을 다루는 듯한 느낌이 든다는 것? 머신러닝 서적을 수십권 보유한 필자와 같은 독자들의 시선을 한번에 사로잡기 어렵다는 정도이다. 필자에게 네이밍 센스는 없지만 적어도 “데이터 유형별 전처리 기법 전부 + 전처리를 통한 실무 모델링” 정도의 느낌이 풍기는 제목이었다면 더 유명해지지 않을까 싶다. 다행히도 본 리뷰를 접하는 독자가 있으시다면 적어도 다른책에서 흔히 다루는 뻔한 머신러닝 모델 의 중복 설명을 다루는 책이 아님을 아실 것이다. 관련 업계 실무자라면 꼭 일독을 권한다. &lt;한빛미디어 출판사&gt; 믿고보는 “한빛미디어 출판사”. IT분야에서 독보적인 양질의 도서를 출판하는 회사입니다. “나는 프로그래머다” 팟캐스트 후원, DevGround2019 행사, 리뷰어 모집, 다양한 학습 지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT분야에 관심 있으시다면 한빛미디어의 책으로 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review book machine learning python cookbook preprocessing",
    "url": "/review/2019/10/04/review-book-ml-cookbook/"
  },{
    "title": "[부동산] 부(富)와 재테크를 위한 유용한 사이트 모음(부동산, 경제, 법률 등)",
    "text": "개요 돈은 수단이지 목적이 아니다.,사람의 그릇보다 넘치는 돈은 그릇을 깨뜨리게 마련이다. 흔한 돈과 관련된 명언들이지만 그 말들 안에는 분명 뼈가 있습니다. 제게 돈은 시간적인 자유를 가져다주는(즉, 직장을 다니지 않아도 내 시간을 온전히 의미있게 쓸 수 있는) 도구 그 이상 이하도 아닙니다만 여러분은 어떠신가요? 목차 내집마련 Site 부동산 Site 경제 Site 법률 Site 부동산 Feature 내집마련 Site 데이터 사이언스는 귀족 학문입니다. 입력층에 투입된 시간과 장비빨이 실력이라는 출력층까지 도달하는데 엄청난 가중치를 차지합니다. 시간과 장비빨을 최대로 얻기 위해선 돈이 무시못할 요소이기에 재테크에 유용한 사이트를 모아봅니다. 먼저 내집마련을 위한 유용한 사이트들을 소개합니다. 아파트투유 : LH 청약센터 : SH : KB 주택청약 : 온나라부동산 : 분양알리미 LH 분양정보 : 행복주택 : 분양정보 : 청약일정 블로그 : 개인이 운영 부동산 Site 부동산 관련 유용한 사이트입니다. 부동산 시세 및 거래 국토부실거래가 : 실거래가 국토부실거래가 DB : 실거래가 다운로드 서울시 부동산 정보광장 : 부동산 거래현황 밸류맵 : 지역별 실거래가 부동산지인 : 지역별 거래정보 호갱노노 : 신고가, 변동, 인구, 출근, 거래량, 역정보, 기사연동 시세미 KB부동산 리브온 (Liiv ON) : 지역별 실거래가, 시장동향, 상업시설 소상공인 상권정보 : 상권분석 통계청 : 대한민국 통계 부동산 정책 및 행정 서울플랜2030 : 서울 도시계획 부동산미래 : 대법원 : 경매정보, 등기부등본 확인 국토교통부 : 부동산 정책(보도, 공고, 고시 등) 도시계획포털 : 도시계획 정보 스마트서울맵 : 도시재생활성화지역 등 용도 정보 세움터 : 국토교통부 건축 행정(부동산 건축물대장, 인허가 확인 등) 재개발재건축 클린업시스템 : 재개발 정보 지존 토지보상 : 토지보상 정보 경기 부동산 포털 : 수도권 개발사업, 연속지적도, 토지이용계획 등 강원도 부동산정보 통합열람 : 부동산 기타 미래철도 DB : 철도정보(주인장님의 접근방식이 이채롭습니다.) 세계도시정보 UBIN : 세계도시 통계 국가교통 DB : 교통 데이터 서울 열린테이터 광장 : 인구, 가족, 복지, 산업, 교통, 교육 등 경제 Site 경제 관련 유용한 사이트입니다. 한국은행 경제통계시스템 : 경제관련 통계(소비자 물가지수, 증권, 부동산 등) 법률 Site 법률 관련 유용한 사이트입니다. 국가법령정보센터 : 공정거래위원회 : 국민권익위원회 : 등기소찾기 : 국민신문고 : 소비자상담센터 : 한국소비자원 : 상속인 금융거래 조회 : 내용증명(우체국 온라인) : 부동산 Feature 본 블로그는 본래 데이터 사이언스에 관한 지식을 공유하기 위한 취지로 만들어졌기에, 부동산에서 유용하다고 생각하는 특징(Feature)을 계속 정리해 나갈 예정입니다. 주 이유는 제가 나중에 부동산 시세 예측등의 머신러닝 시스템을 만들 때 참고하기 위함이나, 유사한 프로젝트를 수행하신다면 도움이 될 정보들입니다. 다만 제 아이디어가 부족하여 공유하고 싶으신 정보가 있다면 댓글 부탁드립니다. 부동산 정책 NLP 분석 누가 그러더군요. 역세권, 학군 등의 피처넣고 과거년도로 학습하면 예측 가능한거 아니냐고? 해봤습니다. 잘 안되더군요. 잘 안되는 가장 큰 이유가 바로 이 정책입니다. 하지만 정책에서 피처를 뽑아내는 작업은 사람이 개입하지 않는 한 딥러닝으로는 거의 불가능에 가깝습니다. 그래서 NLP를 이용해야 합니다. BERT 같은 것을 이용해서 말이죠. 로열동(층) Feature 소음, 방향, 조망, 층, 일조량, 특화설계, 판상형vs탑상형, 출입구까지의 이동거리 본 포스트는 주기적으로 업데이트 예정입니다.",
    "tags": "favorites land economy favorite law site",
    "url": "/favorites/2019/09/05/favorites-economy-fav/"
  },{
    "title": "[커리어] 데이터 사이언티스트가 되기 위한 채용 및 조언 사이트 모음",
    "text": "개요 데이터 사이언티스트가 되기 위한 채용 사이트 및 선배들의 조언 등 유용한 사이트를 모았습니다. 목차 데이터 사이언티스트 채용 Site 데이터 사이언티스트 구직을 위한 조언 Site 현직 데이터 사이언티스트 Site 해외취업을 위한 조언 Site 대학원 Site 교육기관 Site 기타 Site 데이터 사이언티스트 채용 Site 카카오AI 네이버클로바 Search &amp; Clova 쏘카 링크드인 페이스북 - 텐서플로우 코리아 카카오뱅크 카카오모빌리티 네이버 채용전체 원티드 - 데이터 사이언티스트 사람인 - AI,빅데이터 잡코리아 - IT채용관 로켓펀치 - 데이터 사이언티스트 코멘토 - 데이터 사이언티스트 위시캣 프로젝트 데이터 사이언티스트 구직을 위한 조언 Site 변성윤 님의 AI채용 FAQ 이현성은 어떻게 취업을 하게 되었는가 구글면접후기 : 이분의 블로그도 굉장히 얻을 지식이 많습니다. 현직 데이터 사이언티스트 Site 변성윤님 블로그 데이터 사이언스 외국 커뮤니티 해외취업을 위한 조언 Site 해외인터뷰 경험 GMV 채용상담회 구인공고 서울-실리콘밸리 IT 커리어 포럼(2018.10.10, KOTRA) 육아 휴직이 만든 인생의 기회 - 기업가 김홍석님 [미국 이민] 한국에서 영주권 받고 미국 이민가기 - NIW](https://www.youtube.com/watch?v=RT1SHSvqEgo&amp;feature=youtu.be&amp;fbclid=IwAR0Maa3_8Fs-fKKr4wuQ0oDZzzZEt8zVmNPLexMb2iCPFWmt0RAMfcHu6nY) 나이많은 개발자의 해외취업기 실리콘밸리 연봉 비교 대학원 Site 강필성교수님 곽노준교수님 이혁모교수님 교육기관 Site 마이캠퍼스 기타 Site 대한민국 개발자 블로그 모음 : 대한민국 개발자 거의 다 모여있음. 어썸 데브블로그 : 위 개발자 게시물 전부 모임 어썸 데브블로그(메일링리스트) : 메일링리스트 구독을 원한다면 이메일을 등록하세요. 메일링리스트 개발후기 전세계 연봉순위 프리랜서 가이드라인 마인즈랩 작성중입니다. 본 포스트는 주기적으로 업데이트 예정입니다.",
    "tags": "favorites data scientist career recruitment job advice",
    "url": "/favorites/2019/09/05/favorites-ds-career/"
  },{
    "title": "[리뷰] 파이썬 날코딩으로 알고짜는 딥러닝",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"파이썬 날코딩으로 알고짜는 딥러닝(윤덕호 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 논문을 구현하는 방법 국내 딥러닝 서적 중 한 획을 그을만한 책 날코딩이라는데 어느 정도까지 밑바닥인가? 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 논문을 구현하는 방법 데이터 사이언티스트는 비록 하나의 단어이지만 업무 특성에 따라 필요한 핵심 역량이 매우 다양하다. 통계, Quantitive, 금융분석 등의 분야에서는 주로 Tabular 형태의 데이터를 바탕으로 수리통계학 기반의 모델링 설계가 핵심 역량이다. 반면, 딥러닝의 경우 주로 영상, 음성, 텍스트 위주의 데이터를 다루며 예측, 설명에 있어 가급적 사람이 개입하지 않고 머신에게 맡긴다. 그러다보니 수리통계학도 중요하지만 엔지니어로서의 역량 즉, 플랫폼 설계, 최적화, 프로그래밍 스킬 또한 중요시된다. 최근 스프트웨어 2.0과 같은 트렌드가 이슈가 되고 있는 것은 그만큼 딥러닝 기술을 다룰 줄 아는 프로그래머에 대한 수요가 증가하고 있음을 반증하는 예이다. 덕분에 TensorFlow, Keras와 같은 플랫폼이 생겨나고, 범용 프로그래밍 언어인 Python의 점유율이 증가하고 있으며, 직관적인 통계 해석에 강점을 둔 R 진영에서 조차 Tidyverse가 등장하게 된다. 이것도 부족한것인지 보다 속도를 높이고자 Julia가 조명받고 있고, 나아가 구글에서는 Swift와 같은 데이터 사이언스에 최적화 된 언어를 개발하고 있다. 심지어 여전히 C언어로 딥러닝 개발을 고수하는 업체도 많다. 타 산업 및 학문 분야는 논문을 기본으로 한 연구분야와 개발분야가 전통적으로 명확한 경계선을 갖는 편이었으나, 데이터 사이언스 분야의 경우 그 경계가 굉장히 옅어졌다고 생각한다. 워낙 신생 학문이기에 논문 등 연구업적의 축적은 부족한데 반해 산업분야엔 엄청나게 핫한 인기를 끌고 있어 논문에 등장하는 아이디어가 빠른 속도로 구현되고 있다. 이런 연구업적의 소모는 타 분야에 비해 분명 기현상이다. 데이터 사이언티스트의 정의에서 보둣이 이들은 초특급 인재들이다. 수리통계학을 필두로 한 학문의 깊이가 남다르고, 전문가 수준의 프로그래밍 스킬도 보유하고 있으며, 데이터 분석능력 및 비지니스 감각도 탁월하다. 이런 천재들이 연구결과의 소비 속도를 더욱 가속시키고 있다. 그리고 이 현상의 중심에 논문의 아이디어를 구현할 수 있는 능력이 있다. 한때 나프다(나는 프로그래머다)라는 프로그래머들에게 굉장히 인기가 많았던 팟캐스트가 있었다. 애청자로써 나프다가 종료될 때 그렇게 슬플 수 없었는데 다행히도 당시 데이터 사이언스 중심의 싸이채널을 담당하신 김진영 님께서 데이터 지능 팟캐스트를 운영하시면서 당시 필자에게 나프다의 대체제로 큰 위안이 되었다. 데이터 지능 팟캐스트 중 Naver Clova 개발자분들이 등장하신 회차가 있는데 김진영 사회자님이 독자들을 위해 “데이터 사이언스 채용에 합격하기 위해 알아두어야 할 기출문제”를 공개해달라고 위트있는 요청을 하셨던 기억이 난다. 클로바분들의 대답은 간단했지만 꽤 인상적이었다. \"최신 논문이 등장한 일자 - 해당 논문의 구현체가 Github에 올라온 일자 = 0에 수렴\" 위 공식을 보유한 능력자라면 채용하겠다는 힌트를 주셨는데 현업에 종사하는 분들의 조언만 들어도 논문을 이해하고 구현하는 능력이 얼마나 중요한 지 알 수 있는 대목이다. 텐서플로우와 케라스를 자유자재로 다루는 능력은 분명 중요하다. 특히 초보자라면 이해하지 못하는 수식에 사로잡혀 세월을 낭비하고 있기 보다는 해당 라이브러리로 시각적 결과를 확인하며 딥러닝이라는 숲을 이해하고 Top-Down 방식으로 논문과 수식에 접근하는 것도 분명 효과적인 학습법일 것이라 생각한다. 하지만 앞서 언급했듯이 데이터 사이언스 분야에서 언제 어떤 언어 또는 라이브러리가 Python이나 케라스를 왕좌에서 밀어낼지 모르는 급변하는 개발 환경속에 특정 언어 및 라이브러리에만 의존하는 것은, 빠르게 배우고 적용할 수 있는 인재를 원하는 이 분야에서 원하는 생존력을 갖췄다고 보기엔 다소 소극적이지 않을까? 비단 딥러닝의 분야만이 아니다. 모델링 분야도 수식을 이해하는 것이 어렵기는 마찬가지이다. 책 읽기나 강연을 통해 습득했던 방식과는 또 다른 시각인 코딩 구현 과정의 사고 속에서 수식이 더욱 잘 이해되고 오래 기억에 남지 않을까? 어떻게 Keras, Tensorflow 없이 밑바닥부터 날코딩으로 구현하냐며 도망만칠 것이 아니라, 진정한 고수라면 내가 모르는 블랙박스를 최대한 없애기 위한 과정이자 수단으로 날코딩을 즐기지 않을까 생각한다. 상기의 목적을 달성하기 위해 국내에서 집필된 단 한권의 책을 추천하라면 주저없이 본 책을 고르겠다. 아래 그림과 같이 순전파와 역전파를 구현한 후, 관련 수식을 깔끔하게 직관적으로 기술한다. 즉, 수식을 코딩으로 구현하는 방법을 배울 수 있고 논문에서 이해한 수식을 코딩으로 구현하는 능력이 배양된다. 이런 방식의 구현과 설명으로 최신기술인 GAN까지 설명이 이어진다. 국내 딥러닝 서적 중 한 획을 그을만한 책 본 서적에는 논문의 아이디어와 수식을 이해하여 코딩으로 구현할 수 있는 능력을 키울 수 있다는 점 외에도 몇가지 탁월한 장점이 더 있다. 딥러닝을 위한 객체지향 기법을 동시에 배운다. JAVA등의 언어로 객체지향을 객체지향답게 다룰 줄 아는 고수는 많다. 하지만 딥러닝을 객체지향으로 개발하는데 능숙한 고수는 흔치는 않을 것이다. 딥러닝이 각광받기 시작한 역사가 워낙 짧은 편이라 딥러닝 학습 자체에 집중하기도 급급한데, 객체지향 패턴을 입혀 솔루션 출시에 집중하는 것은 아무래도 시간 상 우선순위가 밀리기 마련이다. 이 책을 학습하다보면 별도 시간을 들이지 않아도 자연스레 객체지향으로 구현하는 방법을 익힐 수 있다. 더불어 아래 그림과 같이 상속 재활용 관계도를 제시하여 큰 숲을 보여주므로 현재 읽고있는 위치가 어디인지 명확히 인지할 수 있는 장점이 있다. 객체지향 구현을 통한 구성덕에 얻을 수 있는 또 하나의 장점이 있다. 코드를 상속, 재사용 함으로써 기존 코드를 반복하여 설명하지 않게되고, 지면의 양도 상당히 줄어들어 책의 가격에 부담을 주지 않는다. 즉, 학습이 입체적으로 이루어지고 반복적으로 복습하게 되며 단원별 핵심 내용에 집중할 수 있다는 장점이 있다. 예를들면, GAN을 배우는데 이전에 배웠던 단층 퍼셉트론에서 배웠던 역전파, 경사하강법의 설명이나 코드가 반복적으로 등장하게 되다면 핵심 논점이 흐려져 집중력이 떨어질 수 밖에 없게된다. 디테일한 구성까지 저자와 편집자의 배려가 정말 돋보이는 책이다. Python 다운 Python 기법을 배운다. Google Colab 환경설정 및 사용법 포스팅에서 언급한 바와 같이 딥러닝은 가뜩이나 귀족학문이라 돈도 없어 힘든데, 심지어는 바둑과 같이 신선놀음이기까지 하다. 배우고 또 배우고 날밤을 새며 배워도 시간이 늘 부족하다. 그래서 자연스레 객체지향 기법이나 Python과 같은 엔지니어 스킬 측면의 내공을 뒷전으로 미루게 되기 마련이다. 이 책에서는 그런 부수적인 지식에 대한 부족함으로 드는 불안감을 안정감있게 채워준다. 예제를 실습하다보면 객체지향은 물론이고, 전문가를 위한 파이썬, 파이썬 코딩의 기술등 양서에 나오는 Python을 Python답게 쓸 수 있는 스킬도 자연스럽게 늘어난다. 직접 책을 따라 코딩하다보면 파이썬에선 Global 변수를 함수 내에서 이런 방식으로 전달하는구나, 함수의 리턴값 중 필요없는 값은 _키워드로 불필요 인자를 지정할 수 있구나, 행렬 + 벡터합은 자동으로 반복 연산이 지원되는 구나… 등의 지식을 딥러닝을 공부하며 자연스럽게 익히게 된다. 만약 본인이 비 프로그래머 연구자 출신이라면 이런 장점이 더욱 도움이 될지도 모르겠다. 논문지식을 통해 객체지향과 엔지니어링을 역으로 배우는 신기한 경험을 하게 될 것이다. 이미지 및 삽화가 매우 설명력이 좋다. 설명력이 좋다고 표현한 것은 통계, 머신러닝의 용어를 빌렸다. 이미지 하나만 깊이있게 들여다봐도 해당 챕터의 큰 그림이 그려지기 때문이다. 아래 그림들을 보면 무슨의미인지 직관적으로 와 닿으실 것이다. 참고로 저자가 구현한 소스코드를 확인하고 싶다면 저자의 GitHub에 접속하시기 바란다. 날코딩이라는데 어느 정도까지 밑바닥인가? 밑바닥의 수준을 파악하는 방법은 쉽다. import 모듈 외에는 모두 직접 개발한다고 간주하면 된다. 책의 222p에는 mlputil.ipynb라는 파일이 등장하는데 재사용을 극대화하기 위해 Python에서 필요한 라이브러리들을 공통으로 Import하는 파일이다. 이 파일 외 더 이상의 import문은 등장하지 않는다. 그렇다면 import하는 라이브러리는 무엇일까? 파일 입출력, time, numpy 정도다. Pytorch, Scikit-learn, Pandas는 물론 심지어 Scipy도 안쓴다. 이 말인 즉슨 순전파, 역전파는 물론 sigmoid, softmax 같이 단순한 것들도 모두 구현해야 한다는 의미다. 범위가 딱 적정하다는 생각이 들었다. 파일 입출력을 구현하는 것은 딥러닝과는 무관한 영역이고, numpy 내부를 구현하는 것 또한 딥러닝의 범위라고 하기엔 너무 디테일하다. 예를들어 exp와 같은 기초 수식까지 구현하는 것은 시간낭비가 심하고 내용이 너저분해질 수 있다. 적당한 선에서 끊어 투입 시간대비 능률적인 성과를 올릴 수 있도록 안배한 저자의 능력이 돋보인다. 누가 읽어야 하는가? sw 2.0 프로그래머 : 케라스, 텐서플로우 등 프레임워크 개발, 사내 자체 딥러닝 솔루션 개발, 성능 이슈로 C, Swift 등 속도 빠른 타 언어 구현, 논문 구현시 큰 도움이 될 것이다. 애널리스트 : 부족한 프로그래밍 스킬을 채우고 모델링 이해에 도움이 되는 수식, 논문 아이디어에 대한 습득 속도가 향상될 것이다. 데이터 엔지니어 : 코드에 따른 최적화된 자원 활용에 도움이 될 것이다. 사이언티스트 : 모델링 지식을 복습하고 성능을 보장하는 설계에 도움이 될 것이다. 기타 딥러닝, Python, 논문, 객체지향, 프로그래밍 스킬 등을 배우고 싶은 학생, 실무자, 경영자 등 모든 분들께 강추한다. 책의 구성 및 요약 이 책은 크게 다섯 부분으로 구성되며, 각 파트에서 다루는 내용을 요약해 보았다. 1. 단층 퍼셉트론(0 ~ 3장) 딥러닝의 발전과정 및 딥러닝 수학 등 개요 딥러닝의 핵심 기초개념(경사하강법, 역전파, 편미분, 손실함수, 원-핫벡터 등) 회귀분석, 이진판단, 선택분류 구현 실습 2. 다층 퍼셉트론(4 ~ 6장) 입력층, 은닉층, 출력층 구현 가변적 은닉 계층 구성을 위한 파라미터 생성함수, 순전파, 역전파 심화 미니배치, 평가, 시각화, 정확도 계산, 각종 수학연산 함수 정의 등 아담모델, 오피스31 다차원 분류 기법 등 3. 합성곱신경망(7장 ~ 9장) 합성곱 계층, 풀링 계층의 역전파처리 과적합, L1/L2손실, 드랍아웃, 정규화 등 인셉션, 레스넷 모델 심화 구현 4. 순환신경망(10장 ~ 12장) 기본 셀 순환 신경망, LSTM 구현 CNN과 RNN의 결합, 시계열 포장 등 5. 고급 응용 신경망 구조들(13장 ~ 15장) 오토인코더, 시맨틱 해싱, 인코더-디코더 모델 생성적 적대 신경망(GAN) 등 요약하며… 이미 언급한 바와 같이 딥러닝 분야에 있어 우리 나라에 한 획을 그을만한 멋진 책이라 생각한다. 논문에 등장하는 수식과 아이디어를 정말 제대로 이해할 수 있게 끔 Python 코딩을 통해 구현하는 능력을 키워준다. 딥러닝 논문을 읽고 구현할 수 있다는 자신감이 생긴다. 객체지향 기법으로 딥러닝 솔루션을 구축하는 방법, 그로인한 챕터별 핵심에 집중할 수 있는 입체적인 설명, Python답게 Python을 다루는 방법, 핵심만을 다룬 깔끔한 수식설명과 설명력 있는 삽화까지 지금까지 나온 국내 딥러닝 서적중 최고의 점수를 주고 싶다. 책을 읽으면 아쉬운 점을 보통 한두개 정도는 찾게되기 마련인데, 이 책은 도통 찾기가 어렵다. 수준이 제법 높아 입문자 분들께 벅찰 수 있겠다는 정도(?)이다. 컬러판으로 소스코드에 Syntax Hilight가 입혀져 있었다면 바랄 나위 없었겠지만 그러면 또 정가가 오르지 않겠는가? 이미지는 워낙 설명력이 좋아 컬러일 필요도 못느꼈다. 실습환경을 구축하기 어려울지 모르겠는데 그런 분들은 필자가 정리한 Google Colab 환경설정 및 사용법 포스팅을 참고하시기 바란다. 결론은 10점 만점에 10점이다. 좋은 책을 세상에 선보여 주신 저자, 편집자, 출판사께 감사드린다. &lt;한빛미디어 출판사&gt; 믿고보는 “한빛미디어 출판사”. IT분야에서 독보적인 양질의 도서를 출판하는 회사입니다. “나는 프로그래머다” 팟캐스트 후원, DevGround2019 행사, 리뷰어 모집, 다양한 학습 지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT분야에 관심 있으시다면 한빛미디어의 책으로 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review book deep learning python coding",
    "url": "/review/2019/09/04/review-book-dl-nal/"
  },{
    "title": "[Paper] 논문 읽는법, 쓰는법, 투고하는법",
    "text": "개요 논문과 연구에 거리가 멀어진 셀러던트 직장인이 저처럼 늦깍이 나이에 연구에 철이 들어 논문에 관심이 많아지셨다면 본 포스팅을 꼭 읽어주세요. 학사출신 실무자가 논문을 잙읽고, 잘쓰고, 잘투고할 수 있도록 보고 배운 노하우를 공유합니다. 목차 논문. 연구. 늦었지만 하지 않을 수 없는… 논문을 잘 읽는 방법 논문과 관련된 유용한 필수지식 논문을 잘 쓰는 방법 논문 투고와 심사 논문의 인용수와 질적관리 논문과 영어 (참고) 내용이 상당히 깁니다. 즐겨찾기를 원하실 경우 목차를 클릭 시 변경되는 URL로 관심 위치까지 같이 저장하시기 바랍니다. 논문. 연구. 늦었지만 하지 않을 수 없는… 필자는 연구자도 아니고 석.박사 학위보유자도 아닌 그저 약 10여년간 실무에 몸담아온 직장인으로 학사출신 프로그래머라는 점을 먼저 밝혀둔다. 그런 사람이 왜 논문에 관심이 있는지, 아니 이런글을 쓸 자격이 있는건지 물으실 수도 있겠다. 부끄럽지만 굳이 하나의 자격을 찾자면 필자처럼 연구에 늦게 철이 든 사람을 위해 조금이나마 도움이되지 않을까하는 기대 정도다. 다행인 것은 세월이 흐르다보니 나이를 먹었고 덕분에 주위에 아는 연구자분들이 점점 늘어나게 되었다. 그 분들의 조언 그리고 인터넷을 통해 다양한 분들이 각자의 노하우와 견해를 공유해주시는 바 이구동성의 중요한 조언을 모아 본 포스트를 정리하였다. 데이터 사이언티스트에 관심이 있는 필자에게 논문이란 단순히 관련 직군의 최신 기술을 얻기 위한 정보로서의 가치 뿐만 아니라, 세상에 쌓여온 지식과 실험을 바탕으로 유의미한 인사이트를 얻기 위한 아이디어를 얻는 과정을 위한 스킬을 얻을 수 있다는 점에서 반드시 짚고 넘어가야 하는 산이다. 더불어 필자가 논문에 큰 의미를 부여하는 이유는 일종의 세상을 발전시키데 가장 효율적인 의사소통 수단으로서의 기능 때문이다. 세상 지식에 경계선이 있다면 그리고 그 최전선을 확장시키는 것에 인생의 보람을 느끼는 분이라면 논문과 연구의 중요성은 두말할 나위없다. 보다 자세한 의미는 이미 충분히 표현한 바 인사이트(Insight)! 다시 기본으로 포스팅으로 갈음한다. ※출처 : PhD pitfalls: Part I – The reality of your contribution 이제야 겨우 인생에 목표라는 것이 생겼는데 그러기 위해 대학원을 다녀야 하는데 그러려면 돈을 벌어오기는 커녕 되려 돈을 써야한다. 처자식을 먹여살려야 하는 현실속에서 퇴근 후 시간과 주말에 매진할 수 밖에없는 필자와 비슷한 늦깍이 셀러던트 분들께 조금이나마 보탬이 되었으면 좋겠다. 논문을 읽고 쓰는 과정은 꾸준히 노력하고 매진하면 언젠가 기회는 온다는 점에 의미를 두고싶다. 비슷한 처지에서(실은 훨씬 더 가혹한) 본인이 가고 싶은 길을 꿋꿋히 달려오신 경희대학교 동서의학대학원 박은정 교수님의 일화는 필자에게도 큰 힘이 된다. 인생의 목적이 분명 부와 명예만은 아니므로 가고싶은 길이 생겼으면 열심히 걸어가보려한다. 육아와 가족들의 간병으로 20여년의 시간을 뒤로 한 경력 단절 아줌마께서 클래리베이트 애널리틱스(구 톰슨 로이터)가 선정한 연구성과 세계 상위 1% 연구자(HCR)에 2번이나 오르시고 ‘지식창조대상 장관상’을 수상하셨으며 유명대학 정교수에 이르기까지의 멋진 과정을 읽으며 용기를 얻으셨으면 한다. 그 외 연구실 셋방살이, 한국연구재단의 ‘대통령 포스트닥 펠로우십’ 등의 일화를 통해 힘없는 아웃사이더가 연구계의 인싸가 되기 위한 노하우들도 얻을 수 있을것이다. 논문을 잘 읽는 방법 세상에 읽어야 할 논문은 정말 많다. 어떻게 읽어야 빠르고 정확하게 이해할 수 있을까? 역지사지(易地思之)라 했다. 가장 확실한 방법은 직접 당사자가 되어봐야 한다. 즉, 논문을 쓰고 심사하는 저자와 심사자 입장이 되어보는 것이다. 여기에선 모든 학문범위를 다룰 수 없는 관계로 필자가 관심있는 머신러닝 분야를 사례로 다뤄본다. 1. 저자에게서 얻는 착안 저자 입장에서 생각해보자. 내가 논문 저자라면 연구성과를 명확히 표현함은 물론 심사과정에서 Accept를 얻어내기 위해 최대한 가독성있게 요약 전달하는 부분을 작성하게 될 것 같다. 독자는 그 부분 먼저 읽어야 하지 않을까? 정리하는 과정에서 Andrew Ng 교수의 조언이 많은 도움이 된다. 저자의 시각과 더불어 도움이 될 사항을 아래와 같이 정리해보았다. 앤드류응 - 스탠포트 CS 230 강의을 참고하시기 바란다. Review 논문 Review 논문이란 관련 연구분야를 집대성하여 요약한 논문으로 교과서 반영 직전 단계라 말할 수 있을 정도로 연구성과가 잘 정리되어있다. 즉, 관심있는 연구분야가 생겼다면 먼저 Review 논문을 파악함으로써 연구분야에 대한 논문 간 관계도 및 목차를 구성할 수 있게된다. Nature지의 경우 최근 핫토픽을 정리하는 형태로 자주 올라온다. 참고로, Article은 좀 더 자세한 최신 연구를 다루며, Letter는 좀 더 양이 적은 최신연구를 다룬다. 논문의 목록을 정리 : Review 논문을 통해 읽어야 할 논문 목록을 수집한다. 취사선택 : 목록에서 자세히 읽을 논문과 버릴 논문을 선정한다. 수집한 논문별로 초록(abstract), 도표(figures)를 먼저 읽는다. 기본적인 컨셉과 아이디어를 기술하여 전체 맥락을 잡을 수 있다. 다음으로 도입(introduction), 결론(conclusion), 도표(figures)를 읽는다. 왜 자신의 논문이 게재 승인되어야 하는지 명확히 설명하기 위해 신중하게 요약한 핵심정수가 담겨있다. 잘 읽었는지 확인하기 Check-List 저자가 뭘 해내고 싶어했는가? 이 연구의 접근에서 중요한 요소는 무엇인가? 스스로 이 논문을 이용할 수 있는가? 참고하고 싶은 다른 레퍼런스에는 어떤 것이 있는가? 추천하는 리딩 습관 읽는데 걸리는 시간 : 관련 연구자 기준 보통 1-3시간 한 주에 2개씩 꾸준히 알고리즘 : 수식을 직접 쓰면서 이해하고 밑바닥부터 코드로 구현 수식 : 직접 손으로 연산 논문 정독횟수에 따른 ML/DL 지식수준 5 ~ 20개 : ML/DL 시스템을 적용할 지식은 갖추었지만 최신의 기술을 이해하기에는 부족하다. 50 ~ 100개 : 해당 분야에 대해 자세히 알고 있다. 스터디/커리어의 방향 배울 수 있는 팀으로 : 성장에 도움이 된다. 좋은 프로젝트(직장) 선정 : 세상을 발전시킬 수 있는 가치있는 프로젝트 ML/DL을 다른 산업에 적용 : 헬스케어, 천문학, 기후 등 추천 커뮤니티 Site ML subreddit ML/DL 컨퍼런스 : NIPS / ICML / ICLR 2. 심사자에게서 얻는 착안 이번에는 심사자의 입장에서 생각해보자. 각 저널에는 논문 심사를 위해 저명한 교수님들로 구성된 Reviewer Pool이 있다. 명망이 높고 활동도 활발한 분들인지라 대부분 일반인들보다 24시간의 밀도가 꽤나 짙을 것이다. 따라서 심사에 많은 시간을 할애하길 원치는 않을 것이다. 즉, 그분들은 그동안의 연구 경험을 바탕으로 한 빠른 Accept/Reject 선별 기술이 있을것이다. 이를 바탕으로 심사자의 입장이 되어 그 분들의 눈이 되어보자. Cover Letter 및 Abstract 본 연구는 기존 연구와 이런점에서 다르다. 이와 같은 새로운 사실 또는 개선점을 밝힌다. 위와 같은 형식이 없다면 Reject 가능성이 커진다. 논문을 위한 논문일 수 있다고 선입견을 가지게 되는 것이다. 우리는 이런 형식으로 언급된 부분을 먼저읽어 양질의 논문인지, 원하는 내용이 있는지 미리 가늠해봐야 한다. 심사기준으로는 크게 독창성(Originality), 참신성(Novelty), 혁식적(Innovative) 3가지 기준이 중요한데 비록 기존에 잘 알려진 사실일지라도 해석이나 분석의 방법을 새롭게 하여 차별화된 설명, 혹은 결론으로 이끌 수 있다면 Accept되는 경우도 있다. 다만 이런 유형은 충분히 의미는 있겠지만 공부하는 우리 입장에서는 일단 차순위로 미뤄두는 것이 옳다고 본다. Introduction 이 부분은 연구배경, 목적, 필요성, 기존 연구와의 차별성이 보다 자세히 강조된다. 참고문헌의 약90 %가 참조되는 영역이므로 참고문헌의 갯수가 적은 논문은 사전조사가 부실하게 여겨질 가능성이 크다. Clarivate Analytics(구 톰슨 로이터)의 저널 평가지표 지금까지 논문 평가에 대한 혜안을 빌려왔다면 저널에 대한 평가를 알아보고 싶을때 저널 평가지표가 도움이 된다. 저널의 IMPACT FACTOR(인용률), Rejection Rate 등의 수치 저자들의 국제성, 즉 여러 국가와 기관에서의 투고율 기타 심사자도 사람이므로 논문을 바라보는 시각과 기준, 각자의 논리 성향이 다양할 수 있음을 염두에 두어야 한다. 저널에 실렸다고 완벽하다고 반드시 읽어야 한다고 판단하는 우에서 벗어날 수 있고, 더불어 작성시에는 충분히 참신하고 자신 있음에도 Reject될 경우 다른 저널에 게재하는 등의 방법을 시도할 필요가 있다. 이와 같이 작성자와 심사자의 입장이 되어 논문을 읽는다면, 입체적으로 비판적인 시각으로 보다 빠른 이해에 도움이 될 것이다. 논문과 관련된 유용한 필수지식 먼저 논문 및 그 Eco환경에 대해 전혀 모르는 분들은 반드시 본 파트에 정리한 사항들을 먼저 읽어주시기 바란다. 그동안 궁금했던 것들이 한번에 뚫릴것이다. 1. 논문의 구성요소 논문을 이루는 큰 구성요소들을 친숙한 말투로 정리해보았다. Title : 우리가 한게 뭐다 Author : 누가썼다 + 저자의 소속 기관 Abstract : 요약 : 본 연구는 기존 연구와 이런점에서 다르다. 이와 같은 새로운 사실 또는 개선점을 밝힌다. 판매를 위한 광고용으로도 쓰일 수 있기 때문에 독립된 글로 봐야 한다. Introduction : 우리가 한게 뭐다. 왜 중요하냐면, 본 연구는 기존 연구와 이런점에서 다르다. 이와 같은 새로운 사실 또는 개선점을 밝힌다. 참고문헌을 보면 과거에 어떤 사람들이 이런 시도를 했으나, 이 부분이 부족해서 어떤 아이디어를 가지고 어떤 실험을 했다. Result&amp;Discussion : “실험결과에 대한 논의 후 이런 결과가 나왔다” 반복 Conclusion : 우리가 이런 결론을 얻었다. 미래에는 뭐가 중요하겠다. 이 기술이 기존과 어떻게 다르고 어떤 장단점이 있다. 주로 이런 의미이고 이게 왜 중요하다. Method : 논문에 활용한 실험 방법, 테크닉 ex) 내가 세포를 어떻게 키웠는지, 어떤 수식으로 분석했는지, 어떤 장비로 측정했는지… 등 Acknowledgements : 연구비 누구한테 받고, 누가 도와줬고, 저자 중에 누구는 특허권자다 저자 중에 누구는 연구비를 받고있다 명시(공정성을 위해) References : 타 논문 인용 명시, 과거 언급, 기술 2. 논문 및 저널의 등급 일반적으로 우수한 논문을 판단하는 기준은 IF가 높은 저널에 등재되었는지 여부와 피인용횟수가 높은 논문(일반적으로 2-3년은 지나야 쌓임)이라 할 수 있겠다. 일반적으로 인식되는 저널의 등급을 알아보자. NCS(Nature, Cell, Science) N,S는 대중적 과학전문잡지(짧은논문) C는 전문학술지(의학분야, 전문(全文)) SCI(Science Citation Index, 과학인용색인), SSCI(Social Sciences Citation Index, 사회과학분야), AHIC(Arts&amp;Humanities Citation Index, 인문예술분야) 역사가 더 깊고 개발도상국 등 IT인프라 열악한 국가의 접근성을 위해 유지되고 있음 SCIE(Scienece Citation Index Expanded) Journal information 매체형태 : SCI는 CD/DVD ↔ SCIE는 on-line SCOPUS 네덜란드 엘스비어(Elsevier) 출판사의 학술연구논문 인용 데이터베이스, 전세계 5천여개 출판사의 21,000여 종 저널타이틀 수록 비영어권 모국어도 등재 기회를 줌 제목, 저자, 요약, 키워드, 그림, 테이블, 참고문헌은 영어로 기재(인용문제) KCI(Korean Citation Index, 한국연구재단등재지) 국내 우수논문의 해외유출 및 우리말 품격 유지를 위한 한국형 인용색인 DB 및 인용지수 개발, 운영 대학원생들이 본격적으로 해외저널에 논문을 투고하기 전에 논문투고의 경험을 쌓기 위한 연습용으로 전락했다는 비판도 있다. KCI Nominated(등재후보지) 학술지 국내에서 1인당 논문수를 계량화하여 인정함에 따라 논문의 양이 중요해지면서 다양한 학술지가 등장함 등급제 영향으로 군소학회, 대학 연구소까지 등재학술지를 발행함 연구업적 평가에서 좋은 점수를 받기 때문에 논문게재를 편하게, 쉽게하기 위한 편법으로 활용되는 단점이 있다. 어떤 저널에 투고해야 할까? 저널 홈페이지 Author’s Guide를 반드시 확인 : 저널이 취급하는 연구분야의 주제인 Scope이 다름. Scope에 포함되어있지 않다면 Scope에서 기술된 주제와 어떤 연관이라도 있는 주제임을 억지로라도 이끌어 내야함. Cover Letter에 반드시 적어야 하는 내용이기 때문 Cover Letter와 투고논문의 Abstract 및 Keywords를 보고 판단하는 것이 일반적 3. 교원이 되고 싶다면 공동저자는 큰 의미가 없음 배점, 가중치가 낮음 정성적으로도 심사자들에게 좋은 인식을 주지 못함 SCI, SCIE급 단독저자 2~3편 가지고 있는 경쟁자에게 밀리기 쉬움 제1저자(First Author), 아니면 교신저자(Corresponding Author)로서의 논문이 많아야 함 과정 중 학생은 교신저자는 거의 100 % 지도교수이므로 제1저자 논문이 많아야 함 논문과 관련된 교수님의 역할 Advise, Supervise, 제1저자, 교신저자, 공동저자 Thomson Reuters의 Journal Citation Reports(JCR) 상위 10% ~ 20% 논문은 가산점이 부여되는 추세 인력, 연구장비, 실험장비, 환경은 핑계일 뿐 없으면 없는 대로 아이디어로 승부를 봐야 함 Review Article, Book Chapter, Letter to Editor 등은 연구업적에 포함되지 않는 경우가 있음 Original Research Article 위주로 인정되는 분위기 박사 후 포닥 및 정출연 등에서 연구과정을 거침 보통 3년 ~ 5년 간 대학에서 포닥이나 정출연 등에서 연구과정을 거침 3년 ~ 4년 내 SCI, SCIE급 저널논문 10편 정도는 투고하기도 함 초빙에 대한 현실적인 비판 인맥, 학벌 문제로 서류심사 시 전공적합도라는 편리한 항목으로 걸러지기 쉬움 면접도 마찬가지로 나이, 인성, 발전성 등의 주관적 항목에 의해 걸러질 수 있음 명문 일부의 대학을 제외하고는 논문인용수를 많이 따지지 않는 편 극소수 대학들은 일정기간(보통 최근의 2년 ~ 3년)동안 연구업적 외에 발표기간에 관계없이 대표논문을 요구하기도 함 발표논문수도 중요하지만 발표논문의 질도 평가하다는 의미 임용 후 현실에 대한 비판 학생지도, 과제수주, 연구비 비용처리, 서류작성, 행정업무, 학내 파워게임(흔히들 안하면 바보가 된다고 표현) 외부강연, TV출연, 보직(장.차관, 정출연기관장 등)관련 공부 내지는 연구할 시간이 없다. 상대보다 더 잘해서 이기는 것보다는, 깍아내려서 동등한 수준으로 만드는 것이 더 쉬운 잔혹한 현실 논문을 잘 쓰는 방법 논문의 정의를 사전에서 찾아보면 연구과정을 과학적인 방법에 따라 전개한 체계적인 글임을 알 수 있다. 먼저 논문의 작성 절차를 정리한 후, 잘 쓰기 위한 Tip을 정리해본다. 논문 작성절차 무엇을 연구할 것인가? 참고문헌의 고찰 부실한 검토를 피할것 : 유사논문이 되거나 의도치 않은 표절이 발생할 수 있음 독창성(Originality), 참신성(Novelty), 혁신적(Innovative)이 필요 가독성, 재현성, 정확성, 검증성이 갖춰져야 함 타당한 문제 해결방법 구상 (객관성 입증을 위한) 실험 혹은 이론적 해석 문제해결 및 결과도출 고찰 및 요약 좋은 논문을 작성하기 위한 Tip 기존 연구와의 차별성 독창성(Originality), 참신성(Novelty), 혁식적(Innovative) 중 하나만 만족해도 SCI급 기존 연구에서 부족한 부분 혹은 간과되었던 부분을 새롭게 조명한다거나 틀린 부분을 밝힐 수 있어야 함 변수를 바꾸거나 새로운 시각으로 접근하여 “참신”하거나, “어!! 이렇게도 해석할 수도 있네..”라고 느낄 수 있어야 함 접근방식에 대한 사고 방법1 : 예상 &gt; 확인을 위한 실험 &gt; 얻어진 결과를 바탕으로 논문 작성 방법2 : 아이디어로 초안작성 &gt; 결론, 참고문헌까지 먼저 작성 &gt; Results and Discussion(Verification) 채우기 ex) “왜 지금까지 발표된 방정식은 유체역학의 정수력학(Hydrostatic theory)에서 시작하지?”, “열역학의 이상기체 상태방정식(Equation of the Ideal Gas Law)으로도 충분히 유도할 수 있을 것 같은데…” 시각화와 가독성을 위한 배려 포맷(Format)을 반드시 확인 SCI/SCIE급에 포함된 논문집의 경우에는 자체 논문양식을 제공하므로 양식에 맞춰 작성 Elsevier이나 Springer 계열의 경우에는 논문양식을 제공하지는 않지만 논문의 작성순서, 그림, 테이블, 참고문헌, 글씨체의 크기 및 종류, 단락의 칸 등 형식에 대한 작성법을 지정하고 있기 때문에 요구하는 양식을 맞추어 투고해야 함 - Excel Graph 등 저품질의 시각화는 지양하고 높은 해상도를 유지할 것 - 수식이 길어져 생략하는 경우 투고시 Supplement로 따로 보내면 논문심사자에게 호감을 줄 수 있다. 기타 알베르트 아인슈타인의 조언 지금까지의 누군가 해왔던 동일한 생각으로 접근하면, 어떤 해결 방법도 찾을 수 없다. 모두가 비슷한 생각을 한다는 것은, 아무도 생각하고 있지 않다는 말이다. When all think alike, no one thinks very much. Rejection을 피하기 위하여 Peer Reviewer는 전문가이므로 본인이 이미 다 알고 있는 것들이라면 차별성이 없다고 판단 최근 5년 이내 Reference가 없으면 Rejection 가능성이 커짐 Revision이 통보되었으나 본인의 견해가 옳다고 생각할 경우 대응 예시 “심사위원님 말씀대로 모든 과학적 연구가 실험에 바탕을 두어야 한다면 과학자들이 마주치는 많은 난관을 돌파하기 위해 시도하는 새로운 접근이나 아이디어는 사장될 것이다.” 수식에 가독성이 없어서 어렵게 보이도록 작성하면 Rejection 가능성이 커짐 논문 전체의 내용과 결론이 일치하는지 확인 아무리 실험 논문이지만 공학 논문에서 수식이 하나도 없는 것은 이상하다. 관련 수식을 삽입하고 가능하다면 이론값과 비교하거나 정성적인 해석을 추가하라. 모든 노력을 기울였으나 Rejection된 경우 Reviewer Comment를 잘 활용할 것(실패해도 얻을것은 분명 존재한다.) 참고로 Elsevier 계열의 저널은 3주 ~ 4주 내 Reviewer Comment(심사평)을 작성하므로 늦어지면 문의해 볼 것 Revision, Reviewer Comment의 유형 독자들이 쉽게 이해하기에는 한계가 있을 것 같다. 독자들이 당신 연구에서 적용한 계산을 보다 잘 이해할 수 있도록 계산절차를 설명하는 차트와 그래프를 추가해서 다시 보내라. Major Revision!! 제법 괜찮은 것 같다. 그런데 당신의 연구에 베르누이방정식을 적용해서 계산한 것이 부적절한 것 같다. 이에 대한 설명을 추가해서 다시 보내봐라. Minor Revision!! 문장의 삭제, 수정 검토는 쉬운편 ↔ 방정식이나 그림, 테이블, 논문의 작성순서 등은 많은 시간이 소요됨 논문 투고와 심사 심사절차 보통 4개월 ~ 6개월 정도 소요. (길면 2년 가까이 걸리는 경우도 있을정도) 전자투고시스템(Electronic Editorial System)에 투고 : [Submitted to the Journal] Editor-in-Chief에게 전달 : [Editor Invited ↔ Desktop Rejection, Under Review] Desktop Rejection = Re-submission 30 ~ 40% 이탈. Science지의 경우 90% 이탈 저널이 취급하는 범위와 맞지 않거나 심각한 결여의 결과 한 달에 적으면 200편 많으면 약 300편 정도의 논문이 투고(하루에 7~10편 정도) 시간 제약으로 인해 Abstract, Conclusion만 읽는 경우가 많음 Instruction까지만 읽어줘도 감지덕지 =&gt; 반드시 타 연구와의 차별성 기술이 필요 NCS는 훨씬 엄격하다. 우리 저널의 독자가 별로 흥미를 가질 것 같지 않다. 우리 저널은 임팩트있고 과학계에 지대한 공헌을 할 수 있는 최신 연구내용만을 다룬다. Associate Editor에게 전달 : [With Editor ↔ ] 투고논문에 논문번호가 부여 동일 연구분야에서 특정주제에 대한 유명한 전문가, 학자들이 평가 Peer Reviewr에게 전달(3~5명 / 1~3개월소요) 심사결과확인 : [Rejection, Major Revision, Minor Revision ↔ Required Review Completed] 수정 후 확인 : [Required Review Completed] Accept 후 DOI 등 Volume No.와 Page가 없다면 정식 출판논문으로 여기면 안됨 연구업적기간 및 온라인 출판 연구논문의 인정시기에 대해 엄격히 규정 동료심사(Peer review) Pool에서 투고논문의 심사에 적합하다고 판단되는 Reviewer에게 심사를 의뢰 편집장과 편집위원들로 구성된 편집위원회에서 추천하여 Pool에 등록시키거나, 논문을 최초로 특정저널에 투고할 때 저널측에서 투고자인 교신저자에게 Reviewer로서 활동할 수 있느냐고 문의 전자투고시스템에 투고할 때 일반적으로 교신저자가 선호하는 Reviewer 3명을 추천 Review시에 이 사람만은 피해달라는 심사자의 기피명단을 요구하기도 함 =&gt; 참고문헌의 저자 중 한분을 요구하기도 함 전문적인 지식을 가진 겸손한 Reviewer를 만나야하는데 결정할 수 있는 상황이 아니므로 복불복 심사의견(Reviewer Comment) 작성을 위해 조상부터 알고 있었던 모든 지식을 동원, 관련 논문 다 찾아봐야 함 심사결과도 해당저널에서 평가함 / 저널의 심사탈락율도 저널평가시 중요 지표임 논문투고할 때는 해당저널의 편집위원 구성을 한번쯤 참고하는 것이 좋다. SCI나 SCIE로 등재된 국내저널(물론 영어로 적어야 하고, 투고-심사-수정-게재 혹은 게재불가의 모든 절차가 영어로 진행됨)의 경우는 그들만의 리그인 경우가 많다. 편집진을 보면 90 %가 한국인, 어쩌다가 외국인 1~2명이 ASSOCIATE EDITOR(부편집인)으로 구성. 투고논문 저자들의 소속이 좀 빵빵한 곳은 논문게재가 쉽다. 해외저널도 그런 경향이 좀 있지만 국내저널의 경우는 좀 심함. 교신저자가 세계적으로 알아주는 대가(??)이거나 특정저널의 EDITOR급이면 게재확률은 승률 80 % 이상 심사위원의 유형 영어문법을 중시하는 사람이 있을 수도 있고, 투고논문의 양식이 저널의 요구에 일치하는 지를 볼 수도 있고, 투고논문의 Cover Letter, Abstract, Conclusion만 읽어보고 논문의 가치를 판단하는 사람이 있고 천차만별 저널의 Scope에 맞지 않으면 가차없이 Rejection, 투고논문의 양식이 저널의 요구사항에 맞지 않거나 영어가 맘에 안들거나해도 바로 Rejection 논문의 인용수와 질적관리 Open Access Article Accept후 누구라도 저널의 홈페이지에서 자유롭게 다운로드 받을 수 있는 서비스 보통 800 ~ 1200달러의 비용 지불 Google Scholar 검색 시, pdf 파일이 있다고 할지라도 저자들의 저작권은 이미 출판사가 가지고 있기 때문에 저작권 문제를 염려하여 저자들은 보통 투고시의 심사용 원본파일을 올려놓음. 따라서 저널명, 볼륨, 페이지가 없음 대학이나 연구기관의 경우 저널의 각 출판사들과 년간 일정비용을 지불하고 전자저널시스템을 구축하고 있기 때문에 기관의 소속원이라면 자유롭게 이용할 수 있기도 함. 개발도상국 같은 곳의 연구자들은 IT인프라 전자저널시스템과 같은 것을 이용할 수 없는 환경이 거의 대다수. 그 곳의 연구자들은 어쩔 수 없이 “Open Access Article”의 논문을 많이 참조하고 참고문헌으로 기술. 시스템이 연구자의 피인용수를 올리는 편법인지, 연구자가 비용감수를 불구하고 연구생태계의 성장을 위해 타연구자들에 대한 기여인지 각각 장,단점이 존재함. 논문과 영어 Reviewer Comment를 받아보면 은근히 영어에 관련된 내용이 많다. 예를들면, “당신히 작성한 영어는 도저히 이해가 안된다. 문장에 약간의 실수들이 있다. 하지만 연구내용이 독창적이고, 관련분야에 기여도가 클것으로 판단한다. 내가 지적한 영어 문장들만 좀 수정해서 다시 보내라.” 같은 첨삭을 받게된다. 이렇듯 영어로 발생한 문제를 해결하기 위한 영어 기반의 탬플릿 또는 예시를 Tip으로 정리해본다. Cover Letter 작성 예시 Cover letter for submission of our original paper to 저널명 교신저자명 저자주소 Date: Dear Sir, I wish to submit a new manuscript entitled “논문제목” for consideration by “저널명”. I do confirm that this work is original and has not been published elsewhere nor is it currently under consideration for publication elsewhere. In this paper, I report on the performance improvement of the MSF seawater desalination process. This will be significant because the production cost of fresh water can be reduced by the brine re-utilization of the upstream evaporators. The paper should be of interest to readers in the areas of process and energy saving. I am sure that the submitted paper will provide very useful information to the engineers related with the vacuum distillation and multi-stage evaporation process. &lt;/u&gt; Thank you for your consideration of this manuscript. Sincerely, 교신저자명 장시간 심사 Status가 변하지 않을때 상태확인을 요청하는 메일작성 예시 Dear Dr. ooo. I, along with my co-authors, submitted the manuscript (Manuscrpt #) entitled “(Manuscript Title)” for publication in (Journal Title) as an original paper in Oct., 13, 2016 (Submission Date). Today, we realized that current status/date of the manuscripts has not been changed since we initially submitted the manuscript. We are very concerned about the delay of the review process by an error in the review and tracking system. We would like to ask you to confirm current status of the manuscript. We look forward to hearing from you. Review Comment 예시 Reviewer Comment on the manuscript: Heat Pump Seawater Distillation System Using Passive Vacuum Generation System (Manuscript Number: DES-D-16-00704) The submitted article requires a lot of time and patience to the reviewer. In the scientific or engineering article, the perfect English is not important Although some mistakes in English writing are done in the manuscript it will be good if the authors meanings can be sufficiently transferred to the readers. However, in the submitted article, there are too many awkward English writing. The words included in the manuscript are about 3000. It is not appropriate for the full length artide. Rather than it will be suitable to the short communication. However those defaults do not degrade the originality and the novelty of the submitted article. In the reviewers opinion, the submitted artide should be totally re-written and revised, which means that the reviewer’s decision is the major revision. If the fully revised article is submitted the reviewer is wiling to re-review again. The following table is the detailed comment by the reviewer. Although the reviewer knows that some comments would be not proper from the author’s viewpoint, it will be useful to improve the authors manuscript for the next submission. No Page Original Sentence Comments 01 1 in abstract The passive systems that generate vacuum is a reliable systems and could allow heat pumps that uses traditional refrigerant to be used in seawater distillation process by reducing the Saturation temperature of seawater to be matched with the… Not so easy to understand. Separate the sentence. 출처 : 칼있으마님의 블로그 영어로 논문을 읽고 작성하는 과정에서 유용한 템플릿이 생길때마다 계속 업데이트 예정이다.",
    "tags": "papertomath data science insight analysis basics modeling practice dev",
    "url": "/dev/2019/08/27/dev-papertomath-paper-io/"
  },{
    "title": "[Data Science] 인사이트(Insight)! 다시 기본으로",
    "text": "개요 본 포스팅은 시리즈로 계획되어 있으며, 인사이트와 기본의 중요성을 다시금 새기고 긴 여정의 출발에 대한 각오를 다지는 글입니다. 차후 EDA, CDA 등 데이터 분석을 통해 인사이트를 도출하고 모델링을 거쳐 예측 및 설명의 자동화에 이르기까지 전체 과정을 실무에 즉시 적용할 수 있도록 기술해 나갈 예정입니다. 목차 인사이트(Insight)! 다시 기본으로 인사이트(Insight) 너무 추상적인데.. 보다 구체적으로! 학사 vs 석박사 그리고, 프로그래머 vs 연구자 인사이트(Insight)를 도출하기 위하여 인사이트(Insight), 다시 기본으로 데이터 사이언스, 빅데이터 분석의 최종 목적은 인사이트 도출일 것이다. 우리는 인사이트를 도출해야 하는 분명한 목표를 가지고 있음에도 때로는 목표를 잃고 방황하여 모델의 파라미터 수치를 최적화하는데 집중하기도 하고, P, Z, T값 등을 분석하는데 혈안이 되기도 하며, DQN 등 새로 등장하는 기술을 수학적으로 해석하려고 하거나, Tensorflow, Keras 코드 구현에 집중하며 우리가 왜 이것을 만들고 있는지 망각하기도 한다. 물론 이런 현상이 나 자신에게 부여된 임무가 데이터 엔지니어나 비지니스에 특화된 데이터 분석에 한정되어있다면 큰 문제가 없을지도 모르나, 데이터 분석 조직 전체에서 각 역할을 책임지는 구성원들이 한곳의 목표의식 즉, 인사이트를 구체적으로 이해하고 바라볼 수 있다면 조직이 발휘할 시너지와 역량이 더욱 커져 목표 이상의 성과를 내는데 큰 도움이 될 것이라 생각한다. 그렇기 때문에 그 어떤 분석기법과 수학적 모델링 구현을 넘어 가장 중요한 것은 기본기. 즉, 쉽고 구체적이고 정량화된 인사이트 도출일 것이다. 이 글은 필자가 데이터 분석을 하는데 있어 - 분석 목적이 업(業)에 있든 사적인 목적에 있든간에 - 실무를 해결하기 위한 직접적인 방법론을 기술하고, 당초 수립한 목표(관심사)를 잊지않기 위한 반면교사로 삼고자 작성하는 글이다. 인사이트(Insight) 인사이트.. 만능 키워드이다. 단어만 언급해도 차도남이 된 것 같고 뭔가 멋있어진 느낌이다. 중요한 것도 알겠고 이게 우리의 목표인 것 또한 알겠는데, 도대체 어떻게 얻을 수 있을지 생각하기 시작한 그 순간부터 왠지 부담되고 별로 친해지고 싶지 않은 마음이 든다. 인사이트를 사전에서 찾아보면 “통찰력”이라고 번역되며, 통찰력은 “사물을 통찰하는 능력”이라고 표기되어 있다. 하여 필자는 특정 분야에 대하여 전체 현상을 이해하는 능력으로 가장 효율적인 방식을 판단할 수 있음은 물론 미래 예측에 도움이 되는 능력이라고 스스로 정의하고 있다. 이를 조금 더 구체적으로 빅데이터 분야와 연관지어 본다면 인사이트가 도출될 경우, 전부에 가까운 데이터에 존재하는 패턴을 인식할 수 있기에 향후 의사결정의 방향을 정하는데 결정적인 역할을 담당하게 될 것이라 생각한다. 구성원들이 숲을 보지 못하고 나무만 바라볼 때 우물안에서 개구리를 꺼내주는 역할을 담당할 것이다. 너무 추상적인데.. 보다 구체적으로! 구체적인 예시를 통해 데이터 분석에 쓰이는 인사이트의 개념을 익혀보자. \"10일 이내 친구 7명을 사귄 사용자는 서비스를 오래 이용한다.\" Facebook의 인사이트이다. 세계 트렌드를 이끄는 실리콘밸리 굴지의 기업, 그리고 거창하고 멋있는 인사이트라는 단어와는 사뭇 다르게 뭔가 조촐하지 않은가? 초딩도 이해하기 아주 쉬운 문장이므로 약간의 의구심이 든다. 하지만 의외로 유용한 인사이트는 굉장히 심플하다. 그래서 모든 구성원들이 목표를 정확하게 이해할 수 있게된다. 덕분에 구체적으로 컨트롤하고 싶은 관심사를 분명히 인지할 수 있다. 위의 인사이트가 발견되기까지 Funnel Model 등을 통해 페이스북 결제페이지 도달까지의 유입 경로를 사용자 관점에서 분석하였을 것이다. 더불어 10일, 7명과 같은 구체적화 된 숫자에(정량화, 계량화 되었다고 한다.) 주목해보자. 구체적인 수치가 나오기까지 얼마나 많은 데이터가 전처리되고 시각화 되었을지 생각한다면 데이터분석의 실제가 어느정도 보일것이다. 페이스북의 인사이트를 바라보며 사실 우리가 실무에서 맞닥드리게 될 대부분의 전투는 머신러닝 알고리즘을 어떻게 사용할까도 아니고, 수리통계학을 이용해서 데이터 특성을 반영한 멋진 수식과 모델을 세우는 것도 아니라는 것이다. 오히려 완벽주의 또는 범죄를 수사하는 탐정에 가까울 정도로 수집된 데이터의 측정시기와 배경조사, 본질파악, 오류가능성, Feature 엔지니어링, 전처리 등 디테일과의 싸움의 연속과 맞닥드리게 될 것이다. 학사 vs 석박사 데이터 사이언스 분야의 커리어로 전환하고 싶다는 마음에 관련 채용시장을 조사해보았다. 최근 몇년간 재미있는 구인구직 트렌드 변화를 몇 가지 발견하였는데 최소 석사 이상을 선호한다는 것, 프로그래머 중심의 채용에서 연구자 중심의 채용, 마지막으로 정작 인재들은 그토록 핫한 데이터 사이언티스트의 시장을 걷어차고 보다 딥러닝의 세상으로 도망(?)가는 느낌이 들었다는 것이다. (어디까지나 필자의 주관적인 생각이다.) 이런 현상이 시사하는 바가 무엇인지 나의 상황과 비추어 고민을 해보았다. 적어도 필자와 같이 20년 전 대학을 다니고 학사를 끝으로 취업전선에 뛰어든 사람이라면, 대학원이란 그저 지식의 양과 수준을 조금 더 높이고 인맥 좀 늘리고 커리어 스펙을 한단계 올리는 수단 정도로 생각하기 쉬울 것이다. 필자 역시 마찬가지였다. 필자가 뒤늦게 대학원에 진학하고 싶어 알아본 바 지식의 양이 늘어나는 것은 연구의 최종 목적이 아니라는 것. 지식의 양은 과정일 뿐 최종 목적은 세상에 없었던 지식의 영역을 본인이 최초로 개척해 나간다는 데 있다는 것이다. 인맥이니 커리어니 이런 것들은 사실 다 과정에서 부수적으로 얻는 것이지, 본질이 아니므로 이를 대학원의 목적으로 삼고 진학한다면 좋은 결과를 맞이하긴 힘들겠다는 생각이 들었다. (이때 깨달은 것이 많아 추후 별도 포스팅으로 정리해보겠다.) 대학원과 데이터 분석의 공통점이 있다면 그동안 존재하지 않았던 지식을, 방법론을, 인사이트를 얻어가는 과정이라는 것이다. 그저 미적분 빨리 풀 수 있고 프로그래밍 코드를 빠르게 구현하며 또는 두꺼운 서적을 암기하는 과정이 아니라는 것이다. 즉, 남이 개척해놓은 길을 따라가는 것이 아닌, 남의 개척한 길을 이용해서 새로운 길을 개척하는 것이 이 업(業)의 본질이라는 결론을 얻었다. 그런데 새로운 길을 개척해 나가는 과정이 그저 암기와 단편적인 지식의 결합으로 가능할까? 학사라면 그 정도면 충분했겠지만 석사, 박사의 경우는 완전 다른 방식의 사고가 훈련되어야 할 것이다. 최소 2년 ~ 5년 혹은 그 이상의 오랜시간을 데이터를 직접 특정 수집하며 그 안에 숨은 여러 속성 그리고 상황별로 달라질 수 있는 뉘앙스를 느끼고 변인 통제에 관련하여 시행착오를 여러번 반복하며, 데이터를 느낄 줄 아는 일종의 Sixth sense가 생겼을 것이다. 더불어 통계학 혹은 유사기법을 통해 가설을 세우고 실험을 반복하며 검증, 요약하는 과정을 수없이 반복하며 수많은 고민을 뚫고 사색하여 마침내 학위를 취득하며 연구 능력이 무쇠와 같이 단련되지 않았을까? ※출처 : PhD pitfalls: Part I – The reality of your contribution 그 똑똑한 사람들이 수년동안 고민하며 사색해 온 과정을 학사 출신들이 퇴근하고 몇개월 공부하고 고민한다고 쫓아갈 수 있는 것일까? 이런 이유로 필자는 대학원의 필요성을 느끼고 진실되게 고민을 하게 되었다. 위에서 언급했던 채용 트렌드가 최소 석사 이상으로 이동하는 것, 프로그래머에서 연구자 중심으로 이동한다는 것은 어쩌면 같은 이유이지 않을까? 그리고, 프로그래머 vs 연구자 아마도 기본 연구능력이 없는 상태에서는 데이터 사이언스 프로젝트 수행 시 구성원 간 의사소통 능력이 떨어지고 - 필자 생각으로는 논문을 중심으로 하는 연구 중심의 의사소통 방식은 일종의 별도 언어라고 생각하고 있으며, 한국어나 영어 보다도 더 중요한 인생에서 배워야 할 가장 중요한 언어라고 생각한다. 부끄럽게도 그걸 깨달은 지 5년이 채 안되었다. - 기본적인 통계 및 수학적 지식이 없는 상태에서 새로운 모델을 세우는 것이 가능할까? 프로그래머는 알고리즘 중심으로 데이터 속성의 위대함과 디테일을 경시할 것이고, 연구자들은 프로그래밍 스킬은 본질이 아니라는 것을 강조하다가 계층간 갈등이 생길 우려도 있다. 결코 학사출신 프로그래머 분들을 비하하는 것은 아니다. 그냥 정황을 파악해보며 주관적으로 판단해 본 상상일 뿐이다. 필자 역시 학사 출신 프로그래머이기 때문이다. 스스로 누워서 침 뱉기 하고 싶지는 않다. 하지만 우리 같은 출신들도 데이터 사이언스가 되기 위해 위에서 열거한 데이터의 뉘앙스를 이해하는 능력을 실무에서 키워왔고, 개인 시간을 쪼개어 열심히 수리 통계학을 익혀왔으며, 논문과 연구중심의 의사소통을 위한 스터디를 진행해왔다면… 그동안 우리가 닦아 온 수십년간의 백전노장으로서의 프로젝트의 현실과 실제에 대한 현장감각, 프레임워크 및 패턴의 설계/활용 능력, 수많은 계층과 갈등을 겪으며 발전해 온 의사소통 능력, 프로젝트를 효율적으로 완성시킬 수 있는 프로그래밍 스킬 등을 활용해 더 뛰어난 데이터 사이언티스트가 될 수 있을 것이라 생각한다. 못해도 안드레이 카패시가 작성한 스프트웨어 2.0 같은 멋진 세계에서 활동하는 인재는 되지 않을까? 인사이트(Insight)를 도출하기 위하여 데이터 사이언티스들이 시장을 걷어차고 AI, 딥러닝 분야로 이직이 발생하는 이유 또한 독특한 현상이라고 생각한다. 아마도 데이터가 가진 속성이 너무 다양하고, 적용되는 업무 도메인 분야 또한 다양하며, 데이터 측정 당시 발생하는 본질의 다양성과 사람과의 커뮤니케이션 관계가 한몫하지 않았을까? 딥러닝은 확실히 모델이 이해하기 쉬운 이미 가공된 데이터를 수집하여 활용하며 때문에 영상, 음성인식, NLP 등 한정된 분야에 좋은 성능을 발휘한다. 즉, 위에서 언급한 소프트웨어 2.0과 같은 개념의 프로그래밍 환경이 구성되어있다. 보다 높은 소득을 얻기 쉬운 가시적인 성과를 내기에 투입한 노력 대비 효과가 분명하다. 다시 데이터 분석 본연의 관점으로 돌아와 보자. 훌륭한 데이터 사이언티스트라면 프로젝트를 효율적으로 실행가능하게 만드는 설계능력과 프로그래밍 스킬은 물론 위에서 언급한 데이터의 뉘앙스를 읽을 줄 알고 연구중심의 의사소통능력을 갖춰야 한다. 그런점에서 가설 &gt; 실험 &gt; 검증,분석,요약으로 이어지는 통계 분석적 접근 방법이 필요하다. 그런데 현실은 가설 하나 건지기도 녹록지 않다. 오히려 일반적인 사업도입 관점으로 접근하여 빅데이터를 도입하겠다는 명분아래 하둡, 스파크, NoSQL, 클라우드 등 인프라 도입을 먼저 추진하며 소득없는 비용을 지출하는 경우가 허다하며, 도입 후에도 업무만 늘어나고 의미있는 인사이트는 발견이 안된다. 왜 그럴까? 이유는 간단하다. 관심있는 목표를 명확히 하지 않았기 때문이다. 관심사를 명확히 설정하고 구체적으로 정량화하여 측정가능한 가설을 세웠어야 하고 그러기 위해 조직 내 심도깊은 고민과 인터뷰, 브레인 스토밍 과정이 생략되었기 때문이다. 심도있는 고민끝에 관심사를 끄집어내고 이를 설명할 수 있거나(즉, 컨트롤하여 향상시킬 수 있는) 예측할 수 있는 수백개 이상의 가설을 모은 후 다음 단계에서 기술할 EDA 과정을 통해 가설을 분석할 만한 가치가 있는지? 데이터는 보유하고 있는지? 보유한 데이터는 정말 우리가 알고 원하는 데이터인지? 등을 확인해가며 최종 의미있는 가설을 채택할 수 있게 될 것이다. 인사이트는 그렇게 디테일한 진흙탕에서 구르며 진주를 찾는 과정이다. 단어의 뉘앙스처럼 그저 깔끔한 양복입고 하늘을 바라보고 드라마 한편 찍다 생기는 것이 아니다. 진정한 인사이트를 얻기 위해 우리는 관심사(목표)가 무엇인지를 명확히 하는것을 시작으로 위에서 언급한 단계를 가급적 모든 구성원이 모든 아이디어(그것이 옳던 그르던)를 총동원하여 꽤 오랜 시간 고민해야 할 것이다.",
    "tags": "ml data science insight analysis basics modeling practice dev",
    "url": "/dev/2019/08/25/dev-ml-insight/"
  },{
    "title": "[Colab] Google Colab 환경설정 및 사용법",
    "text": "개요 파이썬을 활용한 머신러닝의 첫 관문! 구글 Colab의 환경구성 및 기본 사용법을 다룬 포스트입니다. 목차 Colab이란 무엇인가? Colab 환경설정 Colab을 활용한 간단한 예제 작성 Colab &amp; Markdown Colab이란 무엇인가? 구글 코랩(Colab)은 클라우드 기반의 무료 Jupyter 노트북 개발 환경이다. 내부적으로는 코랩 + 구글드라이브 + 도커 + 리눅스 + 구글클라우드의 기술스택으로 이루어진 것으로 알려져있다. 흔히들 딥러닝은 귀족학문이라고 말한다. 대학원에서 등록금을 지불해야 함은 물론, 꽤 긴 시간의 고찰과 연구를 필요로 하며(그 시간동안 돈을 못번다. 오히려 더 써야 할지도…), 비싼 Machine을 가진 자가 더 빠른 결과를 얻을 수 있다. 정말 금수저(귀족)에게 어울리는 분야다. 필자처럼 가난한 흙수저들은? 눈물을 흘리며 되지도 않는 성능을 짜내어 몇일에 걸쳐 딥러닝 모델을 학습시킨다. 대학원? 꿈도 못꾼다. 여우같은 마누라와 토끼같은 새끼가 나만 바라보는데 돈을 벌어오지는 못할 망정 되려 공부하느라 돈을 쓰겠다고? 사람이 평생 자신이 하고싶은 것을 찾지 못하고 죽는 경우가 허다한데 이제야 일생을 걸어볼만한 목표를 만나게 되었는데.. 현실이라는 벽에 부딪혀 꿈만꿔야 하는 처지가 되고보니 세상을 조금 더 열심히 살지 못한 것, 쉽게 때를 만나지 못하는 것에 대해 많은 사색에 잠기곤 한다. 여튼 돈없는 사람은 꼭 보시길 바란다. 인생 한탄은 각설하고 Colab의 장단점을 알아보자. 내 PC도 좋은데.. 굳이 써야되나요? 공짜다. 쉽다. 환경설정 및 구동 준비가 5분이면 끝난다. 이 말인 즉슨… 올해 초 Python 3.7 최신버전을 깔았다가 라이브러리 import부터 막혀서 삽질한 분들 activate 문제, graphviz 설치 삽질 등 수많은 실행환경 구축에서 좌절하셨던 분들이라면 뼈저리게 와 닿는 말일 것이다. 클라우드 기반이다. 이게 무슨 말이냐고? 여러명이 동시에 수정 가능하다. 인터넷 브라우저만 깔려있으면 언제 어디서든 접속하여 수정이 가능하다. 심지어 모바일에서 수정할 수 있다. 성능이 좋다. 어지간한 개인 PC보다 성능이 좋고 빠르다. 비록 내 PC 사양이 뛰어나더라도 딥러닝 학습시간동안 내 PC로 아무것도 못하는것 보다는 영화라도 한편보는 것이 낫지 않은가? 학습 및 공유에 최고! 일반 Jupyter Notebook에 비해 더 좋은 기능을 제공한다. (목차기능, Markdown의 미리보기 기능, 파워레벨, 고양이모드 등) Git과의 연동이 용이하여 타인과 지식을 공유하기 좋은 환경이다. 어딘가 모여 스터디를 진행한다면? Python 기반 특성 상 다들 노트북의 환경 맞추기 등에 시간을 낭비하기 마련인데 그럴 걱정이 없다. 오류발생 시 [SEARCH STACK OVERFLOW] 버튼을 클릭하면 자동으로 스택오버플로우 사이트 검색결과가 나타난다. 주의할 점 세상의 모든것은 Trade-off가 존재하는 법. 공짜로 쓰는 대신 다음 사항은 유의해야 한다. 최대 세션 유지시간은 12시간이다. 아무짓도 안하거나 또는 12시간이 지나면 알아서 세션이 끊긴다는 의미이다. 세션이 끊기면? 작업중이던 데이터가 다 날라간다. 그럼 쓰면 안되는거 아냐? 소스코드는 .ipynd 확장자로 구글 드라이브에 안전하게 보관되므로 걱정할 필요없다. 다만, 딥러닝 학습시킬 데이터가 문제인데 구글 드라이브에 저장해 놓으면 된다. 물론 개인이 공짜로 쓰는 구글 계정의 최대 용량은 15G이다. 30G이상 저렴한 비용으로 쓸 수도 있다. 학생이라면 구글 GSuite 서비스로 구글 드라이브를 무제한으로 활용하는 것도 방법이다. 금융권 등 망분리 보안 이슈로 법적으로 클라우드에 데이터를 올릴수 없는 경우는 사내에서 활용하기 어렵다. 보다 자세한 사항은 Google Colaboratory 공식페이지를 참고하시기 바란다. 자! 그럼 이제 이 좋은것을 써 보기 위해 환경설정을 해보자. 위에서 말한대로 5분이면 끝난다. Colab 환경설정 구글 계정에 가입한 후 아래와 같이 진행한다. https://drive.google.com에 접속 후, 우클릭하여 다음과 같이 test 폴더를 만든다. 좌측 상단의 [+새로만들기] 버튼 &gt; 더보기 &gt; 연결할 앱 더보기를 선택한다. 새로 뜬 팝업의 우측 상단에 colab이라는 검색어를 입력하면 아래 그림과 같이 Google Colaboratory 앱이 등장한다. 검색어 바로 밑에있는 녹색 연결하기 버튼을 누른다. (필자의 경우 이미 연결했기 때문에 평가하기 버튼으로 보이고 있다.) 드라이브 메인화면으로 이동 &gt; 톱니바퀴 모양 버튼 클릭 &gt; 설정을 클릭한다. 새로 뜬 팝업의 좌측 메뉴 앱관리 클릭 &gt; Google Colaboratory 우측의 기본값으로 사용 체크박스 클릭 &gt; 완료를 클릭한다. 드라이브 메인화면으로 이동 &gt; 우클릭 &gt; 더보기 &gt; Google Colaboratory 클릭 드디어 .ipynb 확장자 파일의 쥬피터 노트북이 등장했다. 파일이름을 클릭하여 test.ipynb로 이름을 변경하자. 상단 메뉴의 도구 &gt; 환경설정을 클릭한다. 팝업이 등장한다. 원하는 테마를 선택 후, 체크박스들을 클릭한다. (좌측) 편집기 탭 클릭 &gt; 들여쓰기 4선택 &gt; 체크박스 2개 체크 (좌측) 기타 탭 클릭 &gt; 원하는 설정을 적용 &gt; 저장을 클릭한다. 참고로 파워레벨을 선택 시, 코딩마다 불꽃이 튀기는 재미가 있다. 아기고양이 모드를 선택 시, 코딩에 지칠 때 고양이들이 위에 튀어나와 근심(?)을 덜어준다. 상단 메뉴 런타임 &gt; 런타임 유형 변경을 클릭한다. 런타임 유형은 Python3를, 가속기는 GPU를 선택 &gt; 저장을 클릭한다. 아래 그림과 같이 연결 버튼을 클릭한다. 할당중.. &gt; 연결중… &gt; 초기화중.. 으로 텍스트가 변경되며, 최종 RAM, 디스크 사용량 막대그래프가 나올 것이다. 아래와 같이 [&gt;] 모양의 버튼을 클릭하면, 목차, 코드스니펫, 파일등의 기능을 활용할 수 있다. 쥬피터 노트북 수천줄 코드 속에서 셀끼리 비교를 하느라 마우스 드래그에 지치신 적이 있으시다면 할렐루야!가 저절로 나올것이다. 위에서 설명했듯이 Markdown의 기능 또한 강력하다. 다음 다음 챕터에서 설명하겠다. 자! 이것으로 Colab 환경설정은 끝났다. 너무 쉽지 않은가? 이제 간단한 예제를 작성해보자. Colab을 활용한 간단한 예제 작성 먼저 필자의 Colab .ipynb 파일 링크를 클릭하여 접속해주시기 바란다. 아래와 같은 화면이 보일것이다. 이 파일은 Colab에서 필수적으로 사용하게 될 클라우드 원격서버 스펙 확인, 파일다루기, 구글드라이브 연동, 텐서플로우 및 케라스의 예제를 필자가 직접 테스트해 본 Colab 실습예제이므로 복사 또는 공유를 통해 반드시 따라하며 실습하시기 바란다. 10분 정도만 투자하면 전체 매커니즘을 파악하는데 어렵지 않게 될 것이다. 각 셀을 실행하는 방법은 Ctrl+Enter 단축키를 입력하시면 된다. 목차를 활용하셔서 참고하시면 편리하며, 그림에 빨간동그라미로 가리킨 [▼]모양의 버튼을 클릭하시면 다른 문단의 셀이 자동 접기가 되므로 해당 Chapter에만 집중하여 보실 수 있다. 아래에는 블로그 포스트 특성 상 간단한 예제 코드를 기술하였으며, 자세한 내용은 상단의 링크를 참고하시기 바란다. * Colab 서버 스펙 확인 from tensorflow.python.client import device_lib device_lib.list_local_devices() import platform platform.platform() 아래의 코드는 한줄씩 셀에서 실행하기 바란다. 쥬피터 노트북의 셀은 출력결과가 여러개인 경우 맨 마지막의 OutputStream 결과가 남으므로 이전 출력결과를 확인할 수 없기 때문이다. 일반적인 .py파일 작성과는 다르게 .ipynb파일의 경우 가급적 하나의 셀에 많은 코드를 작성하지 않는것이 좋다. !cat /etc/issue.net !cat /proc/meminfo !cat /proc/cpuinfo !df -h !nvidia-smi !python --version !ls from tensorflow.python.client import device_lib device_lib.list_local_devices() * 파일처리 %%writefile test.py print('hello world!') # test.py 실행시키기 %run test.py from google.colab import files # 브라우저에 다운로드 됨을 확인할 수 있다. files.download('test.py') # [Cancel upload] 버튼을 클릭하여 잠시 멈춘 후 파일선택 버튼을 클릭하면 PC 내 파일을 선택할 수 있는 다이얼로그 창이 뜬다. # 리턴값을 받는 변수인 myupload라는 이름의 디렉토리가 생성된다. myupload = files.upload() *구글드라이브 연동 import os print(os.getcwd()) !ls # 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다. from google.colab import drive drive.mount('./MyDrive') # 마운트된 내 드라이브를 확인해보자 !ls # 해당 드라이브로 이동 # 내 드라이브는 원격서버가 아니라 로컬서버로 간주하므로 명령어 실행시 앞단에 !를 붙이지 않는다. cd MyDrive/My Drive # 내드라이브의 전체 목록이 나타난다. ls # 특정파일을 가져오고 싶은 경우 다음과 같이 접근한다. import pandas as pd df = pd.read_csv(\"./MyDrive/test/test.csv\") 텐서플로우 및 케라스 예제 실행은 코드가 긴 관계로 생략하며 상기 링크를 통해 확인하시기 바란다. Colab &amp; Markdown 예전 포스팅에서 이미 마크다운(Markdown) 사용법 및 예제를 자세히 소개한 바 있다. 이 장에서는 당시 다루지 않은 특히 Colab에 특화된 새로운 마크다운 작성방법 및 단축키에 대하여 설명하겠다. 하단의 단축키는 매우 자주 사용되는 단축키이므로 반드시 숙지하시기 바란다. 마우스로 이동하며 일일이 버튼을 클릭하는 것은 노가다가 심해 불편하다. 코드가 오래 실행되어 멈추고 싶은경우 : Ctrl+ M + I 엔터키 : 편집모드(Vi 편집기와 유사) ESC : 선택모드(Vi 편집기와 유사) 선택모드에서 화살표 방향키 : 셀 포커스를 위 아래로 움직일 수 있음 마크다운으로 전환 : Ctrl + M M 코드로 전환 : Ctrl + M Y 선택된 셀을 실행 : Ctrl + Enter 선택된 셀을 실행 후 다음 셀로 포커스 이동 : Shift + Enter 실행 후 다음줄로 이동 : Alt + Enter 위 코드로도 멈추지 않고 작업을 완전종료하고 싶은 경우 : Ctrl+M+. 코드셀에 줄번호 부여 : Ctrl + M L 바로 윗줄에 셀 생성 : Ctrl + M A 바로 아랫줄에 셀 생성 : Ctrl + M B 셀 삭제 : Ctrl + M D 저장 : Ctrl + S 이로써 Colab을 활용하여 머신러닝 및 딥러닝을 학습하기 위한 준비가 완료되었다. 다음 포스트에서는 캐글과의 연동 실습을 주제로 보다 심도있게 사용하는 방법을 다뤄보겠다.",
    "tags": "ml google colab python jupyter notebook dev",
    "url": "/dev/2019/08/23/dev-ml-colab/"
  },{
    "title": "[리뷰] 페이스북 퍼포먼스 마케팅 with 구글 애널리틱스",
    "text": "개요 본 리뷰는 비제이퍼블릭 출판사 \"페이스북 퍼포먼스 마케팅 with 구글 애널리틱스(전민우,유성민 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 구글 애널리틱스 페이스북 퍼포먼스 마케팅 누가 읽어야 하는가? 요약하며… 구글 애널리틱스(Google Analytics, GA) 누구나 한번쯤은 수익 혹은 취미를 목적으로 웹사이트를 제작하겠다고 마음먹은 적이 있을 것이다. 웹사이트를 만들고 싶다면 필자의 블로그인 Github Page, 워드프레스, 네이버블로그 등 훌륭한 플랫폼을 이용하여 큰 비용과 노력을 들이지 않고도 쉽게 만들 수 있는 세상이 되었다. 대신 이 시점에 등장한 새로운 문제가 있다면 제작한 사이트가 목적에 맞게 효율적으로 동작하는지 평가하기 만만치 않다는데 있다. 웹사이트에 누가 방문하고 어떤 행동을 하며 무엇이 문제라고 느끼고 있는지 그리고 내 목표 기준 대비 사용자들은 얼마나 만족을 느끼고 있는지 파악하기 위해 직접 프로그램을 개발한다고 가정하자. 위에서 열거한 블로그 플랫폼들은 워드프레스를 제외하고는 서버 지향 웹 프로그램 개발이 불가한 환경이며 워드프레스를 사용한다고 해도 서버를 운영해야 하는 관계로 별도의 호스팅 비용이 발생하게 된다. 뿐만 아니라 본인이 제법 훌륭한 프로그래머가 아니라면 어떻게 분석 프로그램을 설계할지 결정하는데 있어 많은 시행착오를 필요로 하게 되며 꽤 많은 노력과 시간을 들여 프로그램을 개발해야 한다. 이런 수고를 차치하더라도 본연의 웹사이트 제작 목적에 다가가기 전에 지쳐버리거나 배보다 배꼽이 더 큰 목적이 전도되는 현상에 좌절을 겪을지도 모른다. 이럴때 필요한 것이 바로 구글 애널리틱스이다. 왠만한 공수를 들여 프로그램을 개발하는 것보다 훨씬 다양한 분석도구와 서비스 속도를 자랑하는데 그것도 모자라 무료라는 점이 필자를 푹 빠지게 만드는 계기가 되었다. 구글 애널리틱스는 일종의 비즈니스 애널리틱스(Business analytics, BA) 기능을 제공하는 구글사에서 제작한 솔루션으로 웹사이트의 실적을 높이고 온라인 비즈니스의 성공을 돕는 효율적인 웹사이트 분석 도구이다. Google Analytics Google Analytics is a web analytics service offered by Google that tracks and reports website traffic, currently as a platform inside the Google Marketing Platform brand.[1] Google launched the service in November 2005 after acquiring developer Urchin. - 위키백과 - 더불어 구글 애널리틱스에서 제공하는 기능을 한번쯤은 체계적으로 정리해야겠다는 필요성을 느꼈는데, 첫째는 데이터 사이언티스트가 되고 싶은 사람으로써 GA를 통해 솔루션을 구성한 비지니스적 인사이트를 얻고 싶었기 때문이고, 둘째는 본 블로그를 운영하며 대시보드 정도만 바라보는 수준에서 좀 더 사용자의 유입경로를 세부적으로 분석해보고 싶었기 때문이다. 이런 목적을 달성하는 데 있어 본 서적은 큰 도움이 되었다. 단순히 페이스북, 구글 애널리틱스의 기능을 따라하며 구축하는데 그치지 않고 구축의 필요성과 마케팅 관련 용어 및 개념을 자세히 서술하여 가독성을 높였고 별도의 검색없이도 모든 내용을 이해할 수 있도록 알차게 구성한 덕에 생각보다 빨리 그리고 쉽게 구글 애널리틱스 기능의 전반을 훑어볼 수 있었다. 예를들면 요즘 마케팅 트렌드는 매 시간 광고관리자 페이지를 Refresh하며 캠페인 성과를 추적하고 빠르게 대응하는 것이 핵심이라는 배경 지식을 얻게 된다. LTV, CAC와 같은 마케팅 용어도 상세히 설명하여 솔루션을 이해하는데 지장이 없게 해준다. 이런 매끄러운 가독성을 유지하며 필자가 읽은 내용 및 장점을 아래와 같이 간단히 정리해 보았다. Google Analytics의 장점 쉽다! 공짜다! 마케터의 관점(잠재고객, 획득, 행동분석)에서 데이터를 활용할 수 있다. 분석을 위한 분석에서 피할 수 있게 된다. 올바른 데이터분석을 하고 있는지 판단하기 위한 체크리스트 기존의 가설을 검증하는데 유용한가? 새로운 가설의 수립이 가능한가? 새로운 것을 배울 수 있는가? Google Analytics 개요 및 실습 핵심용어 : 상호작용, 세션, 전환, 이탈률, 종료율, 측정기준, 측정항목 등 전환리포트, 잠재고객리포트, 획득리포트, 행동리포트 활용법 필수설정 : 인구통계 설정, 서치콘솔(검색 키워드 유입분석), 보기설정(백업, 브랜치 기능), 필터 상세기능 실습 : 목표 및 유입경로 설정, 채널, 맞춤알림, 데이터스튜디오 설정 방법 등 아래 그림과 같이 단계별 이미지는 실습을 따라가는데 많은 도움을 준다. 페이스북 퍼포먼스 마케팅 페이스북은 소셜 기능 외에도 마케팅 용도로도 국내외에서 엄청난 인기를 끌어왔다. 문제는 유료 광고비 없이 무료로 활용 가능한 유기적 도달률이 2013년 이후로 계속 하락하여 최초 약 12%에서 현재 약1~2%로 추정될 정도로 하락하였기에 페이스북은 이제 끝물일지도 모른다는 점을 저자는 진솔하게 표현하고 있다. 그럼에도 아직까지 이를 대체할 만한 훌륭한 플랫폼이 없기에 페이스북을 통한 마케팅은 나쁘지 않다는 점과 저자가 제시하는 퍼포먼스 마케팅의 전략으로 효과를 극대화 할 수 있음을 강조하고 있다. 역시 앞선 구글 애널리틱스 파트와 마찬가지로 CTR(클릭율), CPC(클릭당비용), CVR(전환율) 등의 용어설명 및 나아가 퍼널 모델까지 마케팅 관련 지식을 쉽게 자세히 설명하여 큰 지장없이 실습을 따라갈 수 있도록 도와준다. 본 파트에서 읽었던 내용 및 장점을 아래와 같이 요약하였다. 초급기능 페이스북 광고관리자, 캠페인, 광고, 성과지표 제작방법 특히 성과지표 수립 시 데이터를 다루는 방법을 매우 상세히 다뤄 많은 도움이 되었다. 그 외 효과적인 광고의 포맷이나 광고의 트렌드도 설명하여 최신 동향을 파악하기 좋았다. 중급기능 리마케팅, 픽셀, 확장프로그램 설치 방법 특히 페이스북 마케팅을 어렵게 만드는 픽셀에 관해 쇼핑몰별 설치법까지 상세히 다뤄준다. 아래 그림은 픽셀코드를 설치하는 방법으로 어려운 부분마다 친절히 그림이 삽입되어 이해를 돕는다. 고급기능 맞춤이벤트, 픽셀심화, 전환값, 동적 리마케팅 학습 마무리답게 그간 필자의 현장 강의를 토대로 자주 받았던 트러블 이슈를 언급하여 화룡정점을 찍는다 전반적으로 탄탄한 현장 강의 경험을 토대로 저자의 친절한 정성이 담긴 서술 방식이 마음에 들었다. 누가 읽어야 하는가? 최신 트렌드 마케팅 기법을 익히고 싶으신 분 마케팅 분야 데이터분석가 수익 창출을 목적으로 하는 사업가, 자영업자 구글 애널리틱스, 페이스북 유사 프로그램을 개발, 운영중인 프로그래머 기타 웹사이트 운영에 있어 인사이트를 얻고 싶은 분 누구나 요약하며… 필자같은 IT 전공자에게는 마케팅을 실전적으로 알기 쉽게 한권의 책으로 요약하며 배우는 느낌이 들어 큰 도움이 되었다. 더불어 탁상공론식 마케팅이 아닌 최신 트렌드를 기반으로 구글 애널리틱스 및 페이스북과 같은 Hot한 플랫폼을 익힐 수 있는 좋은 기회였다. 비제이 퍼블릭의 서적에서 매번 느껴왔던 극도로 실전적인 기술이 백견이 불여일타를 좋아하는 필자에게 안성맞춤이었다고나 할까. 비제이퍼블릭 책 답게 한권의 책으로도 당장 실전에 적용하여 원하는 목적을 달성할 수 있었다는 점에서 높은 점수를 주고 싶다. 반면 다소 아쉬웠던 점이 있다면 마케팅의 설명 부분과 실습파트를 명확히 분리하면 좀 더 가독성이 향상되지 않았을까 싶다. 더불어 마케팅 초보자를 위하여 컬러 이미지가 삽입되었으면 하는 아쉬움이 남았지만 상당한 지면수에 따른 비용을 감안할 때 어쩔 수 없는 선택이었다는 점도 이해가 된다. 더불어 실습을 진행하는데 있어 구글 애널리틱스 리포트 실습 시 메뉴의 접근방법이 명확히 표기되지 않아 동일 리포트를 찾는데 약간의 시간이 더 소요되었다는 점, 페이스북 파트에 비해 다소 부족한 기능을 설명하였다는 점이 부족한 부분이었다 생각한다. 하지만 전체적인 장점에 비해 큰 단점이라고 생각하지는 않는다. &lt;비제이퍼블릭 출판사&gt; 책 한권만으로도 실무를 바로 수행할 수 있을 정도의 극도의 실용성을 자랑하는 서적을 많이 접해 인상적이었으며, 우수하고 참신한 해외 번역서, 최신 트렌트를 겨냥하는 내용으로 필자를 놀라게 하였습니다. IT 출판 분야의 신기원을 개척하여 어느덧 독보적인 위치를 차지하는 훌륭한 출판사라고 생각합니다. 십여권의 책을 장만하여 평소 즐겁게 읽고 있는 중입니다. 여러분께도 IT기술 실력을 향상시키는데 있어 큰 도움이 될 것이라 믿습니다. 비제이퍼블릭 바로가기",
    "tags": "review book marketing facebook ga google analytics",
    "url": "/review/2019/08/20/review-book-ga-facebook-marketing/"
  },{
    "title": "[리뷰] 자연어처리 딥러닝 캠프",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"자연어처리 딥러닝 캠프(김기현 저)\"를 읽고 얻은 지식을 정리한 글입니다. 목차 인공지능이 논문을 학습한다면 NLP를 배우는데 있어 본 도서의 장점 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 인공지능이 논문을 학습한다면 수학에 뿌리를 둔 통계학. 통계학에 뿌리를 둔 머신러닝. 머신러닝에 뿌리를 둔 딥러닝. 그리고 딥러닝에 뿌리를 둔 NLP가 있다. 자연언어처리라고도 불리는 NLP는 인공지능 기술 중 하나로 기계로 하여금 인간의 언어를 이해할 수 있게 만드는 기술이다. 이 기술이 실현된다면 마치 사람으로 따지면 통역가와 같이 다국어 간의 컴퓨터, 기계를 통한 번역이 가능해진다. 뿐만 아니라 문서 자동분류, 챗봇 등 다양한 응용분야으로의 적용도 가능해진다. 자연 언어 처리(Natural language processing) 자연어 처리(自然語處理) 또는 자연 언어 처리(自然言語處理)는 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 모사 할수 있도록 연구하고 이를 구현하는 인공지능의 주요 분야 중 하나다. 자연 언어 처리는 연구 대상이 언어 이기 때문에 당연하게도 언어 자체를 연구하는 언어학과 언어 현상의 내적 기재를 탐구하는 언어 인지 과학과 연관이 깊다. 구현을 위해 수학적 통계적 도구를 많이 활용하며 특히 기계학습 도구를 많이 사용하는 대표적인 분야이다. 정보검색, QA 시스템, 문서 자동 분류, 신문기사 클러스터링, 대화형 Agent 등 다양한 응용이 이루어 지고 있다. - 위키백과 - 이 자체로도 충분히 전도 유망한 기술이지만 필자가 딥러닝 기술 중 NLP에 가장 관심이 많은 이유는 다른데에 있다. 바로 소제목에서 명시한 바와 같이 논문을 학습하게 될 가능성 때문이다. 필자가 보기에 논문은 그야말로 딥러닝의 언어이다. Review 논문은 마치 Graph Theory를 활용하여 구성하기에 딱 좋은 형태이며 학문이 발전해 온 전체 목차를 구성하기 좋고, 대부분 논문의 Abstract에는 Review 논문의 흐름 중 어느 카테고리에 붙어야 어울리는지 힌트를 제공한다. 또 내부 지식은 대부분 귀무가설과 검증에 대한 시행착오가 담겨져 있어 통계학을 원류로 하는 현 딥러닝 모델이 인식하기에 적합한 지식 구조로 이루어져 있다. 딥러닝이 핫이슈이긴 해도 현재까지는 적용되는 범위가 한정되어있는데 주로 이미지, 음성, 영상 인식 등에 국한되어있다. 이 분야들의 공통점은 자연 그대로가 아닌 1차 가공을 거친 데이터를 피처로 활용한다는 점이다. 신호처리 등 그간의 학문분야에서 심도 있게 연구되어 온 축적된 지식이 있기에 딥러닝 모델이 인식하기 좋은 형태로 변환하거나 특징을 추출, 선택, 변환할 수 있는 것이다. (보다 자세히 다룬 내용은 필자의 블로그 “통계학 vs 컴퓨터공학, 멋대로 써보는 Data Science 미래에 대한 소고(小考)-세상 모든 논문들을 딥러닝이 이해할 수 있게” 부분을 참고하시기 바란다.) 이런 측면에서 볼 때 NLP는 목적 그 자체가 될 수도 있으나, 세상의 거의 모든 분야의 데이터를 딥러닝이 이해하기 쉽고 좋은 성능을 낼 수 있도록 Feature생성 혹은 전처리 역할을 담당하는 일종의 중간 Layer로 활용될 가능성이 크지 않을까 생각한다. DQN 등 Reinforcement Learning과 결합하여 논문을 이해하고 구현체를 만들 수 있는 날이 온다면 세상은 대 격변을 맞을 것이라 생각한다. 때문에 필자는 NLP를 활용하여 논문을 학습할 수 있는 모델을 설계하거나 이를 뒷받침하는 연구를 하고 싶다는 꿈이 있다. NLP를 배우는데 있어 본 도서의 장점 NLP 기술이 왜 중요한지 그리고 미래에 얼마나 유망한지 위 장에서 간단하게 언급하였다. 그렇다면 NLP 기술은 어떻게 익혀야 할까? 적어도 한국어와 관련된 자연어 처리라면 이 책이 해답 중에 하나라고 말하고 싶다. 그간 시중에 출간된 NLP 서적은 흔했지만 깊이가 너무 얕았다. 이와 관련하여 본 도서가 가지는 특출난 장점이 몇가지 있어서 소개해 본다. 일단, 한국어 NLP를 다룬다. 한국어에는 다른 언어에 비해 다음과 같이 자연어 처리를 어렵게 만드는 요소가 있다. 교착어 : 접사가 붙어 의미와 문법적 기능이 정해진다. (예: 잡히시었겠더라) 띄어쓰기 : 표준이 계속 변한다. 평서문과 의문문 : 동일한 문장구조를 가진다. 주어생략 : 명사를 중요시하는 영어와는 달리 동사를 중요시한다. 한자(漢字) 기반의 언어 : 표음문자의 특징인 중의성 문제(예: 건널제, 끌제, 제목제) 대부분의 장에서 영어 기반의 NLP 기술과 수학적 근간을 설명한 후 한글에 적용해보며 특수성을 언급하고 해결책을 제시한다. 깊이가 있다. (재미는 입문서급이다.) NLP 초보자라면 언어모델링도 쉽지않은데 이 책은 그 이상의 심화주제를 소개한다. 듀얼리티, 전이학습, NMT시스템 구축 등은 그간 https://arxiv.org에서 접하면서 이해가 까다로웠던 부분인데 비교적 최신 기술을 학습하고 그 결과를 공유해주려는 저자의 배려가 돋보였다. 이런 심화 내용에 대한 개념을 잡을 수 있었던 것이 필자에게는 가장 큰 소득이었다. 다양한 전처리 기법 및 정규표현식의 소개 그간의 전처리에서 겪었던 시행착오를 한방에 깔끔하게 정리해주었다. 아래 그림과 같이 알기쉬운 도식을 통해 정규표현식 또한 엑기스를 뽑아 전수해준다. 기계번역을 정면돌파한다. 그간 시중의 NLP 서적이 마음에 안들었던 이유는 크게 한국어, 번역 두 부분이었다. 특히 기계번역은 난이도가 높아 다른 서적에서 잘 언급하지 않는데 이 책은 과감히 기계번역을 시도한다. 수학을 정면돌파한다. 확률변수부터 MLE, MSE까지 직관적으로 설명한다. 문제를 푸는 수학이 아닌 어디에 왜 써야하는지의 관점으로 접근한다. 아래 그림은 몬테카를로 샘플링에 관한 설명이다. 기본 수학을 떼신 분이라면 꽤 깔끔하게 직관적으로 설명하는 저자의 능력 덕분에 그간 복잡하게 엉켜있던 수학의 개념이 쉽게 정리되는 것을 느끼실 수 있을 것이다. 아래 그림은 벡터유사도에 관한 설명이다. 쉽게 설명한다는 핑계로 수식을 열거하지 않는 책들이 많은데 그렇게 쉽게 얻은 지식은 희미한 개념으로만 남을 뿐 다른 분야에 적용할 때 그 한계를 드러내기 마련이다. 가급적 간명한 설명을 통해 하나도 근본 원리를 놓치지 않으려는 저자의 시도가 마음에 들었다. 기타 그 외에도 패스트캠퍼스의 검증된 강의를 기반으로 한다는 점 Pytorch와 친숙해질 기회라는 점 저자의 GitHub에 완성도 높은 구현체가 공개되어 있다는 점이 장점이라고 할 수 있겠다. 내용의 깊이, 구성, 저자의 열정 뭐 하나 떨어지는 구석없이 잘 만들어진 책이라는 생각이 들었다. 누가 읽어야 하는가? 한국어 NLP, 전처리에 고민이 깊었던 분 NLP 기술 전체를 한 눈에 알기쉽게 정리하고 싶은 분 NLP 입문 서적 수준을 넘어서고 싶으신 분 그 외 필자와 같은 NLP 빠돌이 분들 책의 구성 및 요약 이 책은 크게 네 부분으로 구성되며, 각 장에서 다루는 내용을 요약해 보았다. 1. NLP 학습을 위한 준비운동(0 ~ 3장) 딥러닝 및 NLP의 개요, 기술의 발전과정 기초수학에 대한 내공쌓기 개발실습환경 구축 및 Pytorch 기본문법 학습 2. NLP 기초체력 다지기(4 ~ 7장) 정규표현식 등 전처리의 다양한 기술 학습 원핫인코딩, TF-IDF, 벡터유사도, 중의성 해소 등 NLP 입문서 수준의 주요 개념 정리 word2vec 등 단어임베딩 및 시퀀스 모델링, 텍스트 분류 등 중급 내공 쌓기 3. 기계번역(9장 ~ 11장) SRILM, NNLM 등 언어모델의 활용법 및 n-gram실습 seq2seq, 어텐션 등을 활용한 신경망 기계번역 다국어 신경망 및 트랜스 포머 등 심화주제 소개 4. __심화주제(12장 ~ 15장) 강화학습 적용 및 자연어 생성 듀얼리티, NMT 시스템 구축, 전이학습 등 최신기술 소개 요약하며… 언제나 그렇듯이 이런 양서는 왜 늦게 나오는지 모르겠다. 그동안 다른책에 낭비된 시간이 아깝기 때문일까. 꽤 어려운 주제들을 언급하고 있는데 반해 솔직히 꽤 재미있게 읽었다. 물론 정독하고 수식까지 음미하려면 꽤 많은 시간이 소요될 것 같다. 책을 읽으며 가장 인상 깊었던 점은 수준높은 주제를, 저자가 그동안 공부하고 노력하고 연구했던 지식들 전부를, 어떻게든 한정된 지면안에 간결하게 쏟고자하는 노력이 느껴졌다는 점이다. 수학, 통계학 등 기초 체력이 잘 다져진 분들에게는 NLP 기술이 잘 집대성된 양서를 만났다는 느낌이 들 것이다. 아쉬웠던 점이 있다면 그런 저자의 열정때문에 설명에 다소 축약이 많아 기초 내공이 약하거나 입문자이신 분들한테는 실습 환경 하나 구축하기 조차 벅차다는 생각이 드실 것 같다. 더불어 수식으로 열거된 설명 부분은 좀 더 자세하고 쉽게 설명이 되어있다면 교과서로 쓰여도 무난하지 않을까 하는 약간의 아쉬움은 들었다. 하지만 이런 아쉬운 점은 전체 장점 대비 극히 작은 영역이기에 별로 중요하지 않다. &lt;한빛미디어 출판사&gt; 믿고보는 “한빛미디어 출판사”. IT분야에서 독보적인 양질의 도서를 출판하는 회사입니다. “나는 프로그래머다” 팟캐스트 후원, DevGround2019 행사, 리뷰어 모집, 다양한 학습 지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT분야에 관심있으시다면 한빛미디어의 책으로 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review book nlp deep learning camp",
    "url": "/review/2019/08/05/review-book-nlp-camp/"
  },{
    "title": "[Paper] 데이터 사이언스 관련 해외학회 모음",
    "text": "개요 데이터 사이언스 관련 해외학회 목록을 정리해 보았습니다. 목차 Machine Learning / Data Mining Information Systems Computer Vision / Image Processing 기타 MeetUp / Community Machine Learning / Data Mining IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Neural Information Processing Systems (NIPS) European Conference on Computer Vision (ECCV) International Conference on Machine Learning (ICML) IEEE International Conference on Computer Vision (ICCV) ACM SIGKDD International Conference on Knowledge discovery and data mining Meeting of the Association for Computational Linguistics (ACL) ACM SIGMOD International Conference on Management of Data Conference on Empirical Methods in Natural Language Processing (EMNLP) AAAI Conference on Artificial Intelligence ACM International Conference on Web Search and Data Mining (WSDM) International Conference on Data Engineering Workshops (ICDE) IEEE International Symposium on Information Theory (ISIT) ACM SIGIR Conference on Research and development in information retrieval (SIGIR) IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Conference of the International Speech Communication Association (INTERSPEECH) International Joint Conference on Artificial Intelligence (IJCAI) ACM International Conference on Information and Knowledge Management (CIKM) British Machine Vision Conference (BMVC) International Conference on Language Resources and Evaluation (LREC) IEEE International Conference on Data Mining (ICDM) Allerton Conference on Communication, Control, and Computing International Conference on Computational Linguistics (COLING) Asian Conference on Computer Vision (ACCV) SIAM International Conference on Data Mining (SDM) International Conference on Pattern Recognition (ICPR) IEEE International Conference on Image Processing (ICIP) ACM Conference on Recommender Systems (RecSys) Conference On Learning Theory (COLT) Conference on Uncertainty in Artificial Intelligence (UAI) International Conference on Learning Analytics And Knowledge (LAK) Conference of the European Chapter of the Association for Computational Linguistics (EACL) European Conference on Machine learning and knowledge discovery in databases (ECMLPKDD) International Conference on Artificial Intelligence and Statistics (AISTATS) International Joint Conference on Neural Networks (IJCNN) International Symposium on Information Processing in Sensor Networks (IPSN) European Conference on Artificial Intelligence (ECAI) International Conference on Biometrics (ICB) ACM International Conference on Multimedia Retrieval (ICMR) IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG) Artificial Intelligence in Medicine (AIME) International Conference on Information Fusion (FUSION) International Conference on Intelligent User Interfaces (IUI) IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS) International Conference on Document Analysis and Recognition (ICDAR) International Conference on Automated Planning and Scheduling (ICAPS) Principles of Knowledge Representation and Reasoning (KR) IEEE International Conference on Autonomic Computing (ICAC) IEEE International Conference on Big Data (Big Data) IEEE Spoken Language Technology Workshop (SLT) Conference on Innovative Data Systems Research (CIDR) Workshop on Statistical Machine Translation (WMT) IEEE International Conference on Systems, Man and Cybernetics (SMT) European Conference on Information Retrieval (ECIR) International Conference on Artificial Intelligence and Soft Computing (ICAISC) Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) IEEE Conference on Computational Intelligence and Games (CIG) IAPR International Workshop on Document Analysis Systems (DAS) International Conference on Intelligent Tutoring Systems (ITS) IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS) International Conference on Affective Computing and Intelligent Interaction (ACII) Intelligent Virtual Agents (IVA) IEEE International Conference on Fuzzy Systems (FUZZ) International Conference on Computational Linguistics and Intelligent Text Processing (CICLing) International Conference on Machine Learning and Applications (ICMLA) International Conference on Complex, Intelligent and Software Intensive Systems (CISIS) International Conference on Tools with Artificial Intelligence (ICTAI) International Conference on Neural Information Processing (ICONIP) IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology (WI-IAT) International Conference on Integration of AI and OR Techniques in Constraint Programming (CPAIOR) Learning and Intelligent Optimization (LION) Workshop in Computational Approaches to Subjectivity and Sentiment Analysis (WASSA) Intelligent Data Analysis (IDA) Combinatorial Pattern Matching (CPM) International Symposium on Visual Computing (ISVC) International Conference on Fuzzy Systems and Knowledge Discovery (FSKD) European Symposium on Artificial Neural Networks (ESANN) International Conference on Artificial Neural Networks (ICANN) International Symposium on String Processing and Information Retrieval (SPIRE) International Conference on Agents and Artificial Intelligence (ICAART) European Conference on Logics in Artificial Intelligence (JELIA) International Joint Conference on Natural Language Processing (IJCNLP) International Conference on Advances in Swarm Intelligence (ICSI) Data Warehousing and Knowledge Discovery (DaWaK) Recent Advances in Natural Language Processing (RANLP) International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU) Iberoamerican Congress on Pattern Recognition (CIARP) IEEE International Conference on Intelligence and Security Informatics (ISI) International Conference on Hybrid Artificial Intelligent Systems (HAIS) International Symposium on Applied Machine Intelligence and Informatics (SAMI) Information Systems International Conference on Very Large Databases (VLDB) ACM SIGKDD International Conference on Knowledge discovery and data mining (SIGKDD) ACM SIGMOD International Conference on Management of Data (SIGMOD) ACM Symposium on Theory of Computing (STOC) International Conference on Data Engineering Workshops (ICDE) IEEE International Symposium on Information Theory (ISIT) ACM SIGIR Conference on Research and development in information retrieval (SIGIR) International Conference on High Performance Computing, Networking, Storage and Analysis (SC) IEEE Symposium on Foundations of Computer Science (FOCS) ACM International Conference on Information and Knowledge Management (CIKM) ACM European Conference on Computer Systems (EuroSys) Conference on File and Storage Technologies (FAST) International Conference on Autonomous agents and multi-agent systems (AAMAS) Hawaii International Conference on System Sciences (HICSS) Mining Software Repositories (MSR) ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA) SIAM International Conference on Data Mining (SDM) International Conference on Financial Cryptography and Data Security (FC) International Conference on Parallel Architectures and Compilation Techniques (PACT) Conference On Learning Theory (COLT) International Conference on Extending Database Technology (EDBT) International Conference on Learning Analytics And Knowledge (LAK) ACM Symposium on Applied Computing (SAC) European Conference on Machine learning and knowledge discovery in databases (ECMPKDD) IEEE/ACM International Conference on Automated Software Engineering (ASE) International Symposium on Software Testing and Analysis (ISSTA) International Conference on Artificial Intelligence and Statistics (AISTATS) Conference on Designing Interactive Systems (DIS) IEEE International Conference on Pervasive Computing and Communications (PERCOM) ACM Symposium on Information, Computer and Communications Security (AsiaCCS) International Symposium on Information Processing in Sensor Networks (IPSN) IEEE/ACM International Conference on Computer-Aided Design (ICCAD) International Society for Music Information Retrieval Conference (ISMIR) ACM International Conference on Multimedia Retrieval (ICMR) International Conference on Information Systems (ICIS) IEEE International Conference on Cloud Computing Technology and Science (CloudCom) International Conference on Information Fusion (FUSION) ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (SIGPATIAL) Asilomar Conference on Signals, Systems and Computers (ACSSC) Principles of Knowledge Representation and Reasoning (KR) ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS) ACM International Symposium on Mobile Ad Hoc Networking and Computing (MOBIHOC) European Conference on Information Systems (ECIS) IEEE International Conference on Multimedia and Expo (ICME) IEEE International Conference on Big Data (Big Data) Symposium on Theoretical Aspects of Computer Science (STACS) IEEE International Geoscience and Remote Sensing Symposium (IGARSS) IEEE/IFIP Network Operations and Management Symposium (NOMS) Information Theory and Applications Workshop (ITA) Conference on Information Sciences and Systems (CISS) International Conference on Tangible, embedded, and embodied interaction (TEI) Conference on Innovative Data Systems Research (CIDR) IEEE International Conference on Systems, Man and Cybernetics (SMC) IEEE Symposium on Mass Storage Systems and Technologies (MSST) IEEE Information Theory Workshop (ITW) European Conference on Information Retrieval (ECIR) IEEE Real-Time Systems Symposium (RTSS) Americas Conference on Information Systems (AMCIS) International Conference on Advanced Information Systems Engineering (CaiSE) Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD) International Conference on Intelligent Tutoring Systems (ITS) International Conference on Mobile Data Management (MDM) International ACM/IEEE Joint Conference on Digital Libraries (JCDL) International Conference on Database Theory (ICDT) International Conference on Complex, Intelligent and Software Intensive Systems (CISIS) ACM International on Systems and Storage Conference (SYSTOR) Data Compression Conference (DCC) Pacific Asia Conference on Information Systems (PACIS) International Conference on Database Systems for Advanced Applications (DASFAA) International Conference on Information and Communication Technologies and Development (ICTD) International Conference on Web and Internet Economics (WINE) EUROMICRO Conference on Software Engineering and Advanced Applications (SEAA) IEEE International Workshop on Information Forensics and Security (WIFS) International Conference on Research Challenges in Information Science (RCIS) International Symposium on String Processing and Information Retrieval (SPIRE) International Conference on Search based Software Engineering (SSBSE) Annual International Conference on Digital Government Research (DGO) Data Warehousing and Knowledge Discovery (DaWaK) International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU) International Conference on Enterprise Information Systems (ICEIS) Asian Symposium on Programming Languages and Systems (APLAS) IEEE International Conference on Intelligence and Security Informatics (ISI) International Conference on Hybrid Artificial Intelligent Systems (HAIS) International Conference on Web Information Systems and Technologies (WEBIST) Computer Vision / Image Processing IEEE Conference on Computer Vision and Pattern Recognition (CVPR) European Conference on Computer Vision (ECCV) IEEE International Conference on Computer Vision (ICCV) IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) ACM Multimedia (ACMMM) British Machine Vision Conference (BMVC) Asian Conference on Computer Vision (ACCV) International Conference on Pattern Recognition (ICPR) IEEE International Conference on Image Processing (ICIP) Medical Image Computing and Computer Assisted Intervention (MICCAI) Workshop on Applications of Computer Vision (WACV) International Conference on Biometrics (ICB) ACM International Conference on Multimedia Retrieval (ICMR) IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG) IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS) ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems International Conference on Document Analysis and Recognition (ICDAR) European Signal Processing Conference (EUSIPCO) IEEE International Conference on Multimedia and Expo (ICME) Eye Tracking Research &amp; Application (ETRA) IEEE International Geoscience and Remote Sensing Symposium (IGARSS) ACM Multimedia Systems Conference (MMSys) IEEE International Conference on Systems, Man and Cybernetics (SMC) IAPR International Workshop on Document Analysis Systems (DAS) Working Conference on Advanced Visual Interfaces (AVI) IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS) International Conference on Affective Computing and Intelligent Interaction (ACII) International Workshop on Quality of Multimedia Experience (QoMEX) IEEE Symposium on Visual Analytics Science and Technology (VAST) IEEE Workshop on Multimedia Signal Processing (MMSP) Picture Coding Symposium (PCS) IEEE Pacific Visualization Symposium (PacificVis) International Conference on Machine Learning and Applications (ICMLA) ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (i3D) Data Compression Conference (DCC) IEEE Virtual Reality Conference (VR) Workshop in Computational Approaches to Subjectivity and Sentiment Analysis (WASSA) IEEE International Workshop on Information Forensics and Security (WIFS) International Symposium on Visual Computing (ISVC) ACM Symposium on Virtual Reality Software and Technology (VRST) International Conference on 3D Web Technology (Web3D) International Conference on Information Visualization (InfoVis) ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC) International Conference on Digital Image Computing Techniques and Applications (DICTA) International Conference on Image Analysis and Processing (ICIAP) International Conference on Virtual Systems and Multimedia (VSMM) International Conference on Image Analysis and Recognition (ICIAR) International Conference on Systems, Signals and Image Processing (IWSSIP) Iberoamerican Congress on Pattern Recognition (CIARP) ACM workshop on Information hiding and multimedia security (IHMMSEC) International Symposium on Mathematical Morphology (ISMM) International Conference on Control, Automation, Robotics and Vision (ICARCV) 기타 MeetUp / Community International Winter School on Big Data (BigDat) International Conference on Statistical Language and Speech Processing (SLSP) International Summer School on Deep Learning (DeepLearn) International Conference on Algorithms for Computational Biology (AlCoB) International Conference on Language and Automata Theory and Applications (LATA) International Conference on the Theory and Practice of Natural Computing (TPNC) Pycon SciPy DjangoCon Europe EuroPython EuroScipy O’Reilly Open Source Convention (OSCON) PyData PyGotham AWS summit Microsoft Azure + AI Conference Microsoft Ignite Google Cloud Next TensorFlow Dev Summit 기타 자세한 사항 및 다른 분야의 학회정보가 필요하신 분들은 guide2research로 접속하시면 됩니다. ※ 수시 업데이트 예정(오류 또는 최신정보가 있으신 경우 댓글로 알려주시면 감사하겠습니다.)",
    "tags": "papertomath academy list dev",
    "url": "/dev/2019/07/10/dev-papertomath-abroad-academy-list/"
  },{
    "title": "[리뷰] 게임으로 익히는 코딩 알고리즘",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"게임으로 익히는 코딩 알고리즘\"을 읽고 얻은 지식을 정리한 글입니다. 목차 알고리즘으로 게임하는 세상이 올 줄이야… 개발인생 15년. 알고리즘이 중요하다고 느낄 때 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 알고리즘으로 게임하는 세상이 올 줄이야… 신선하다. 세상이 빠르게 변하고 점점 좋아진다는 건 알았지만 게임으로 알고리즘을 배우는 책이 나오리라고는 생각하지 못한것이 사실이다. 게임이든, 책이든 수단이 뭣이 중요한가 생각이 들기도 했지만 내 안에서는 알고리즘은 왠지 수학과 친밀하고 교과서 같은 분위기에서 벗어나면 안된다는 일종의 프레임이 형성되어 있었던 것 같다. 책을 펼치자마자 코딩게임 공식 사이트로 바로 들어가 보았다. 링크를 클릭하면 구글 OAuth 덕분에 별도 회원가입 없이 로그인이 가능하다. 구글 계정 등으로 로그인한 후 아래 그림과 같은 화면을 볼 수 있다. 처음이라 복잡해 보이지만 단순히 비교문 if만 알면 쉽게 풀 수 있는 쉬운 문제이다. 빨간색 동그라미 부분의 코드를 작성한 후 PLAY ALL TESTCASES를 클릭하면 좌측 상단의 멋진 게임실행 화면을 볼 수 있다. 그 후 GOT IT 또는 SUBMIT 버튼을 누르면 결과가 제출하고 메인화면으로 이동하게 된다. 어떤가 꽤 참신하지 않은가? 너무 참신하여 혹시 게임 플랫폼 광고용으로 만든 책은 아닌건지, 혹은 첫인상은 매혹적으로 다가왔는데 실상 알고리즘 핵심은 텅텅 비어있는 껍데기에 불과한 책은 아닌지 의심이 되기 시작했다. 그러나 기우였다. 사실 지금까지 읽은 어떤 알고리즘 책 보다도 쉽게 설명하고, 가독성을 높이기 위한 시각화 처리가 잘 되어있으며, 실전에 필요한 케이스를 다룸으로써 집중력을 높여준다. 아래 그림에서 보듯 주요 알고리즘을 시각적으로 쉽게 설명해 준다. Science에 가까운 흥미를 떨어뜨리는 요소는 실전에 필요한 만큼으로 간결하게 압축하여 설명하였고 대신 이해의 깊이가 필요한 것은 놓치지 않고 다루었다. 알고리즘의 핵심개념과 시간복잡도, 공간복잡도를 알기 쉽게 설명한 후 초보자를 위해 조건문, 반복문, 인코딩과 관련된 기본기를 탄탄하게 해주는 게임부터 시작한다. 이어 2차원 배열, 큐, 스택, 해시맵 등의 자료구조를 다루고 탐욕 알고리즘, 그래프, 탐색(너비우선, 깊이우선), 재귀, 트리 등 알고리즘에서 가장 중요한 개념도 다룬다. 심지어 다익스트라, 동적프로그래밍까지 다루며 화룡정점을 찍는다. 흥미뿐만 아니라 지식의 깊이도 놓치지 않으려는 노력이 돋보였다. 구성 측면에서도 학습 능률을 높이기 위한 장치로 더 생각해 봅시다 코너를 두었는데 이 부분의 구성이 가장 마음에 든다. 배운 것을 토대로 나아가야 할 방향을 제시하고 흥미를 유발시켜 준다. 10장의 폭탄의 위치를 찾는 게임의 경우 학부시절 혼자서 만들었던 게임 생각이 났는데 그렇게 고심해서 혼자 만들었던 과정에서 얻은 내공 덕에 지금 먹고 살 수 있다고 생각한다. 여러 Chapter를 거치면서 그런 유익한 시간이 된다면 독자들에게는 큰 보탬이 될 것이다. 내용의 깊이, 구성, 흥미, 참신함 뭐 하나 떨어지는 구석이 없어서 솔직히 단점을 잘 못찾았다. 잘 만들어진 책이라는 생각이 들었다. 개발인생 15년. 알고리즘이 중요하다고 느낄 때 사실 한국 IT개발 시장에서 알고리즘을 쓸 일이 흔치는 않다. 알고리즘은 보통 각 언어들에 Library 형태로 잘 구현이 되어 있으며 잘 구현된 예제를 보며 찍어내기를 얼마나 빨리 하느냐가 그동안 IT시장에서 바라는 인재상이었기 때문에 쓸 일이 많지 않았다. 정말 슬픈현실이다.지금도 크게 다르진 않으나 AI, 딥러닝 분야를 필두로 뭔가 변화가 보이기 시작했다. 보다 수학, 통계학에 가까운 Science를 다룰 줄 아는 인재가 필요해지고 있다. 알고리즘이 뭔데? 간단히 말해서 문제를 해결하기 위한 (가급적 최선의 방법을 찾는)절차라고 할 수 있다. 그런데 그게 왜 중요한데? 문제 자체도 해결할 수 있고 시간복잡도, 공간복잡도 등을 예상하여 자원 사용을 효율화하고 연산 속도를 높이는데 큰 도움을 준다. 어차피 컴퓨터로 돌리면 빠른거 아냐? 이해를 돕기위해 간단한 예시를 들어보겠다. 이 책의 11장에서 다루는 외판원 문제를 생각해보자. 외판원 문제? (출처 - 위키백과) 여러 도시들이 있고 한 도시에서 다른 도시로 이동하는 비용이 모두 주어졌을 때, 모든 도시들을 단 한 번만 방문하고 원래 시작점으로 돌아오는 최소 비용의 이동 순서를 구하는 것이다. 그래프 이론의 용어로 엄밀하게 정의한다면, “각 변에 가중치가 주어진 완전 그래프(weighted complete graph)에서 가장 작은 가중치를 가지는 해밀턴 순환을 구하라”라고 표현할 수 있다. 도시가 2개라면, A-&gt;B, B-&gt;A [2개] 도시가 3개라면, A-&gt;B-&gt;C, A-&gt;C-&gt;B, B-&gt;A-&gt;C, B-&gt;C-&gt;A, C-&gt;A-&gt;B, C-&gt;B-&gt;A [6개] … 도시가 10개라면, [3,628,800개]의 경우의 수가 나온다. 핵심은 도시가 n개라면 이동 가능한 경로의 수는 n!이 된다는 것이다. 시간 복잡도가 O(n!)가 되는 것이다. A라는 컴퓨터에서 위 알고리즘을 수행하는 데 걸리는 시간 O(1)을 무난하게 1초라고 가정하자. 그리고 우주의 나이를 계산해보자. 지금까지의 관측한 결과를 바탕으로 ΛCDM 모형을 적용하면 우주의 나이는 약 137.98 ± 0.37억 년으로 추정된다. 비교를 위해 년단위를 위에서 가정한 초단위로 변경해보자. 1년 = 365일 = 365 * 24시간 = 8,760시간 = 8,760 * 60분 = 525,600분 = 525,600 * 60초 = 31,536,000초 이므로, 우주의 나이 = 1.3798 * 10^{10} * 3.1536 * 10^7 = 약 4.35 * 10^{17}초 가 나온다. O(n!) &gt; O(2^n) 2^{10} = 1024 = 10^3 두가지 상황을 어림잡아 17 × 10 /3 = 약 57. 위의 가정하에 대략 57개의 도시만 있으면 A컴퓨터로 우주의 나이 만큼 연산을 수행해야 결과를 구할 수 있다는 의미가 된다. (예시를 위해 대략적으로 계산하며 생긴 오차가 있을 수 있으니 양해 부탁드린다.) 그리고 언제 또 쓰이는데? 구글 검색 서비스가 2초 이상 걸린다면 고객이 구글을 사용할까? 구글의 가장 큰 자료는 피처수가 10억개에 육박하는 것도 있다. 아무리 서버 컴퓨팅 파워가 버텨준다 쳐도 조금이라도 빠르게 검색 알고리즘을 개발해야 하지 않을까? 모바일에서 딥러닝을 작동시키려면 한정된 자원을 최대한 활용할 수 있어야 한다. 딥러닝의 연산량도 마이크로 Sec 차이가 최종 성능에는 엄청난 영향을 미친다. 일반적인 App도 자원을 얼마냐 쓰느냐가 배터리 소모에 직결된다. 구글이 온도 1도씨라도 줄이기 위해 IDC센터를 폭포 옆으로 이전한 이야기를 들은적이 있는가? 알고리즘이 발열량에도 영향을 미친다. 그 뿐만이 아니다. 알고리즘을 사칙연산처럼 쉽게 쓰는 것은 어렵다. 상황에 따라 다르게 쓰일 수 있기 때문이다. 그 개념에 한바탕 푹 빠져든 채로 살아야 숨쉬듯이 자연스럽게 몸에 베는 것이다. 단순히 알고리즘 지식만 얻는 것도 아니다. 자연스레 프로그래밍 스킬도 얻게 되고 사고의 속도에 박차를 가할 수 있는 된다. 그럼에도 알고리즘 공부를 하지 않을 것인가? 누가 읽어야 하는가? 초보 프로그래머, 컴퓨터 전공 학부 초년생 경력은 많아도 찍어내기 신공에만 탁월한 응용력이 부족한 현직 개발자 기타 프로그래밍에 관심이 있으신 분 데이터 분석, 경제학 등 프로그래밍을 알면 큰 시너지 성과를 얻을 수 있는 타 학문 전공자에게도 알고리즘 개념의 기초를 다지기에 정말 도움이 되는 책이다. 책의 구성 및 요약 이 책은 크게 세 부분으로 구성되며, 각 장에서 다루는 내용을 요약해 보았다. 1. 알고리즘을 위한 최소한의 기초지식(1 ~ 5장) 알고리즘의 핵심 개념과 시간복잡도, 공간복잡도 개념 파악 조건문, 반복문, 인코딩 등 기본기 전수 코딩게임에서 노는 방법 그 외 프로그래밍 기본 문법과 데이터 타입에 익숙해질 수 있다. 2. 핵심 자료구조와 알고리즘(6 ~ 13장) 배열, 큐, 스택, 해시맵 등의 자료구조 탐욕 알고리즘, 그래프, 탐색(너비우선, 깊이우선), 재귀, 트리 등 알고리즘 실전에서 알고리즘을 활용하게 되었을 때 대처하는 자세 3. 고급 알고리즘(14장 ~ 15장, 부록) 다익스트라, 원형큐, 동적프로그래밍 등 고급 알고리즘 취업한 선배들이 알려주는 Tip 및 수도코드 제시 요약하며… 예전과는 달리 알고리즘을 그림으로도 공부하고 심지어 게임으로도 공부할 수 있는 멋진 세상이 왔다. 딥러닝 등의 열풍으로 우리나라에서도 찍어내기식 개발자 양산에서 Science에 튼튼한 프로그래머를 요구하는 문화도 다가오고 있다. 한 10년 만 늦게 태어났더라면 얼마나 좋았을까 그런 생각도 해본다. 위에서 언급한 바와 같이 알고리즘은 컴퓨터 공학의 핵심이자 정수이다. 중요성은 두말할 나위 없고 어떻게 깊이있게 빨리 익히는가가 관건이라 할 수 있는데 시중에 나와있는 어떤 책보다 알고리즘에 흥미롭고 빠르게 적응할 수 있을 것이라 생각한다. 큰 틀만 잡으면 이젠 깊이있는 교과서, 참고서도 두렵지 않을 것이다. 뭐든지 알면 개뿔도 아닌데, 모르면 그렇게 무서울 수가 없다. 이 책으로 알고리즘을 개뿔처럼 보실 수 있게 되시길 바란다. &lt;한빛미디어 출판사&gt; 믿고보는 “한빛미디어 출판사”. IT분야에서 독보적인 양질의 도서를 출판하는 회사입니다. “나는 프로그래머다” 팟캐스트 후원, DevGround2019 행사, 리뷰어 모집, 다양한 학습 지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT분야에 관심있으시다면 한빛미디어의 책으로 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review book algorithm game",
    "url": "/review/2019/07/08/review-book-game-algorithm/"
  },{
    "title": "통계학 vs 컴퓨터공학, 멋대로 써보는 Data Science 미래에 대한 소고(小考)",
    "text": "개요 통계학과 컴퓨터공학이 보이지 않는 곳에서 작은 다툼을 하고 있습니다. 현 시점만 놓고 봤을 때 이 작은 다툼은 양측 모두 일리를 가진 의미있는 싸움이라고 생각하는데요. 이 부분에 초점을 맞추어 정반합의 개념으로 데이터 과학의 미래가 어떻게 변화할지 추측해보려합니다. 그동안의 경험, 감(感), 약간의 공상과학(?)을 가미하여 다소 건방져 보일지 몰라도 소신있게 예측해 보겠습니다. 목차 딥러닝의 약점은 무엇일까? 통계학 : Data Science? AI? 모두 예전에 우리가 했던 일이다. 컴퓨터공학 : 수학, 통계학이던 Data던 전부 컴퓨터 안에 집어넣으면 그만이다. 개인이 생산한 데이터의 라벨링은 누가 해 줄 것인가? 세상 모든 논문들을 딥러닝이 이해할 수 있게 딥러닝은 사이언스가 아닌가? 또, 내부는 블랙박스인가? 요약하며… 딥러닝의 약점은 무엇일까? 누가 감히 알파고 욕하는 소리를 하였는가? - feat.궁예 감히 나 따위가 어떻게 폄하하겠는가. 하지만 모두 다 Yes라고 할 때 No라고 말할 수 있는 습관은 꽤 의미있다고 생각한다. 최근 기업에서 나오는 우스개 소리로 결재가 반려될 경우 “AI, 딥러닝, 빅데이터”라는 마법의 단어를 사용하면 결재를 득할 수 있다는 풍자가 나올 정도로 A.I는 분명 핵심 트렌드이다. 그 중에서도 특히 인류가 쌓아온 지식의 분야에서 딥러닝이 강점을 보이는 분야는 “동영상, 이미지, 음성인식, NLP” 등의 분야일 것이다.이 분야의 공통점은 무엇일까? 대부분 비정형 데이터이면서 동시에 2차 가공을 거친 데이터들로 채워진 분야이다. 이 세상 그대로의 데이터를 컴퓨터가 이해할 수 있도록 전달할 방법이 없기에 아날로그를 디지털화 하는 등 일종의 “변환” 작업을 거친다. 그 과정에서 이 세상의 Real 데이터들은 Computing Vision, 신호처리, 푸리에 변환 등 인류가 장기간 축적해 온 지식들의 손바닥안에서 놀 수 있는 데이터의 형태로 변환된다. Min ~ Max의 유한한 범위를 갖는 일종의 해석가능한 범주 형태의 피처로 변환이 되고 축적된 지식으로 만든 모델의 Input값으로 최적화 되어있는 셈이다. 연산의 시간 복잡도 문제는 컴퓨팅 파워만 충분하다면 해결 가능해졌고, 이러한 가공을 통해 피처 선정 및 추출, 모델의 선택에 있어 경우의 수를 크게 좁혀준 셈이다. 그렇다면 가공을 거치지 않은 Tabular 성격의 일반 데이터는? 딥러닝의 단점을 누구보다 통계학자가 잘 말해준다. 통계학 : Data Science? AI? 모두 예전에 우리가 했던 일이다. 맞다. 통계학에서 보기에 딥러닝은 기존에 존재하던 Neural Net 모델이 여러 층으로 연결된 것일 뿐이고 덕분에 회귀를 여러번 실행 가능하게 하여 전처리를 내부에서 처리하는 정도에 지나지 않는다고 표현한다. 심지어 Logit의 결합일 뿐인데, 이것이 마케팅인가? 기술인가?라고 폄하하는 분도 있다. Science 측면에서 분명 일리가 있는 말이다. “Tabular 기반의 데이터에서 - 즉, 2차 가공을 거치지 않은 세상의 진정한 의미가 담긴 데이터 - 가치있는 Insight를 추출하거나 미래를 예측하기 위해서는 모델링이 핵심인데 컴퓨터 공학도이 과연 그런것을 아느냐? 그런 수학적, 과학적 깊이가 있는가?”라고 다그치신다면 컴퓨터 공학 전공자로서 솔직히 좀 위축됨을 인정하며, 우리의 컴퓨터 공학 분야는 수학, 통계학, 타 분야의 공학의 금자탑이 쌓아놓은 위대한 업적 덕분에 빛나는 것이라고 감사를 표하고 싶다. 컴퓨터공학 : 수학, 통계학이든 Data든 전부 컴퓨터 안에 집어넣으면 그만이다. 컴퓨터 공학은 짧은 역사에도 불구하고 산업과 자본을 이끌어 온 저력이 숨어있다. 바로 컴퓨터를 만들고 그 위에 컴퓨터를 잘 다룰 줄 아는 또 다른 학문을 세워놓았기 때문이다. NLP는 세상의 데이터 기본단위를 바꿀것이다. 필자는 NLP가 현재의 RDB를 비롯한 세상의 대부분의 데이터(적어도 우리가 컴퓨터, 모바일을 사용하면서 생산하는 데이터 만큼은)의 저장 형태 및 기본 단위를 바꿀것이라 생각한다. 딥러닝의 자동화를 위해서 말이다. 자연어를 TF-IDF 등 NLP 기술을 활용하여 Data-Meta 구조 형태의 기본단위를 확립하여 그에 적합하게 고안된 유한한 모델을 갖게 될 것이고, 그렇게 변환된 데이터와 모델들은 인공지능을 위한 가교 역할 일종의 중간 Layer를 담당하게 될 것이다. 데이터는 대부분 이런 기본단위의 규칙을 갖는 반정형 형태로 축적될 것이고, 모든 자료는 Json, XML과 유사하게 Tree등의 자료구조를 활용한 Meta 정보를 쌍으로 가질 것이라 생각한다. Meta도 결국 피처로 쓰이게 될 것이고 세상의 거의 모든 자료는 서적부터 동영상에 이르기까지 이런 형태의 ‘Data-Meta’ 단위로 재편될 것이다. 말도 안된다고? 그정도 데이터가 재편되는데는 사실 그리 오랜 시간이 걸리지 않을 것이다. 요즘 Tabular, Pad 등 글을 생산하는 플랫폼을 전부 컴퓨터 공학에서 주도하고 있기 때문이다. 그래서 Tabular 데이터도 결국은 이미 고안된 유한한 모델링의 일부에 활용될 것이다. 기생산된 데이터의 재편이 어려울지도 모른다. 하지만 빠른 시일내에 원저자 혹은 관련 연구자들이 일종의 라벨링처럼 반정형 형태로의 변환을 가능하게끔 연구해 줄 것이라 생각한다. 어려움에 봉착할지라도 신호처리, 푸리에 변환이 활용된 것처럼 다른 학문에 쌓인 축적된 지식을 가져와 모델링에 활용할 것이라 예측한다. 그 중심에 NLP가 있다. 컴퓨터 공학이 주도해 온 실행력, 스피드, 실행가능성, 실용성, 자동화 그리고 논문 타 학문이 이룩한 지식을 딥러닝이 이해할 수 있는 형태로 피처 형태를 정의하고 모델을 구성하는데 오랜 시간이 걸리리라 생각하는가? 컴퓨터 공학이 가진 “실행력, 스피드, 실행가능성, 실용성, 자동화”의 힘이라면 이미 100년에 가까운 역사동안 알려진 지식이 있다면 구현하여 활용하는데 그리 오랜 시간이 걸리지 않음을 입증해오고 있다. 더욱이 모든 학문 분야는 \"논문\"이라는 일종의 공통된 언어를 사용한다. 논문에는 대부분 가설이 존재하고 검증을 위한 모델이 담겨있다. 딥러닝이 받아들이기 아주 쉬운 구조로 되어있다는 의미이다. 컴퓨터 공학 자체의 축적된 Science 컴퓨터 공학은 기초 Science와 동떨어져 그 깊이가 얕다고 비웃음을 당하곤 한다. 고상한 진리를 찾는데는 분명 뒤떨어진지도 모른다. 하지만 S/W공학 등 짧은 시간동안 빠른 구현 및 활용을 위한 철학을 기저로 한 고민과 무수한 시행착오를 녹인 강한 실행력을 가진 나름의 Science가 존재하기에 고도의 순수 사이언스를 쉽게 받아들이고, 결합하고, 그 과정에서 창조적인 지식이 재탄생한다. 그리고 그 지식을 누구보다도 빠르게 구현할 수 있는 학문 분야를 연구한다.(GAN이 출현하여 AI 트렌드를 지배하는 현 상황이 대표적인 예라고 할 수 있다.) 더불어 언제나 누구든 쉽게 인사이트를 착상할 수 있게 고취시킬 수 있는 환경을 가지고 있다. 때문에 세상의 자본을 쥐락펴락할 수 있는 것이다.(물론 이러한 강점이 인문학적인 측면에서, 기초 과학 진리의 측면에서 사람이 살아가야 할 방향과 일치하느냐고 물으신다면 별개라고 답하고 싶다.) 개인이 생산한 데이터의 라벨링은 누가 해 줄 것인가? 공학분야의 앞으로의 가능성은 이즈음에서 각설하고, 각각의 개인들이 생산한 정보에 대한 라벨링 산업의 성장 가능성에 대해서 언급해보려한다. 예시를 하나 들겠다. 본인이 패드 또는 종이에 글을 쓰고 이를 딥러닝을 이용해 텍스트로 자동 변환하는 솔루션을 개발한다고 가정하자. 이때 [Space]는 어떻게 변환이 될까? 단어 간 띄어쓰기가 1cm일 때 스페이스를 넣을 것인가? 아니면 1mm? 이 애매한 기준을 가지고는 과적합 발생 시 한글자 한글자마다 스페이스가 생겨 사용자로 하여금 노가다를 유발시키는 저질 변환이 이루어질 것이고, 과소 적합시에는 아에 줄 글로 주욱 붙어있어 스페이스를 찾기 어려운 지경에 빠질지도 모른다. 그렇다면 이를 해결하기 위해 무엇이 필요할까? 당연히 데이터다. 더 정확하게 말하면 이 사람이 쓴 글의 이미지와 그에 기반한 텍스트로 변환된 라벨링이 필요하다. 피처가 단순히 이 사람의 라벨링에 기반한 스페이스 부분의 길이 정보 하나라면, 당연히 Min ~ Max의 띄어쓰기 길이가 존재할 것이고 한 글자 간 거리또한 유요한 범위가 존재할테니, 아주 간단한 경우에는 2개의 길이가 겹치는 지점에 대한 학습을 수행하여 정확도를 판별할 것이고, 더 나아간다면 특정 글자또는 단어가 들어가는 경우 스페이스의 길이가 줄어드는 특징을 잡아내어 정확도를 높일 수도 있을 것이다. 어쨌든 누군가는 라벨링을 해야한다. 물론 사람의 눈은 이미 타인의 글도 상당히 정확하게 읽어내는 능력이 있으므로 라벨링 대행업체가 존재하겠지만 결국 딥러닝의 품질을 향상시키기 위해서 글쓴이의 라벨링이 직접 필요해질 수 밖에 없다. 그런데 과연 사람들이 이 귀찮은 라벨링을 순순히 참여할까? 이 문제에 대한 몫은 솔루션을 개발하는 사업가의 몫이다. 바로 개인들의 데이터에 대한 라벨을 어떻게 자연스럽게 어렵지 않게 수집할 것인지 말이다. 이 예는 비단 NLP관련 극히 일부의 사례다. 앞으로 모바일을 통해서 더 많은 개인 데이터가 수집될 것이다. 개인화된 라벨을 마치 산소 마시듯이 편리하고 자연스럽게 축적시켜 기왕이면 개인정보 관련 본인의 동의여부 통제하에 제공될 수 있는 플랫폼을 가진 회사가 향후 딥러닝을 위시한 인공지능의 미래를 주도할 큰 축이 될 것임을 조심스럽게 예측해본다. 세상 모든 논문들을 딥러닝이 이해할 수 있게 딥러닝의 미래는 이 주제를 마지막으로 마치려한다. 위에서도 잠깐 언급한 바와 같이 타 학문의 축적된 기술과 이론을 피처 및 모델과 연동시키는 분야가 유망할 것으로 판단한다. 이 산업 분야는 상기의 과제 외에도 논문, 연구와 관련된 메카가 될 것이다. 더 나아가 특허청과 유사한 역할로 블록체인을 활용하여 최초 지식 생산자부터의 위변조 감지도 담당할 것이고, 축적된 기술을 피처와 연관짓는 과정의 산출물로 특허 지식의 실현가능성, 적합성 등을 판단하여 특허 등록에 대한 엄밀한 예행 판단을 서비스 할 수 있게 될지도 모른다. 뉴턴과 라이프니치의 미적분 발명 논쟁도 이젠 역사속으로 사라진다. Review 논문의 역할과 유사한 Reference Map + 그 위에 핵심 아이디어(해당 논문이 전달하고자 하는)만 간략히 Graph형태로 배치될 것이고, 연구 지식이 가장 효율적인 형태로 축적된 신 인류의 지식 저장소 역할을 수행할 것이며, 사회적으로도 단일국가 탄생을 견인할 가능성도 까지 이어질지도 모르겠다고 한다면 너무 과도한 생각일까? 학문적 깊이와 연결되기 힘든 일반인들의 창의적인 발상을 기존 학문이 축적한 깊이에 연결시켜 줌으로써 인류 지식의 초고속 발전에 기여하게 될지도 모른다. 딥러닝은 사이언스가 아닌가? 또, 내부는 블랙박스인가? 흔히들 딥러닝은 Science가 아니라고 말한다. 딥러닝 관련 논문 중 전체 매커니즘에 관해 수식으로 표현하는 논문이 없다. 엄밀한 증명을 적용할 수식이 없고 더 나아가 알파고를 이길 수 있었던 규칙이 무엇이었는지, 어떤 알고리즘으로 승리한 것인건지 만든 창조자도 그 어떤 사람도 설명을 못한다. 그래서 블랙박스라고 표현하기도 한다. 개인적으로는 엄밀히 블랙박스는 아니라고 생각한다. 특정 은닉층의 값을 출력해보면 weight의 값이 0.32943.., bias가 0.324, .. 등 수치적으로 분명히 확인할 수 있으니 보이지 않는다고 표현할 수는 없다. 다만, 내부에 엄청나게 많은 노드와 가중치, 그리고 노드들의 활성 여부를 사람의 기억력으로는 전체를 바라볼 수 없어 수식과 같은 연역적 규칙으로 표현하지 못하는 것이다. 아직까지 사람이 가진 수식으로는 표현 못하는 거대함이 숨어있는 것이다. 사실 노드 하나하나의 활성화 과정은 Logit과 거의 유사하므로 부분만 놓고보면 수식으로 충분히 표현할 수 있음은 물론 매우 쉽다. 하지만 전체를 설명 못하는 것이다. 그렇다면 그냥 동작방식을 모른채로 살아가야 하는가? 원하는 결과만 얻으면 되는가? 그렇지 않다. 인류는 어떤 형태로든 항상 답을 구해왔으니 결국은 이뤄내지 않을까? 이미 내부를 들여다보기 위한 여러 시도들이 이루어 지고 있는것으로 알고있다. 특히 CNN과 같은 Vision 분야의 경우 피처 자체가 시각 데이터임에 착안하여 층별 시각화 이미지를 통해 내부를 들여다 보고 전체 로직을 설명하기 위한 연구가 이루어지는 것으로 알고 있다. 이와 관련하여 앞으로 개인적으로 해보고 싶은것이 있다. 전체 숲은 모를지언정 부분의 판단이 나오게 된 원인을 수식으로 찾아내는 시도를 해보고 싶다. 특정 결론이 나오기까지 영향을 미친 노드들은 전체노드는 아닐 것이다. 관련 노드들 N개만 추출하여 해당 노드간의 수식을 구한다면 전체를 추적하는 것보다는 분명 쉽게 수식을 도출할 수 있을것이라 생각한다. 물론 ReLU함수와 같이 구해도 별 의미도 없는 수식이 등장할지도 모르겠다. 하지만 회귀와 같은 유의미한 수식이 도출될 가능성도 있다. 학습이 부족하여 ReLU 형태의 수식이 등장한 것인지도 모른다. 데이터 사이언스의 고유 업무 중의 하나가 패턴을 찾아내는 일이므로 패턴속의 규칙을 찾아내려는 시도 및 연구 결과가 모여 이 무의미한 수식들을 유의미한 수식으로 바꿔갈 것으로 기대한다. 더 파격적으로 생각하면 수학에 새로운 방식의 표현이 등장할 것으로 예상한다. Sigma와 같이 동일 패턴의 묶음을 상징하는 기호가 많이 등장하여 천문학적인 IF의 갯수를 파격적으로 줄여주거나, 수없이 다양한 패턴이 표준 기호화되어 그 데이터 위의 패턴을 찾으려는 시도가 진행되지 않을까 예측해본다. 너무 어려운 분야의 이야기를 짧은 지식으로 표현하였기에 허무맹랑한 이야기가 되었는지 모르겠으나 일부 역사에서 그래왔듯이 이 허무맹랑함이 인류의 과학 문명을 업그레이드시키는 씨앗이 되었으면 좋겠다. 요약하며 위에서 언급한 크게 세가지의 예측을 보며 필자의 짧은 지식에 눈쌀이 찌푸려 지시거나, 논리적인 오류에 너만의 개똥철학에 경의를 표한다고 하실지 모르겠다. 하지만 개요에서 언급한 바와 같이 표현의 자유로 소신껏 포스팅한 자료이니 가급적 재미로 너그러이 봐주셨으면 감사하겠다. 개인적으로 위와 관련된 소재의 연구를 하고싶고 또는 유관 산업에서 일을 하고 싶다. 만에 하나라도 기회가 된다면 주도하고도 싶은 마음이다. 그 중에서도 ‘세상 모든 논문들을 딥러닝이 이해할 수 있게’라는 소제목으로 표현한 분야에 가장 관심이 많은데 네이처 논문지, 특허청 같은 유사 기관들이 이미 설립되어 있으므로 그 곳에 들어가서 위처럼 변화를 주도하던가, 아니면 꿈틀대는 태동기로 준비 중인 회사를 찾아 들어가던가, 아니면 새로 만들던가 하고 싶지만 당장 생업에 종사하느라 열심히 갈고 닦을 시간이 너무나도 부족함에 눈물이 난다. 하루종일 데이터 사이언스 분야만 할 수 있다면 정말 잘할 수 있을것 같은데 적지 않은 나이에 새로운 분야로 나아가는게 겁이나는 것도 사실이다. 하지만 나약한 자가 되어 핑계만 늘어놓고 싶지는 않다. 그래서 평소 http://arxiv.org/ 논문 중 NLP는 놓치지 않고 열심히 읽으며, 최신 지식에 뒤쳐지지 않도록 내 뉴런에 Core Incremental만 잘 연결시켜 나가고 이 논문을 인용하는 산업체, SNS 동향을 수집, 분석하며 예의주시하는 중이다. 아무쪼록 긴 글임에도 여기까지 읽어주심에 깊이 감사드린다. 주) 참고로, 필자가 블로그 이름을 TheoryDB라고 지은 이유는, 바로 이 논문들을 딥러닝(또는 또 다른 개념의 AI라고 표현해야 할지도 모르겠다.)이 이해할 수 있도록 ‘Data-Meta’ 구조의 데이터베이스로 재편하고 싶은 욕구 때문이다.",
    "tags": "think ai ml dl data science future r competition",
    "url": "/think/2019/06/25/think-future-ai/"
  },{
    "title": "[Competition] 데이터 과학 경진대회 사이트 모음",
    "text": "개요 국내/외 데이터 과학 경진대회 목록을 정리해 보았습니다. 목차 데이터 과학 경진대회란? 데이터 과학 경진대회 사이트 모음 데이터 과학 경진대회란? 데이터 과학 분야로의 진출을 희망하는 사람이라면 데이터 과학에 대한 학습 및 연구를 필요로 할 것이다. 본인의 내공을 키우기 위한 첫 단계는 데이터 확보이며, 두번째로 모델 구현 및 기계학습에 필요한 개발 환경 구축 그리고 마지막으로 이를 뒷받침하는 컴퓨팅 파워가 필요하다. 나아가 데이터 사이언스로서의 스킬을 공신력있게 입증할 필요도 있을 것이다. 데이터 과학 경진대회에서 제공하는 플랫폼을 활용하면 이 모든 것들을 한큐에 해결할 수 있다. 가장 유명한 데이터 과학 경진대회 플랫폼은 캐글(Kaggle)이다. 캐글은 세계 최대의 데이터 과학자 커뮤니티로 2010년 예측모델 및 분석을 위한 플랫폼 서비스로 출발하여 2017년 구글에 인수되었으며, 2019년 기준 13,000여개의 데이터를 공개하고 있는 명실상부 최대 데이터 과학 서비스 플랫폼이다. 의료, 경제, 자연과학, 공학 등 거의 모든 분야의 데이터를 다루며 무려 190개 이상의 국가로부터 100만명 이상의 회원이 가입하여 활동중이다. 주어진 과제에 예측모델을 만들고 학습 결과를 업로드 하면 정확도가 나오고 이를 기반으로 포인트를 획득하여 레벨을 업그레이드 할 수 있으며 커리어와 직결된다는 점에서 왠만한 게임보다 레벨업이 더 재미있다. 고수가 되어 수상을 하게되면 25,000 ~ 100,000달러에 이르는 상금을 얻을수도 있고, 데이터 과학자로 취업할 수 있는 기회가 주어지기도 한다. 대부분의 입문자들이 처음으로 도전해보는 과제는 Titanic: Machine Learning from Disaster으로, 타이타닉 승선자의 피처 데이터를 바탕으로 생존율을 예측하는 미션이다. 아래 그림은 필자가 이 과제에 도전한 내역으로 Public Score 항목을 보면 0.78947점을 얻은 것을 확인할 수 있다. 모델을 개선하거나 전처리 및 파라미터 튜닝으로 성능을 높일 수 있고 업로드를 반복하여 점수를 올릴 수 있다. 데이터 과학 경진대회 사이트 모음 그동안 알려진 국내외 경진대회 사이트들을 정리해보았다. 경진대회 일정은 수시로 변하기 때문에 본인의 업무 도메인 및 보유 스킬셋과 가장 적합한 커뮤니티를 골라 수시로 방문한다면 많은 정보를 얻을 수 있다. 1. 국내 AI RUSH 2019 (Naver) 2019 AI Starthon (과학기술정보통신부) 2019 빅콘테스트 문화.관광 빅데이터 분석대회 데이터 사이언스 경진대회 인공지능 R&amp;D 그랜드 챌린지 대회(과학기술정보통신부) Samsung AI Challenge 2018(삼성) 네이버 Data Science Competition 2019 카카오 아레나 빅 콘테스트(한국정보통신진흥협회) 날씨 빅데이터 콘테스트(기상청) 디지털 헬스 해커톤(삼성융합의과학원) 2. 해외 캐글(Kaggle) 데이터 사이언스 게임(Data Science Game) 일본 SIGNATE 경진대회 데이터 해커톤(Data Hackathon) 오픈 데이터 해커톤(Open Data Hackathon) 아시아 오픈 데이터 해커톤(Asia Open Data Hackathon) 3. 기타 WEVIFY (공모전 공고 모음) ※ 수시 업데이트 예정(오류 또는 최신정보가 있으신 경우 댓글로 알려주시면 감사하겠습니다.)",
    "tags": "competition list dev",
    "url": "/dev/2019/06/23/dev-competition-list/"
  },{
    "title": "[리뷰] 혼자 공부하는 자바",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"혼자 공부하는 자바\"를 읽고 얻은 지식을 정리한 글입니다. 목차 이 책을 읽으며 Java와 함께한 20년을 돌이켜보았다. 신용권 저자께 직접 들었던 모바일 웹앱 강의 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 이 책을 읽으며 Java와 함께한 20년을 돌이켜보았다. 처음 Java와 만난건 2000년 어느날 프로그래밍 수업 시간이었다. 리뷰는 안하고 왜 갑자기 추억을 회상하는지 의아해 하실 분들이 계실지 모르겠다. 책을 읽기 전에 20년 간 Java를 익혀왔던 프로그래머의 회상과 푸념을 듣는다면 책을 읽으면서 본인이 어떤 학습 위치에 있는지 파악하기 쉬울 것이기에 향후 학습방향에 도움이 된다는 의미로 재미있게 읽어주셨으면 좋겠다. 게시판과 메모장 요즘 학부 프로그래밍 수업에서도 메모장 또는 웹 게시판을 만들라는 과제가 부여되는지 모르겠다. 처음으로 GUI 방식으로 메모장을 만들면서 느끼는 희열은 대단했다. 내가 만든 프로그램을 내가 유용하게 쓰고 더불어 누군가가 편리하게 써주는 것에 보람을 느꼈다. 다만 데스크탑 위에서 돌아가는 GUI 개발에 있어 Java는 실망감을 안겨주기도 했다. 주로 Applet 또는 Swing을 활용하였는데 당시 이클립스가 대세가 아니었던 시절이었기에 Kawa Java라는 IDE를 쓰곤 했다. 지금도 마찬가지로 데스크탑 프로그램으로는 별로 추천하고 싶지 않은 언어이다. 게다가 \"exe\" 파일을 만드는 데 있어서도 너무 낯설었는데 Java가 깔려있어야 돌아갈 수 있다는 사실에 조금 불만이 있었다. JVM위에서 독립성을 보장하는 언어이니 당연한 얘기일진데 초보때는 Visual C++과 달리 UI 컴포넌트 배치도 불편하고, 배포 및 설치도 생소한 이 프로그램에 대한 안타까움을 느꼈던 것도 사실이다. 물론 이 부분에 느끼는 단점은 지금도 마찬가지이다.ㅎㅎ 대신 WEB, Network 기반의 개발에 있어서는 차원이 다른 고급 언어이다. C언어로 게시판을 개발했던 필자는 당시 초보자로서 웹에서 인코더/디코더를 구현하는 것이 얼마나 어려운 일이었는지 모른다. 그런데 Java는 원했던 모든것이 라이브러리화 되어있어 편하게 웹 프로그램을 개발할 수 있었고, 채팅방 Back-End 로직을 만들면서 네트워크 통신을 안전하게 보장하고 Thread등을 활용하여 성능을 높이고 동기화 할 수 있다는 점에 큰 매력을 느꼈다. SCJP1.4 자격증 취득 지금은 이 자격증이 사라지고 Oracle사의 다른 자격증으로 대체되었다고 들었다. Dump라는 일종의 족보(?)를 구하고 달달 외우기만 하면 붙을 수 있었던 시험. 비록 문제와 답이 영어로 되어있다는 점이 어색하긴 해도 합격이 크게 어려운 시험은 아니었다. 비록 취득이 쉬운 자격증이었지만 국제공인 프로그래밍 자격증 중에는 꽤나 인기있는 자격증이었고, 나름 Class, 상속, 인터페이스, 추상화, 다형성, 예외처리 등 객체지향의 개념을 다잡고 정리하는데 좋은 계기가 되었다. 대학 전산원 근로장학생 비록 신분은 근로장학생으로 미천(?)했으나 2000년 초반 대부분의 대학들은 학사서비스를 WEB방식으로 운영하기 시작했으므로 나름 학부에서 배운 기술을 실전에 적용했던, 처음으로 실무로 뛰어들게 된 계기이다. 학생치고는 개발을 잘한다고 칭찬을 들었기에 더 열심히 개발하게 된 계기가 되었고 내가 쓰는 학사 프로그램을 내가 고친다는게 너무 신기했어서 마치 게임하 듯 개발에 푹 빠진 시간이었다. 대학 전산원 알바 시절에는 대부분은 웹 프로그램이 Servlet으로 구현되어 있었다. 물론 지금도 JSP가 Servlet 방식보다 개발하기 편리할 뿐 사실 내부적으로 WAS가 내부적으로 JSP를 Servlet으로 변경한다는 점은 변함이 없으므로 지금보다 low level의 개발을 하고 있다고 표현하는 것이 옳다고 하겠다. 그런데 Ajax도 등장하지 않았던 시절 HTML, Javascript, Java, SQL 언어가 뒤죽박죽 섞여 있는 코드를 보며 정녕 이것이 최선인가 자주 탄식할 수 밖에 없었다. 전산장교 시절과 Factory 개념 진짜 개발자로서의 첫 출발은 바로 이때가 아니었을까 싶다. 생각보다 꽤 큰 규모의 전산실에서 최초로 MVC 기법의 개발을 시작했다. 더불어 DB의 활용도를 높이고자 Factory 클래스를 만들어 싱글톤 디자인 패턴도 활용하기도 했고 나름 Java가 가진 맛을 제법 느껴볼 수 있는 경력을 쌓는 계기가 되었다. 다만 우리나라의 현실은 왜 이렇게도 DB, SQL 중심인가? 그리고 이런 Java의 패턴 몇 가지가 최선인가? 라는 질문이 계속 들기 시작했다. 지금까지 이어온 개발자의 삶 군을 전역할 즈음 우리나라는 최악의 개발 환경을 가지고 있었고 SI, SM 정도의 직군만 존재하여 그것이 영원하리라 생각했다. 다 년간 개발을 하면서 창의적인 개발보다는 찍어내기식 개발에 스트레스는 늘어가고 보람은 줄어드는 패닉에 빠졌고 고민 끝에 가장 개발환경이 편하다는 안정적인 직장에 들어오게 되었다. 그리고 마침내 Framework를 다루기 시작했다. 스트럿츠, 스프링까지 프레임워크를 다루며 그 안에 녹아있는 철학을 음미했다. 예를 들면 스프링에서 활용되는 개념인 DI, AOP등의 개념을 익히며 이 정도는 알아야 프로그래머라고 할 수 있는 것이겠구나, 더불어 소프트웨어 공학을 열심히 익혀야 이런 훌륭한 철학을 기반으로 한 프레임워크를 만들 수 있겠구나라고 깨닫는 계기가 되었다. 모바일의 등장 지금까지도 대세인 모바일 개발 환경이 등장하기 시작한다. 안드로이드 OS가 리눅스와 비슷하여 친숙함도 들겠다 직접 만든 App이 내 모바일 기기 위에서 돌아가겠다 마치 처음 Java를 접했던 호기심이 다시금 발동해서 한동안 App개발에 푹 빠져 들던 시절이었다. 역시 자바는 자바였다. 자바를 아니까 .java도 개발하기 결코 어렵지 않았다. 그리고 광풍이 부는 격변의 개발환경 SNS까지 확산되며 MEAN스택이 등장하기 시작했다. 예전엔 LAMP만 알면 되었는데 프런트엔드 중심의 개발환경이 등장하고 앵귤러, 익스프레스, 리액트, D3, Vue.js 등 프런트 엔드에 엄청난 기술들이 등장했고 이 블로그를 운영하는 Jekyll같은 정적 컴파일 언어, Grant 등이 등장했다. 백엔드 단에는 Go, 얼랭 같은 언어가 등장했고, 딥러닝과 빅데이터가 화두가 됨은 물론 이더리움과 같은 블록체인 개발 언어, Iot 사물인터넷, 그리고 함수형 언어의 대세.. 광풍이라고 표현할 수 밖에 없는 격변의 개발환경이 등장하기 시작하고 심지어 수학, 통계학 등 기초 사이언스에 대한 내공이 얼마나 중요한지 보여주는 시점에 도달하게 되었다. 주제가 인생사로 벗어나는 듯 하여 이쯤에서 각설하려 한다. 그렇다면 이 책이 다루는 범위는 위 개발자 20년 인생에 어느 부분을 다루는 것일까? 미안하지만 잘 쳐줘야 “대학 근로장학생” 시절까지 정도이다. 이 책이 좋지 않다는 말인가? 그렇지 않다. Java의 객체 개념을 정확히 알지 못한다면 그리고 하나의 언어를 정확히 알지 못한다면 다른 언어, 기술, 철학, 프레임워크와 비교할 수 없고 창의적인 개발을 할 수 없게된다. 가야할 길은 멀지만 이 책으로 개념의 초석을 튼튼히 쌓는다면 갈길이 먼 길을 축지법을 써서 이동하는 느낌으로 발전하는 자신을 보게 될 것이다. 이 책은 필자가 처음 Java 개념을 접했던 기본서와 비교하자면 개념을 전달하는 깊이가 수십배는 되는 전달력, 가독성이 좋은 책이다. 프로그래밍 스킬이 쌓일수록 알고리즘과 기초 사이언스, 철학이 얼마나 중요한 것인지 깨닫게 된다. 그저 지식의 양이 중요한 것이 아니라 그 안에 녹아있는 철학과 수학이라는 사이언스가 녹아있는 공학의 핵심 개념을 잡아내는 것이 중요하다. 지식의 양은 세월이 흐를수록 늘어나는 것이지만, 개념을 잡지 못한다면 그저 Copy&amp;Paste 코더를 벗어나지 못하게 된다. 그런 점에서 이책을 강력하게 추천하고 싶다. 신용권 저자께 직접 들었던 모바일 웹앱 강의 이 책의 장점을 하나 더 소개하자면 바로 저자의 내공과 전달력이다. 2013년 경 삼성멀티캠퍼스에서 모바일 웹 앱 &amp; 하이브리드라는 IT전문 교육을 1주일 정도 이수했다. 그때 강사님이 이 책의 저자이신 신용권님이었는데 그때 Core개념을 제대로 전수받은 것이 이후 모바일 개발을 하는데 있어 큰 도움이 되었고 더불어 애매하게 알고 있었던 Java의 개념과 기술을 다시 정리하고 토대를 다지는데 결정적인 계기가 되었다. 이 책은 혼자 공부하는 자바 동영상 강의도 제공한다. 사투리가 구수하게 섞인 음성에 수강생들이 멍하게 있으면 유머스럽게 질책하던 기억이 아직도 생생하다. 더불어 책 마지막 부분엔 혼공용어노트라는 배운 핵심개념을 정리한 용어집도 있다. 프로그래밍도 암기력이 탄탄하면 그 위에 더 큰 작품을 더 빠르게 올릴 수 있다. 이런 저런 측면에서 이 책은 초보자가 프로그래머로서의 길을 나아가기에 참 많은 배려를 한 디테일이 보이는 양서라고 생각한다. 소스 코드의 동작 순서를 화살표로 배치하거나 자바의 Stream을 가독성있게 그림으로 표현한 부분을 보신다면 어떤 의미인지 이해하실 수 있을 것이다. 누가 읽어야 하는가? 초보 프로그래머, 컴퓨터 전공 학부 초년생 Java에 관심있는 타 언어 개발자 기타 비전공자 데이터 분석, 경제학 등 타 분야에서도 프로그래밍을 알면 큰 시너지 성과를 얻을 수 있는 타 학문 전공자에게 컴퓨터 공학 개념의 기초를 다지기에 정말 도움이 되는 책이다. 책의 구성 및 요약 이 책은 크게 세 부분으로 구성된다. 1. Java의 기본문법(1 ~ 5장) 자바 및 이클립스를 그 어떤 블로그 or 책 보다 쉽게 설치할 수 있다. (시작이 반이다.) 변수, 타입, 연산자, 조건문, 반복문 등 기본문법을 익힌다. 어느 프로그래밍 언어에나 존재하는 공통점을 배운다. 2. Java와 객체지향언어의 특징(6 ~ 10장) 객체지향 언어의 특성과 개념을 익힌다. Java에서 설계된 객체지향 기법을 다룬다. 객체의 핵심 개념인 Class, Class의 재사용을 위한 상속, 메소드의 추상화와 구현을 위한 Interface, 중첩 Class, 예외처리까지 익힐 수 있다. 3. Java의 멋진 도구들(11 ~ 14장) Java에 얼마나 유용한 라이브러리가 많이 있는지 체험하는 시간이다. 기본 API클래스인 java.lang., java.util. 사용법을 익힌다. 스레드 및 동기화, 컬렉션 프레임워크, 입출력 스트림 등 자료구조나 병렬처리에 필요한 기법을 익힌다. 요약하며… 20년 전과는 달리 요즘엔 참 좋은 서적과 레퍼런스가 많아 막 프로그래머로서의 길을 출발하는 요즘 친구들이 너무 부럽다. 필자가 처음 Java를 공부한 기본서는 어색한 번역의 내용도 전달력도 딱딱한 그저 그런 책이었다. 친절함은 커녕 이제 뭐 좀 알겠다 싶으면 이상한 코드들이 무더기로 나와 보는 순간 질려버리게 만들기도 했다. 코드는 색상도 들여쓰기도 가독성도 형편없었다. 심지어는 오류도 있었다. 이 책의 저자는 Java의 핵심 개념을 마스터한 전문가이며 나아가 WEB, Mobile 기술에 정통하다. 책도 친절한데 무료 동영상 강의도 해 준다. 부럽지 않을 수가 없다. 이 책은 위에서 언급한 바와 같이 입문자에게 더할나위 없이 좋은책으로 처음 시작하시는 분께 강력 추천한다. 아쉬운 점이 하나 있다면 Java에 해박하신 전문가께서 너무 얕은 기초 지식만 내공을 전수해주신다는 점이다.(하지만 그 개념은 매우 중요하다.) 언젠가 스트림 이후의 Java 세계에 대하여 시리즈로 집필을 계속 이어나가시길 바라며 본 포스팅을 마친다. &lt;한빛미디어 출판사&gt; 개발자라면 믿고보는 “한빛미디어 출판사”라는 수식어가 따라다닐 만큼 IT분야는 물론 다른 분야에서도 양질의 도서를 끊임없이 출판하는 회사입니다. 개발자로서 “나는 프로그래머다”라는 유익한 팟캐스트를 즐겨 듣곤 했는데 한빛미디어에서 후원을 하였기에 수년간 방송이 이어져올 수 있었다 생각하며, 그외에도 리뷰어 활동, 학습지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT 분야에 관심이 많은 분이라면 한빛미디어의 책으로 시작하시면 후회없는 출발을 하실 수 있습니다. 한빛미디어 바로가기",
    "tags": "review java",
    "url": "/review/2019/06/20/review-book-self-study-java/"
  },{
    "title": "[리뷰] 파이썬 라이브러리를 활용한 머신러닝",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"파이썬 라이브러리를 활용한 머신러닝(번역개정판)\"을 읽고 얻은 지식을 정리한 글입니다. 목차 머신러닝을 위한 파이썬의 도구들(Scikit-learn 등) 이 책이 중요한 이유 : 누가 더 적임자인가? (통계학 vs 컴퓨터공학) 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 머신러닝을 위한 파이썬의 도구들(Scikit-learn 등) 바야흐로 딥러닝의 시대다. 알파고를 언급하는 건 이젠 너무나 식상한 일이 되어버렸다. 대신 아카이브에는 일주일에도 수십편의 논문이 쏟아져 나오고 있고, Google I/O 2018에서는 일종의 튜링테스트 기법으로 사람을 속이는 인공지능 상담원이 등장하였다. 구글에서는 멀티미디어 검색 기능은 물론 심지어 세상에 존재하지 않는 강아지의 사진을 만들어 내기도 한다. 그러기에 필자같은 프로그래머를 비롯하여 통계, 수학 분야는 물론이고 경영, 의학 등 다양한 도메인 분야의 전문가들로 부터 깊은 관심을 받고 있으며,이들은 자연스레 최신기술을 익히고자 다양한 방법으로 데이터 사이언스에 접근하고 있다. 딥러닝, 머신러닝 등 Data Science 영역에서 가장 많이 사용하는 프로그래밍 언어 2가지를 꼽으라면 단연 Python과 R일 것이다. 프로그래머라면 보다 범용적으로 활용도가 높은 Python을 선호하는 편이고, 통계분야 전문가를 비롯한 비 프로그래머 계열은 R을 선호하는 편이다. 물론 필자처럼 양쪽에 깊은 관심을 가지고 활용하는 사람도 있다. \"그렇다면 왜 Python일까? 그리고 머신러닝을 위해선 어떤 도구들이 필요한 것일까?\" 이를 위해 먼저 Python이 제공하는 머신러닝 도구들을 살펴 볼 필요가 있다. Scikit-learn 머신러닝에서 가장 많이 활용되는 분류, 회귀, 랭킹, 예측 등 다양한 알고리즘을 내장하고 있는 Python 라이브러리로 머신러닝 알고리즘을 별도로 구현할 필요가 없게 해준다. 오픈소스로 사용 및 배포에 거의 제약이 없다. 더불어 소스코드를 통해 동작방식을 익히기에 적합하기에 자칫 Keras, Tensorflow 등으로 구현에만 너무 치중되어 모델링과 알고리즘에 갈증을 느낀다면 돌아와 머신러닝의 내면을 바라보기에 매우 유용한 라이브러리이다. Pandas 데이터 처리와 분석을 위한 라이브러리로 R의 data.frame을 본떠서 설계 SQL과 같은 질의 기능을 수행할 수 있으며, 마치 RDBMS의 테이블과 같이 각 열의 데이터 타입이 달라도 무관하다. Scipy 고성능 과학 계산용 라이브러리로 선형대수, 함수최적화, 신호처리, 특수 함수 및 통계 분포 기능이 구현되어있다. scipy.parse(희소행렬)을 활용하여 연산속도를 높이는 데 큰 효과를 볼 수 있다. Numpy 기본 수학관련 알고리즘은 물론 선형대수, 푸리에변환, 난수생성기 등을 지원한다. 다차원 배열 ndarray을 기본 데이터 타입으로 간주하며 원소로 동일한 데이터 타입만을 가질 수 있다. 기타 시각화를 위한 Matplotlib, 학습과정을 용이하게 해주는 대화식 실행환경인 주피터 노트북 Ipython, 결정 트리 시각화에 필요한 graphviz 등 다양한 라이브러리로 머신러닝을 지원하고 있다. 이 책은 위와 같은 도구를 소개로 시작한다. 출발하기에 앞서 기본적인 체계를 잡아주기에 가독성과 이해를 도와준다. 더불어 저자가 위의 도구 중 가장먼저 언급한 Scikit-learn의 핵심 개발자 안드레아스 뮐러, 세라 가이도이기에 그 어떤 책 보다도 본서가 Scikit-learn의 철학 및 내부 구조를 학습하는데 적격이라고 생각하며, 보다 수준높은 데이터 사이언티스트가 되기 위해선 머신러닝 알고리즘의 내부를 내것으로 만들어 다양한 데이터로부터 모델링의 능력을 키우는 것이 가장 중요하다고 생각하기에 이 책을 강력히 추천한다. 이 책이 중요한 이유 : “통계학자 vs 컴퓨터공학자” 누가 더 적임자일까? 머신러닝 분야에 종사하고 싶다면 다양한 길이 있을 것이다. 하지만 진정한 데이터 사이언티스트가 되고 싶다면 개인적으로는 위에서 간략히 언급한 바와 같이 모델링이 핵심이라고 생각한다. 데이터가 어떤것이 주어지더라도 분석을 통해 인사이트를 뽑아내는 인재라면 EDA를 비롯 전처리 경험이 풍부해야 하며, 이를 기반으로 유의미한 모델링을 설계하는데 필요한 능력이 충분할 것이기 때문이다. IT의 태생이 그러했듯 딥러닝이 등장하면서 과학으로 불리는 Science의 영역이 Engineering의 영역으로 상당히 많이 옮겨진 듯 하다. 대신 다루는 데이터 영역이 이미지, 동영상, 음성, 텍스트라는 소재에 국한되는 경향이 있으며, 데이터 대신 GAN 같은 의미있는 기법이 등장하긴 하였지만 그래도 딥러닝이 Tabular 성격의 데이터에 유독 취약한 것은 현재까지는 부인할 수 없다고 생각한다. 이에 프로그래머라면 더더욱 머신러닝 더 나아가 통계, 수학, 수리통계학, 정량분석의 분야를 소홀히 해선 안된다는 것을 깨닫곤 한다. 다음의 질문을 생각해보자. \"로또번호를 머신러닝으로 예측할 수 있는가? 과거의 당첨번호로 학습을 시켜 당첨번호를 예측한다? 원리는 몰라도 직관적으로 말도 안되는 일이라고 생각하는 사람이 많을 것이다. 한편으로는 딥러닝이 얼마나 위대한데.. 혹시 딥러닝이라면 가능하지 않을까? 이 질문이 유의미한지 알고 싶다면 그저 Keras, Tensorflow를 잘 다룰 줄 안다고 해결될 문제는 아닐 것이다. 에포크 횟수, 파리미터 튜닝을 열심히 한다고 답이 구해질까? 데이터는 과거 당첨번호를 이용해야 하나? 아니면 어떤 사건이나 영향을 주는 데이터를 수집하면 될까? 말도 안되는 질문들이지만 초보자라면 한번 깊게 생각할 필요가 있다. 우리가 허상에 빠지기 쉬울 때 바른 방향을 제시해주는 것이 통계학과 같은 사이언스 지식이라 생각을 하며 아주 쉬운 확률의 독립사건 개념부터 좀 더 나아가 자기상관성에 이르기까지 기본 개념이 탄탄하면 애초에 말도 안되는 주제로 시간과 노동력을 낭비하지는 않을것이다. 프로그래머라면 Python이라는 범용프로그램 언어에 취해 R과 같은 데이터 분석 전용언어를 비하하거나 Python의 코딩 능력이 데이터 분석 능력을 좌우한다는 일종의 선입견(?)도 버릴 필요가 있다고 생각한다. 필자 또한 비슷한 과정을 거쳤는데 R은 Python과는 달리 행렬 표현이 2차원에 국한되어 종속적이지 않으며, 대부분의 함수에서 포뮬러를 지원하기에 불필요한 코딩을 방지하게 하여 보다 분석에 집중하게 해주는 엄청난 장점이 있다. 물론 Python처럼 범용언어가 아니기에 타 기능과의 매끄러운 연결은 어렵다. 각설하고 Python으로 데이터 분석을 원하는 사람이 모델링의 능력을 키우기 위해선 먼저 Scikit-learn의 내부 구조를 들여다 보는 것이 좋은 수순이라고 생각하며, 이책은 그런 관점에서 매우 훌륭한 스승이 될 것이라고 본다. 더불어 mglearn 라이브러리로 핵심에 벗어나는 코딩량을 줄여 알고리즘의 핵심을 바라보게 해주는 장점을 가지고 있으며, 한가지 더, 시각화된 이미지가 아래 사진과 같이 컬러로 되어 가독성이 좋고 집중도 잘된다. 그리고 역자가 참 맛깔나게 번역을 잘했다. 구현 소스는 Github 리파지토리에 공개되어 있다. 누가 읽어야 하는가? 개발자 개발자의 최대 장점인 코딩 구현능력과 구조화된 사고방식을 활용하여 사이언스 원리 중심의 서적보다 머신러닝의 핵심에 더 쉽게 다가갈 수 있다. 데이터전문가, 연구자, 과학자 통계, 수학, 모델링 지식에 해박하나 좀 더 성능좋은 도구를 자유자재로 다뤄 연구 수행에 있어 효율성을 높이고 싶은 분들도 이 책이 많은 도움이 될 것이다. 그 외 ‘데이터사이언티스트 = 프로그래머’라는 생각을 가지신 분 이유는 위 절에서 자세히 설명하여 생략한다. 책의 구성 및 요약 이 책은 크게 세 부분으로 구성된다. 1. 머신러닝의 핵심 알고리즘(지도, 비지도 학습)(1 ~ 3장) 사실상 책의 핵심내용으로 친절한 개념 설명과 컬러 시각화를 통해 직관적인 이해를 돕는다. 분류, 회귀에 주요 고려사항인 일반화, 결정함수, 예측확률, 불확실성, 차원축소 등 통계 핵심기법이 담겨있다. 거의 모든 핵심 알고리즘을 다룬다.(KNN, 선형모델, 나이브베이즈, 결정트리, SVM, 신경망, K-means, 그외 군집알고리즘 등 2. 특성공학, 모델평가, 파이프라인 등 모델링 핵심지식(4 ~ 6장) 데이터의 성격을 분석하여 다양하고 유용한 전처리 기법을 배울 수 있다. 교차검증을 통한 성능 향상은 물론 다양한 평가지표에 대하여 학습한다. 전처리에 이어 파이프라인을 활용하는 방법을 익힐 수 있다. 3. 텍스트마이닝 및 기타(7 ~ 8장) BOW변환, 불용어 등 전처리, tf-idf 생성, 고급 토큰화, stemming 등 텍스트마이닝 필수 지식을 배운다. 그 외 실무에 유용한 지식 및 앞으로 나아가야 할 방향을 제시한다. 요약하며… 이 책은 프로그래머로서 자칫 데이터 분석을 코딩만으로 배울뻔한 우(愚)에서 벗어나게 해주었기에 높은 점수를 주고 싶으며, 이 책을 읽으며 남이 만든것을 생각없이 따라가 만드는 데 급급했던 습관에서 모델링의 본질에 대해 깊이 생각하며 창조적인 시각의 중요성을 생각해보는 계기가 되었다. 보통 이럴땐 깊은 생각에 잠겨 책을 읽는 속도가 느려지게 마련인데 신기하게도 가독성이 너무 좋아 생각보다 읽는 시간이 오래 걸리지는 않았다. 데이터 사이언티스트의 영역에 발을 담그고 싶다는 꿈을 가지신 분이라면 누구나 꼭 한번 정독하시길 권한다. 끝으로 이 책을 읽을 기회를 주신 한빛미디어에 깊은 감사를 드린다. &lt;한빛미디어 출판사&gt; 개발자라면 믿고보는 “한빛미디어 출판사”라는 수식어가 따라다닐 만큼 IT분야는 물론 다른 분야에서도 양질의 도서를 끊임없이 출판하는 회사입니다. 개발자로서 “나는 프로그래머다”라는 유익한 팟캐스트를 즐겨 듣곤 했는데 한빛미디어에서 후원을 하였기에 수년간 방송이 이어져올 수 있었다 생각하며, 그외에도 리뷰어 활동, 학습지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT 분야에 관심이 많은 분이라면 한빛미디어의 책으로 시작하시면 후회없는 출발을 하실 수 있습니다.(저 역시 최근에 출간된 ‘데이터를 부탁해’까지 100권은 넘게 산 것 같네요…^^;) 한빛미디어 바로가기",
    "tags": "review ml python",
    "url": "/review/2019/06/05/review-book-intro-ml-py/"
  },{
    "title": "[Jekyll Blog] 마크다운(Markdown) 사용법 및 예제",
    "text": "개요 지금 당장 필요한 마크다운(Markdown) 문법부터 단계적으로 배워봅시다. 목차 Markdown이란? Markdown 에디터 뭘 쓸까? Markdown 문법1(반드시 알아야 하는) Markdown 문법2(유용한 부가기능) 실전연습 이미지를 쉽게 업로드 하는 방법 소소한 Tip 그리고 고장났을 때 Markdown이란? Markdown은 문서 작성을 지원하는 태그(Tag) 형식의 문법이다. What is Markdown? (출처 - 위키백과) 마크다운(markdown)은 일반 텍스트 문서의 양식을 편집하는 문법이다. README 파일이나 온라인 문서, 혹은 일반 텍스트 편집기로 문서 양식을 편집할 때 쓰인다. 마크다운을 이용해 작성된 문서는 쉽게 HTML 등 다른 문서형태로 변환이 가능하다. 익숙한 MSWord나 한글(HWP)로 작성하면 안 되나요? 가능하다. 하지만 WEB에서 글을 쓰고 싶다면 이 둘은 적합한 도구가 아니다. 세상의 거의 모든 컨텐츠가 WEB 기반으로 생산되고 소비되기 때문에 이 문제는 중요하다. 물론 이런 편집기로도 Save As(다른 이름으로 저장) 기능을 통해 HTML 확장자로 변환 후 WEB에 올릴수도 있지만 변환된 소스코드의 양과 복잡도를 보면 경악을 금치 못하게 된다. 더욱이 스타일, 표 등이 온전하게 변환되지 않아 원본과 동일한 품질로 보기 어렵다. WEB문서라면 HTML이나 웹 프로그래밍 언어를 써도 되잖아요. 가능하다. 하지만 생산성(작성 속도 및 편리성)에 큰 차이가 있다. 더욱이 WEB언어를 모르는 사람이라면 익숙하지 않은 프로그래밍의 문법을 다시 배우고 능숙해지는데 있어 상당한 노력과 시간을 필요로 하게 될 것이다. 그 외에 좋은점은 뭔가요? 배우기가 정말 쉽고 직관적이다. Text로 저장 후 HTML으로의 변환이 가능하다. 변환을 지원하는 도구나 Eco(생태계)가 매우 많다. Text로 저장하기 때문에 Git을 통한 버전관리가 가능하고, 용량이 적어 보관이 용이하다. Python의 Jupyter Notebook, R의 R Markdown 등 다른 기술을 익히는데 있어 훌륭한 커뮤니케이션 도구로써 가치가 있다. 안 좋은점은? 유일하게 안 좋은 점이 하나 있는데 표준이 없다. 핵심 문법을 제외하고는 에디터에 따라 결과물이 달라질 수 있다. Markdown 에디터 뭘 쓸까? 윈도우 메모장도 상관없다. 하지만 그따위 것을 쓰려고 이 포스팅을 보시진 않을 것이다. 선정기준 Q. 다양한 표현이 가능한가? 논문 수준 수식, 다양한 Icon 이미지, Code Block, UML 다이어그램,.. 등 Q. 어디에 저장되니? PC에만 저장되어서 USB에 들고 다녀야 하는건지? 클라우드 개념으로 어디서든 수정 가능한지? Q. 퍼블리쉬(Publish) 지원여부? Git, 구글드라이브, 블로거, 드랍박스, 워드프레스, 텀블러, PC, 개발서버, .. 등 Q. 얼마면 돼? 얼마나 편리한데? 등 그 외 온라인(인터넷)이 차단(비행기, 네트워크 장애, 비용 문제 등)될 때도 대비할 필요가 있다. 추천 에디터 Prose.io 어디서나 접속 가능하다. Git에 접속하여 배포없이 바로 Markdown의 수정이 가능하다. 위 링크를 클릭하여 필자가 작성한 Prose.io 설치 및 사용법을 알아보자. StackEdit 위에 열거한 구글드라이브, Git, 텀블러 전부 저장 및 배포 가능하다. 어느 PC에서 접속해도 동시성이 보장된다. 예쁜 Icon부터 논문 수식까지 거의 모든 마크다운 표현이 가능하다. MarkdownPad 위 링크에서 다운로드 가능하다. 단, 무료버전은 한계가 많다.(특히 편집탭이 4개밖에 열리지 않는다.) 클라우드 공유 방식이 아닌 PC에 설치하는 프로그램이다. 기능이 아주 뛰어나진 않다. 가끔 서버가 다운되거나 인터넷이 느린 경우 PC에서 작업하기 때문에 사용할 수 있다는 점이 장점이다. 그 외의 에디터 MacDown(for MAC) https://jbt.github.io/markdown-editor 필자의 경우 위 세가지 모두 사용한다. 인터넷이 느린 반응을 보일 경우 Markdown Pad로 초안을 작성하고, StackEdit에서 보정 및 예쁜 시각화 지원을 이용한 후 Git을 통해 배포한다. 이후 경미한 수정을 할 경우 배포까지 하는 것은 배보다 배꼽이 큰 형국이므로, Prose를 이용하여 즉시 수정한다. 일단 초보자라면 또는 포스트를 보고 실습을 원하신다면 위의 Markdown Pad 공식 사이트 링크를 클릭하여 PC에 설치 후 아래 소개될 문법을 익히며 예제를 따라 타이핑해보자. 설치가 아주 쉽다. 그냥 다운로드 버튼으로 다운받아 Next 버튼만 누르면 금방 깔린다. 온라인 에디터의 경우 퍼블리쉬 등 클라우드 환경의 복잡한 부가 기능을 배우느라 Markdown 자체에 집중을 못할 수도 있다. 나중에 차차 익히면 되므로 먼저 Markdown을 익히는 것에 집중하자. Markdown 문법1(반드시 알아야 하는) 글을 작성할 때 마다 꼭 사용하는 문법들만 간추려 글을 쓰는 순서 및 흐름대로 기술하였다. 초보자라면 이 부분만 숙지해도 오늘을 보람찬 날로 만들 수 있을 것이다. 너무 쉬워 더 어려운 문법을 배우고 싶으시다면 다음 Chapter인 Markdown의 기타 문법으로 넘어가시기 바란다. 실습은 간단하다. 마크다운 에디터를 실행한 후, 아래의 [1단계] ~ [8단계]까지 회색박스의 코드를 직접 손으로 코딩하신 후, 미리보기 화면으로 어떻게 보이는지 확인하시면 된다. [1단계] 헤더(Header) : 제목, 문단별 제목을 쓰고 싶을 때 글의 구조(개요) 및 큰 틀을 잡을 때 사용한다. # 제목 1단계 ## 제목 2단계 ### 제목 3단계 #### 제목 4단계 ##### 제목 5단계 ###### 제목 6단계 제목 1단계 제목 2단계 제목 3단계 제목 4단계 제목 5단계 제목 6단계 [2단계] 수평선 : 내용을 명시적으로 구분하고 싶을 때 --- [3단계] 엔터키(줄바꿈, 개행) : 라인을 바꾸고 싶을 때 띄어쓰기 2번을 입력하면.(from) (to)&lt;!-- from과 to 사이에 스페이스 2번 입력--&gt; 줄이 바뀐다. 띄어쓰기 2번을 입력하면.(from) (to) 줄이 바뀐다. [4단계] 목록(List) : 요소를 나열할 때 1. 첫번째 1. 두번째 1. 세번째 + 순서없음 - 홍길동 * 중대장 + 프로실망러 첫번째 두번째 세번째 순서없음 홍길동 중대장 프로실망러 [5단계] 강조 : 문장 내 강조하고 싶은 단어를 눈에 띄게 __볼드(진하게)__ _이탤릭체(기울여서)_ ~~취소선~~ &lt;u&gt;밑줄&lt;/u&gt; __볼드로 진하게 만들다가*이탤릭으로 기울이고*다시 볼드로__(중복 활용도 가능하다.) 볼드(진하게) 이탤릭체(기울여서) 취소선 밑줄 볼드로 진하게 만들다가이탤릭으로 기울이고다시 볼드로(중복 활용도 가능하다.) [6단계] 인용구 : 인용할 경우 또는 분위기 전환시에도 사용(중복 형태 가능) &gt; 위키백과? &gt;&gt; 중대장 드립 검색 &gt;&gt;&gt; \"오늘 중대장은 너희에게 실망했다\" 위키백과? 중대장 드립 검색 “오늘 중대장은 너희에게 실망했다” [7단계] 링크(Link) : 클릭하면 다른 페이지, 다른 부분으로 이동 가능 유형1(`설명어`를 클릭하면 URL로 이동) : [TheoryDB 블로그](https://theorydb.github.io \"마우스를 올려놓으면 말풍선이 나옵니다.\") 유형2(URL 보여주고 `자동연결`) : &lt;https://theorydb.github.io&gt; 유형3(동일 파일 내 `문단 이동`) : [동일파일 내 문단 이동](#markdown의-반드시-알아야-하는-문법) 유형1(설명어를 클릭하면 URL로 이동) : TheoryDB 블로그 유형2(URL 보여주고 자동연결) : https://theorydb.github.io 유형3(동일 파일 내 문단 이동) : 동일파일 내 문단 이동 유형3 문단 매칭방법 : 제목(header)를 복사 붙여넣기 후, 1) 특수문자제거 2) 스페이스를 갯수만큼 -로 변경 3) 대문자-&gt;소문자로 변경 예) “#Markdown! 장점” -&gt; “#markdown–장점” 유형4(상대 경로로 서버 내 파일이동) 기능은 쓸 일이 거의 없어 제외한다. [8단계] 이미지(Image) : 이미지 보여주기 유형1(`이미지` 삽입) : ![이미지](https://theorydb.github.io/assets/img/think/2019-06-25-think-future-ai-1.png \"인공지능\") 유형2(`사이즈를 조절`하고 싶은 경우 HTML 태그로 처리) : &lt;img src=\"https://theorydb.github.io/assets/img/think/2019-06-25-think-future-ai-1.png\" width=\"300\" height=\"200\"&gt; 유형3(이미지 삽입 후, `링크 걸기`) [![이미지](https://theorydb.github.io/assets/img/think/2019-06-25-think-future-ai-1.png)](https://theorydb.github.io/think/2019/06/25/think-future-ai/) 유형1(이미지 삽입) : 유형2(사이즈를 조절하고 싶은 경우 HTML 태그로 처리) : 유형3(이미지 삽입 후, 링크 걸기) 이상 글을 쓸 때 매번 사용하는 Markdown의 문법을 알아보았다. Markdown 문법2(유용한 부가기능) 이 Chapter에서 배울 것들은 위의 기능보다는 사용 빈도가 낮지만 굉장히 고차원 적인 표현을 가능하게 해주는 매우 유용한 문법들이다. 필요할 때마다 참고하여 익히면 큰 도움이 될 것이다. [1단계] 표(Table) : 표 그리기 | | 수학 | 평가 | |:--- | ---: | :---: | | 철수 | 90 | 참잘했어요. | | 영희 | 50 | 분발하세요. |   수학 평가 철수 90 참잘했어요. 영희 50 분발하세요. 라인 단위로 생각하면서 구분자(|)로 열을 구분해주면 위와 같이 대충 그려도 알아서 예쁘게 완성된다. 헤더(머리글)를 분리하고 싶은 경우, 위 예제와 같이 2번째 라인에 ---을 사용하면 된다. 정렬이 필요한 경우, 콜론(:) 기호를 구분선(---) 왼쪽, 오른쪽, 양쪽에 배치한다. [2단계] 수식 : 수학, 논문분석 등에 사용 $$f(x)= if x &lt; x_{min} : (x/x_{min})^a$$ $$otherwise : 0$$ $$P(w)=U(x/2)(7/5)/Z$$ $$p_{\\theta}(x) = \\int p_{\\theta}(2z)p_{\\theta}(y\\mid k)dz$$ $$x = argmax_k((x_t-x_u+x_v)^T*x_m)/(||x_b-x_k+x_l||)$$ f(x)= if x &lt; x_{min} : (x/x_{min})^a otherwise : 0 P(w)=U(x/2)(7/5)/Z p_{\\theta}(x) = \\int p_{\\theta}(2z)p_{\\theta}(y\\mid k)dz x = argmax_k((x_t-x_u+x_v)^T*x_m)/(||x_b-x_k+x_l||) 필자가 사용하는 지킬 테마는 별도 설정없이 위 예제와 같이 자유롭게 사용할 수 있다. 수식 표현에 제한이 있는 경우, MathJax Javascript를 include하여 사용한다. &lt;script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\"&gt; &lt;/script&gt; 표현형식은 Latex 표기법과 동일하다. 몇가지 예를 들자면, 수식은 $$으로 둘러쌓여야 하고 (),{}으로 감싸면 우선순위를 고려한 동일 단위로 인식한다. [3단계] 코드 블록(Code Block) : 소스코드, 외부 인용자료 블록처리 등에 사용 ```python py_vector = one_hot_encoding(\"파이\",word2index) py_vector.dot(torch_vector) &gt;&gt;&gt; 0.0 ``` py_vector = one_hot_encoding(\"파이\",word2index) py_vector.dot(torch_vector) &gt;&gt;&gt; 0.0 `뒤에 python이라고 쓰면 python 언어 스타일에 맞게 구문이 강조된다. 보통 강조하고 싶은 프로그래밍 언어를 그대로 쓰면 된다. ex) bash, cpp, dockerfile, markdown, yml, html, http, json, r, ruby, xml, sql … 등 [4단계] UML 다이어그램 : 순서도, 흐름도 등을 표현할 때 유용하다. 필요시 아래 링크를 참조하기 바란다. Flow charts Sequence diagrams 실전연습 자! 이제 Markdown의 거의 모든 문법을 알아보았다. 백견이 불여일타이므로 반드시 직접 마크다운 문서를 작성해보자. 연습문제1 : 위의 문법 실습을 그대로 타이핑하는 문서 만들기 연습문제2 : 이 포스팅과 동일한 문서 만들기 최대한 정답 없이 위에서 배운 문법을 이용하여 본 포스팅과 동일하게 만들어보자. 성공한다면 앞으로 그 어떤 마크다운 문서 작성도 두렵지 않을 것이다. 연습문제2 정답 필자의 블로그 Github에 접속 우측의 Clone or download(녹색버튼) 클릭 /theorydb.github.io-master/_posts/ 폴더 이동 2019-05-22-envops-blog-how-to-use-md.markdown를 확인 이미지를 쉽게 업로드 하는 방법 이미지를 웹 어디에 저장하는 것이 편리할까? 더불어 포스트 뿐만 아니라 이미지도 마치 데이터베이스 처럼 평생 관리하고 싶다면? 필자가 추천하고 싶은 방식은 크게 3가지이다. GitHub의 Issue를 이용하는 방법 일종의 편법인데 GitHub에서 Issue를 하나 생성한다. Write 탭에 PC에 있는 이미지를 Drag &amp; Drop한다. 최종 저장을 안해도 GitHub에 자동으로 업로드가 된다. 업로드가 다 되면 위 그림과 같은 경로가 생기므로 해당 URL을 복사해서 사용한다. 테스트로 복사한 URL로 접속해보았다. 이 방식은 즉석 URL을 생성하는데는 최고의 방법이나, 대신 이미지를 체계적으로 관리하기가 어렵다. 대신 중요하지 않은 그림은 이 방식으로 운영하면 편리하다고 하겠다. 쉬어가며 저장할지 취소할지 결정되지 않은 작성중인 글에 종속된 이미지를 저장하는 것은 분명 자원 낭비다. 보통 이런 웹프로그램을 개발할 때는 브라우저의 Cache Storage 같은 영역에 임시로 올려두고 글의 저장 요청이 발생하는 순간 같이 전송시켜 I/O 접근을 최소화한다. 또, 단순히 저장공간 낭비만의 문제가 아니다. 네트워크 사용량이 증가하기 때문이다. 만약 AWS같은 클라우드 인프라 위에 이런 프로그램을 개발한다면 엄청난 네트워크 사용량을 유발하게 되고 속도, 저장 공간 문제를 떠나 네트워크 사용량에 따른 과금 폭탄을 맞게 될 것이다. 그런데 GitHub이 그걸 몰라서 이렇게 자원을 낭비하는 프로그램을 개발했을까? 원래도 GitHub은 소스 코드부터 이미지까지 무한에 가깝게 업로드 가능한 저장소이다. 더불어 짧은 생각에 GitHub Pages를 운영하는 사람들이 편리하게 이미지 관리를 할 수 있도록 서비스 개념으로 열어두지 않았을까 싶기도 한다. 한 수 더 바라보면 딥러닝 등에 활용하지 않을까 싶기도 하고.. 아무튼 이렇게 계속 퍼주기만 하는 Git 당신은 도대체… 리스펙트 그 이상이다. GitHub를 이용하는 방법 필자가 자주 애용하는 방식이다. 예를 들면 theorydb.github.io\\assets\\img\\의 위치에 포스트 계층과 동일하게 폴더를 만들어 포스트 제목-일련번호의 형태로 파일을 저장한 후, https://theorydb.github.io/assets/img/think/2019-06-25-think-future-ai-1.png와 같은 방식으로 링크를 걸어 활용한다. 물론, 이미지 파일 관리에 있어 노가다가 첨가되고 GitHub에 이미지를 먼저 올리지 않으면 Markdown을 작성하며 실시간으로 확인할 수 없다는 불편한 점이 있다. 하지만 필자가 처음 블로그를 개발했을 때 가장 중요했던 목적 하나는 블로그 서비스가 종료되더라도 포스트와 이미지를 개인 DB화 하여 영구 보존하는 것이었기에 큰 불만이 없는 방식이다. 더불어 숙달되어 큰 불편을 느끼지 않는다. 기타 구글드라이브, 플리커, 드랍박스에 이미지를 체계적으로 관리하고 URL을 생성하여 연결하는 것도 한가지 방법이다. 큰 불편함을 느끼지 않아 더 찾아보지는 않았는데 이 부분을 쉽게 처리해 줄 Plug-in이 존재할 것으로 믿는다.ㅎㅎ 이미지 만들기 막막할 때 간단한 이미지는 직접 만들자 간단한 도식이나 관계도를 정도는 쉽게 만들수 있도록 서비스를 제공하는 사이트를 추천해보겠다. 오토드로우 정말 자주 애용하는 등장한지 얼마 안된 따끈따끈한 사이트로 강추한다. 마우스로 대충 그리면 그와 유사한 이미지를 AI가 추천하여 선택할 수 있게 해준다. 간단한 그림을 그릴 수 있게 도와주는 사이트 무료 이미지 제공 Site 도저히 개인 실력으로 만들 수 없는 고급 퀄리티 이미지는 아래 무료로 제공하는 사이트를 이용하자. https://pixabay.com/ https://morguefile.com/ http://gratisography.com/ https://unsplash.com/ http://littlevisuals.co/ 소소한 Tip 그리고 고장났을 때 이 부분은 본 포스트에 댓글로 질문이 달릴 경우 하나씩 추가해 나갈 예정이다. 더불어 언제나 통용되는 한가지 해결책을 남긴다. 기능용도로 사용하는 특수문자(*,+,- 등)를 있는 그대로 표현하고 싶은경우 \\ 기호를 앞에 붙이면 된다. 마크다운에서 지원하지 않거나 표현하기 어려운 경우 HTML 태그로 직접 표현하는 것도 한가지 방법이다. 예를 들어 테마의 특성 때문에 줄바꿈이 잘 되지 않으면, 줄바꿈을 원하는 문장뒤에 &lt;BR/&gt; 태그를 원하는 라인 수만큼 넣으면 된다. 이것으로 Markdown의 사용법 강좌를 마치려한다. 꽤 오랜 시간을 내어 성의를 들여 작성했기에 자주 레퍼런스로 활용해 주시면 감사하겠다. 부족한 점은 댓글로 남겨주시면 보완하겠다.",
    "tags": "envops blog github pages jekyll markdown",
    "url": "/envops/2019/05/22/envops-blog-how-to-use-md/"
  },{
    "title": "[Jekyll Blog] (운영에 필요한) GitHub &amp; Jekyll 사용법",
    "text": "개요 이전에 올린 포스팅 GitHub 연동 및 Jekyll 설치는 최초 구축(1회성)에 포커스를 맞추고 있다. 이번 글은 매일 사용하게 되는 글쓰기 즉, 운영에 초점을 맞춰 GitHub &amp; Jekyll 사용법을 다뤄보려한다. 목차 Git &amp; GitHub. 꼭 써야 하나요? (최초 1번만 참고) 회사에서만 작성하다가 처음으로 집에서 수정할 일이 생겼다. (매일 참고) 집, 회사 가리지 않고 아무데서나 적성하고 싶다. 더 알고 싶다면… Git &amp; GitHub. 꼭 써야 하나요? Git은 분산(여러명이 수정할 수 있다.)버전(최최..종을 자동으로 관리해준다.)관리시스템이며, GitHub는 Git으로 생산된 산출물이 저장되는 Git저장소라고 할 수 있겠다. 지역저장소를 관리하기 위한 도구가 Git이며, 원격저장소의 집합체가 GitHub이다. Git을 쓰는 대표적인 이유 여러분이 지금 작성하고 있는 파일은 절대 최종 파일이 아니다. 기껏 회사에서 수정했건만.. 집에 있는 파일 또 수정해야 하나? USB, 클라우드 활용도 지겹다. 열심히 고쳤는데 다른 사람이 고치면서 내가 수정한 것 다 날라갔다. Git Bash vs Git GUI Git을 사용하기 위한 인터페이스는 크게 2가지 방법으로 나뉜다. Bash란 커맨드 모드로 텍스트 기반의 명령어를 통해서 Git을 사용하는 방법이고, GUI는 화면을 마우스로 제어하여 Git을 사용하는 방법이다. 처음에는 GUI가 편하다. 다만 갈수록 복잡해지는 기능을 숙달하기에는 직관적이지 않고 사용하기 어려워진다. 태생이 리눅스 버전을 관리하기 위한 용도로 개발되었기 때문에 Bash 모드에 적응하는 것이 Git의 활용도를 높이는 길이다. 프로그램 소스코드 관리에만 쓰는거 아니예요? 아니다. 물론 프로그램 소스 관리에 주로 사용되지만 그 어떤 문서도 관리 및 공유가 가능하다. 예를들어 개발 블로그 모음페이지는 일반문서로 깃헙으로 관리되고 있다. (최초 1번만 참고) 회사에서만 작성하다가 처음으로 집에서 수정할 일이 생겼다. 먼저 블로그를 작성하려는 장소(PC, 노트북 등)가 변경 시 최초 1회에 한하여 셋팅해야 하는 내용을 다룬다. 이미 블로그를 운영중이라는 가정하에 작성하였으므로 아직 블로그를 구축하지 않은 분들은 GitHub 연동 및 Jekyll 설치를 참고하시기 바란다. 1. Git 설치 및 내 블로그 복사(Clone) Git 설치 먼저 Git을 다운로드하기 위해 아래 그림과 같이 https://git-scm.com/에 접속하여 Download 메뉴를 클릭한다. PC OS버전에 맞는(필자의 경우는 Windows 64-bit) Git 설치파일을 다운로드 받아 설치한다. 이후 설치는 디폴트 설정 그대로 Next 버튼만 누르면 된다. 폴더 생성 및 Git Bash 실행 내 블로그를 다운로드(복사)할 폴더를 생성한다. (필자의 경우는 보통 C:\\githubPages\\theorydb.github.io 위치에서 블로그를 관리한다.) 해당 폴더에 들어가 마우스 우클릭 후 Git Bash Here를 선택하여 Git Bash창을 실행시킨다. Git 사용자 등록 Git Bash창 프롬프트에서 아래와 같은 명령어로 본인 GitHub 계정을 등록한다. $ git config --global user.name \"사용자\" $ git config --global user.email \"사용자 이메일\" 예를들면, 필자는 아래와 같이 설정한다. 참고로 Git Bash창에서의 붙여넣기는 우클릭 혹은 Shift+Insert 단축키를 활용하면 된다. (Git Bash창에서는 대부분의 리눅스 명령어 사용이 가능하다.) $ git config --global user.name \"MIN-HEO\" $ git config --global user.email \"theorydb@gmail.com\" 내 블로그 다운받기(원격저장소 PC에 Clone) 본인의 블로그를 올린 GitHub 원격저장소에 접속하여 아래 그림과 같이 Clone 명령어를 복사한다. Git Bash창에서 아래 그림과 같이 git clone + [복사한 Clone 명령어] 형태로 붙여넣기한다. $ git clone https://github.com/theorydb/theorydb.github.io.git 엔터키를 치면 위에서 만든 폴더에 내 블로그 파일이 복사될 것이다. 2. Ruby &amp; Jekyll 설치 Ruby 설치 루비 인스톨러 공식페이지 https://rubyinstaller.org/downloads/에 접속하자. 아래 그림과 같이 =&gt;로 표시된 Ruby+Devkit 2.5.5-1 (x64)을 클릭하여 다운로드 한 후 Next만 누르며 디폴트로 설치하면 된다. Ruby Prompt 실행 아래 그림과 같이 윈도우 검색창에서 Start Command Prompt with Ruby를 실행한다. 아래 그림과 같이 프롬프트 상에서 chcp 65001를 실행한다. 인코딩을 부여하기 위한 명령어로 실행하지 않을 경우 이후 진행될 온갖 명령어에서 오류가 발생하므로 꼭 진행한다. &gt; chcp 65001 위에서 블로그를 Clone했던 폴더로 이동한다. (아래 명령어는 필자의 예시로, 여러분이 지정한 경로로 이동하여야 한다.) C:\\&gt;cd \"C:\\githubPages\\theorydb.github.io\" Jekyll 라이브러리 설치 Ruby의 gem 명령어를 활용하여 아래 그림과 같이 Jekyll 및 필요한 라이브러리를 설치한다. (참고로, gem이란 python의 pip install과 유사한 기능으로 라이브러리를 설치할 수 있도록 지원하는 도구다.) C:\\githubPages\\theorydb.github.io&gt;gem install bundler jekyll minima jekyll-feed tzinfo-data rdiscount “Successfully…” 메시지가 반복해서 보이다가 “XX gems installed” 문구가 나오면 성공적으로 설치된 것이다. Jekyll 초기화 설정 블로그를 Clone했던 폴더에서 아래 코드와 같이 초기화 설정을 진행한다. C:\\githubPages\\theorydb.github.io&gt;jekyll new theorydb.github.io 위 초기화 과정은 생각보다 많은 에러를 경험하는 부분이기에 자주 발생하는 오류 몇가지를 정리해본다. (트러블슈팅 1) You have already activated i18n 1.8.2, but your Gemfile requires i18n 0.9.5. 오류 발생 시 : project의 버전과 jekyll 설치된 버전이 달라서 발생하는 의존성 문제로 bundler를 설치함으로써 해결할 수 있다. &gt; gem install bundler &gt; bundle install &gt; bundle exec jekyll new theorydb.github.io # 기존 명령어 앞에 \"bundle exec\"을 추가하여 재 실행 (트러블슈팅 2) Could not find public_suffix-3.1.1 … (Bundler::GemNotFound) 오류 발생 시 : bundle을 최신버전으로 업데이트 해준다. &gt; bundle update 혹은 아래와 같이 오류에 명시된 특정 패키지별로 최신버전을 지정하여 설치해준다. &gt; gem install public_suffix --version 3.1.1 3. 블로그 접속 Jekyll 서버 구동 및 블로그 접속 드디어 모든 준비과정이 끝났다. 지킬 서버를 구동해보자. C:\\githubPages\\theorydb.github.io&gt; bundle exec jekyll serve 웹브라우저를 실행하여 http://127.0.0.1:4000/주소로 접속해보자. Apache 등 웹서버를 설치하지 않았지만 블로그가 로컬에서 아래 그림과 같이 잘 실행되고 있음을 확인할 수 있다. 이로써 새로운 기기에서의 블로그 포스팅을 위한 준비 과정이 끝났다. 위에서 언급한 바와 같이 본 설치과정은 처음 1회만 성공하면 다음부터는 별로 고려할 일이 없다. 이어서 매일 포스팅을 작성하는 과정에서 필요한 명령어를 정리해보겠다. 어려움이 있으시다면 댓글을 남겨주시기 바란다. (매일 참고) 집, 회사 가리지 않고 아무데서나 적성하고 싶다. 이제 Git과 Jekyll을 사용하기 위한 환경 구축은 끝났다. 본 장에서는 업무상 긴급한 업무가 발생하여 집에서 수정하여 올린 후, 회사에서 수정한 내용을 다운로드 후 다시 수정하여 올리는 시나리오를 가정하여 설명드리고자 한다. 1. 최신파일 다운로드(Pull) 파일을 작성하기에 앞서 각각의 기기별 상황을 살펴보자. GitHub(원격저장소) : 서비스 운영중인 최종버전 상태 집PC(지역저장소) : 마지막으로 업로드 한 시점이 1년전이라 GitHub와 비교할 경우 최근 1년동안 작성한 A,B,..등의 파일이 없다. 회사PC(지역저장소) : 현 시점엔 GitHub와 동일한 상태이지만 집PC에서 Z라는 문서를 만들어서 올리게 되면 역시 불일치가 발생하게 된다. 이러한 불일치 상황 때문에 파일을 작성하기 전 가장 먼저 해야할 작업은 GitHub의 최신버전 파일을 다운로드 받아 갱신하는 일이다. 블로그를 Clone한 최상위 폴더에서 Git Bash를 실행한 후, 아래 코드와 같이 최신파일을 다운로드(Pull) 한다. $ git pull # 다운로드 $ git log # 커밋내용 확인 2. 새로운 파일을 작성한다. Jekyll 활용(정적컴파일 테스트 기능) : 시작버튼 -&gt; start command prompt with ruby -&gt; 블로그 최상위 폴더 이동(예: C:\\githubPages\\theorydb.github.io) -&gt; jekyll serve (바로 위 챕터 “3.블로그 접속” 참고) 마크다운(Markdown) 사용법 및 예제 : 본 블로그 게시글 참고 3. 아래 코드와 같이 수정한 파일을 포함한 모든 파일을 로컬 저장소에 업로드(Staging) 한다. $ git add --all (혹은)$ git add test.txt # 수정한 파일만 올리는 경우 (트러블슈팅) warning: LF will be replaced by CRLF in Gemfile.lock. 오류 발생 시 : 시스템 간 개행문자(Line Feed)가 달라서 발생하는 문제이다. 리눅스는 LF, 윈도우는 CRLF을 사용하기 때문이다. 협업자간 시스템이 동일하다면 autocrlf 기능을 활용하여 아래와 같은 명령어로 해결할 수 있다. $ git config --global core.autocrlf true # 윈도우끼리 사용하는 경우 $ git config --global core.autocrlf true input # 리눅스, 맥끼리 사용하는 경우 4. 수정된 파일들을 로컬저장소에 업로드(Commit)한다. $ git commit -m \"updates at HOME\" 5. 로컬저장소에 커밋된 파일을 원격저장소에 업로드(Push)한다. 업로드 도중 본인의 GitHub 아이디와 비밀번호 인증을 통과해야 업로드가 성공적으로 완료된다. (매번 인증이 귀찮을 경우 SSH 원격접속 기능을 활용할 수도 있다.) $ git push -u origin master 6. 여러분의 블로그 URL(https://[username].github.io)에 접속하면 수 분 이내로 수정된 사항이 반영된 것을 확인할 수 있다. 이상으로 2개 이상의 기기에서 작업을 수행하는 경우 운영방법을 알아보았다. 이와 같은 상황에서는 풀(Pull) &amp; 푸쉬(Push)를 습관화하는 것이 좋다는 것을 유념하자. 더 알고 싶다면… 개인 블로그는 보통 자기자신 즉, 1인이 관리하기 때문에 위에서 다룬 시나리오와 같이 장소(혹은 기기)가 변경되는 경우 외에는 버전 관리에 큰 이슈가 생기지 않는다. 하지만 사내 기술 블로그와 같이 팀원들과 협업하여 관리하게 되는 경우는 어떻게 될까? 수정한 시점의 차이로 인해 A, B가 지역저장소에 소유한 두 파일이 서로 약간씩 다른 충돌 문제가 발생할 수 있다. 더 깊게 다루고 싶지만 본 블로그의 운영 취지와는 맞지않아 생략한다. 더 관심있는 분들은 [리뷰] Do it! 지옥에서 온 문서관리자 깃&amp;깃허브 입문 포스팅에서 보다 자세한 이슈 사항을 정리하였다. Git관련 여러 도서를 보았지만 제법 난이도가 있기에 본 리뷰에서 소개한 책이 초보자 분들께는 가장 추천하고 싶은 도서이다. 보다 많은 기능을 배우고 싶다면 아래 링크들을 참고하시기 바란다. GitHub 공식 가이드 : https://guides.github.com/activities/hello-world/ 누구나 쉽게 이해할 수 있는 Git 입문 : https://backlog.com/git-tutorial/kr/ 버전관리를 들어본적 없는 사람들을 위한 DVCS - Git : https://www.slideshare.net/ibare/dvcs-git svn 능력자를 위한 git 개념 가이드 : https://www.slideshare.net/einsub/svn-git-17386752 지금까지 GitHub &amp; Jekyll 기반의 블로그 운영 방법에 대해 알아보았다. 블로그, 문서, 프로그래밍 소스코드 가릴것 없이 중복과 버전 관리로 인한 스트레스가 해결되시길 바란다.",
    "tags": "envops blog github pages jekyll",
    "url": "/envops/2019/05/21/envops-blog-how-to-use-git/"
  },{
    "title": "[Jekyll Blog] Tipue Search를 이용하여 블로그 검색 기능 만들기",
    "text": "개요 Tipue Search를 활용하여, 블로그 검색 기능을 구축한 과정에 대한 기록입니다. 목차 Tipue Search란? Tipue Search 설치 Tipue Search 환경설정 최적화 적용을 위한 디테일 마무리 Tipue Search란? 블로그를 운영하다보니 검색 기능이 필요해졌다. 시간이 지날수록 포스트 개수가 늘어나게 되는 것은 당연한 일이기 때문이다. 기술 블로그는 다른 이들과 기술을 공유하는 목적도 있지만 개인적으로 효율적인 기억 관리를 위해 활용하기도 하는데, 정작 본인이 필요한 포스트의 위치가 기억나지 않아 한참 해맨다면 블로그 운영이 무슨 의미가 있을까? 그래서 검색기능을 만들어보기로 결심하였다! 검색 기능을 뭘로 만들지? 우리나라 개발 환경의 고질적인 병폐일까? 검색 기능을 구현하기 위해 가장 먼저 떠오른 것이 슬프게도 DB였다. Oracle, Mysql, Pgsql,… 어떤것을 운영할까? 클라우드를 이용해야 하나? 아! 맞다. 이 블로그는 Jekyll로 만들었지! 당연히 데이터가 DB에 있어야 DB 검색을 활용할 수 있다. 정적 컴파일러 Jekyll로 제작된 블로그에 DB는 당연 고려대상이 아니다. 그렇다면 Front-End 기술을 이용해야 한다는 의미니깐.. JavaScript를 이용해서 만들어야 하나? 고민하던 중 sitemap.xml, feed.xml이 생각났다. XML 기반의 Data가 구조적으로 모여있으니 XML 파싱을 이용하여 검색 기능을 구현해야겠다 마음먹던 중 귀차니즘이 밀려왔다. 난 Front-End 기술을 공유하기 위해 블로그를 만든것이 아닌데.. 데이터 사이언스 기술 공유가 목적인데.. 이걸 굳이 만들어야 하나?^^; Jekyll도 있는데 Front-End 검색 기능이 없겠어? 위대한 구글신께 물어보니 역시나 훌륭한 site search plugin을 발견하게 되었다. 그 이름하여 Tipue Search! 메인 페이지를 들어가보니 대문에 제이쿼리를 활용하여 만들었고 무료이며 빠르다라고 자랑하고 있다. (실제로 자랑 할 만 하다.) 쓸만한지 테스트가 필요하시면 본 블로그 좌측메뉴 About 밑에 붙어있는 검색창을 사용해보시기 바란다. Tipue Search 설치(Window PC 버전) 필자의 블로그에 구현된 기능이 맘에 드셨다면 바로 설치를 시작해보자. Github Repository 접속 : https://github.com/jekylltools/jekyll-tipue-search Clone or download 를 클릭하여, jekyll-tipue-search-master.zip 파일을 다운받아 압축을 푼다. 압축을 풀면 나오는 search.html파일을 본인의 깃헙 블로그 최상위 디렉토리(예: C:\\githubPages\\theorydb.github.io)에 복사한다. 압축을 푼 폴더/assets/안에 있는 tipuesearch 폴더를 본인의 깃헙 블로그 최상위 디렉토리/assets/(예: C:\\githubPages\\theorydb.github.io\\assets\\tipuesearch)아래에 복사한다. 이것으로 설치는 끝났다. 참 쉽죠~? 이제 환경설정을 통해 블로그에 적용해 보자. Tipue Search 환경설정 Jekyll 테마에 따라 설정이 약간 다를 수 있다. 본 블로그의 테마는 Clean Blog로, 동일한 테마를 사용하시는 경우 그대로 적용하면 된다. (운영중인 테마가 달라 적용에 어려움이 있는 경우 맨 하단의 Disqus에 댓글을 남겨주시면 아는 범위내에서 최대한 설명해 드리겠습니다.) 본인의 깃헙 블로그 최상위 디렉토리/_config.yml (예:C:\\githubPages\\theorydb.github.io_config.yml) 파일을 열어 맨 아래에 다음의 코드를 추가한다. tipue_search: include: pages: false collections: [] exclude: files: [search.html, index.html, tags.html] categories: [] tags: [] include 부분의 pages: false의 설정은 pages 레이아웃에 해당하는 일반 html페이지는 검색하지 않겠다는 것을 의미한다.(포스트 내용 검색에 집중하기 위함) exclude 부분의 search.html, index.html, tags.html 페이지는 검색에서 제외하겠다는 것을 의미한다. 본인의 깃헙 블로그 최상위 디렉토리/_includes/head.html (예: C:\\githubPages\\theorydb.github.io_includes\\head.html) 파일을 열어 META영역 제일하단, LINKS영역 바로 위의 위치에 다음의 코드를 추가한다. &lt;!-- tipuesearch --&gt; &lt;link rel=\"stylesheet\" href=\"/assets/tipuesearch/css/tipuesearch.css\"&gt; &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\"&gt;&lt;/script&gt; &lt;script src=\"/assets/tipuesearch/tipuesearch_content.js\"&gt;&lt;/script&gt; &lt;script src=\"/assets/tipuesearch/tipuesearch_set.js\"&gt;&lt;/script&gt; &lt;script src=\"/assets/tipuesearch/tipuesearch.min.js\"&gt;&lt;/script&gt; 본인의 깃헙 블로그 최상위 디렉토리/search.html (예: C:\\githubPages\\theorydb.github.io\\search.html) 파일을 열어 아래 그림과 같이 설정한다. layout : page 부분은 포스팅이 담기는 레이아웃 명칭이다.(테마에 따라 다를 수 있음) permalink: /search/ 부분은 다음 단계에서 설정할 검색어 및 버튼 Element의 form 태그 내 action 속성과 일치시켜야 한다. 'wholeWords' : false 속성은 한글 검색을 가능하게 하는 옵션이다. 'showTime' : false 속성은 검색이 완료되기 까지 소요된 시간을 표시하는 옵션이다. 'minimumLength' : 1 속성은 최소 검색 글자수에 대한 설정으로 필자는 한단어 이상이면 검색가능하게 설정하였다. 그 외의 옵션은 Tipue 메인홈페이지 Tipue Search에 접속하여 Options in the .tipuesearch() method에서 상세하게 확인할 수 있다. 마지막으로 본인의 깃헙 블로그 최상위 디렉토리/_includes/sidebar.html (예: C:\\githubPages\\theorydb.github.io_includes\\sidebar.html) 파일을 열어 아래 그림과 같이 설정한다. [주의사항] sidebar.html 페이지를 수정하는 이유는 필자가 검색창을 붙이길 원하는 위치의 페이지가 sidebar.html이기 때문입니다. 본인의 블로그에 검색창을 붙일 위치를 정한 후 해당 파일 및 파일 내 위치를 정한 후 해당 부분을 수정해야합니다. &lt;form action=\"/search\"&gt; &lt;div class=\"tipue_search_left\"&gt; &lt;img src=\"/assets/tipuesearch/search.png\" class=\"tipue_search_icon\"&gt; &lt;/div&gt; &lt;div class=\"tipue_search_right\"&gt; &lt;input type=\"text\" name=\"q\" id=\"tipue_search_input\" pattern=\".{1,}\" title=\"At least 1 characters\" required&gt;&lt;/div&gt; &lt;div style=\"clear: both;\"&gt;&lt;/div&gt; &lt;/form&gt; action=\"/search\" 설정은 위의 search.html 파일의 permalink 속성과 일치시킨것이다. pattern=\".{1,}\" 속성은 검색어가 1글자 이상이면 검색을 허용한다는 의미로 활용하는 정규표현식 설정이다. title=\"At least 1 characters\" 설정은 위의 pattern을 지키지 않은 채 검색을 시도할 경우 나타나는 알림메시지 문구이다. 설치가 마무리 되었으므로 아래 그림과 같이 검색이 잘 동작하는지 확인한다. 최적화 적용을 위한 디테일 마무리 Tipue Search의 디폴트 기능만 설치된 상태이므로 필자는 블로그에 보다 친화적으로 어울릴 수 있도록 기능을 수정해보았다. 이번 단계는 귀차니즘 가동 시 건너뛰셔도 무방하다. 검색 입력창 사이즈 조정을 위해 C:\\githubPages\\theorydb.github.io\\assets\\tipuesearch\\css\\tipuesearch.css의 CSS 속성을 변경하였다. #tipue_search_input { color: #333; max-width: 150px; max-height: 20px; padding: 17px; border: 2px solid #626591; border-radius: 0; -moz-appearance: none; -webkit-appearance: none; box-shadow: none; outline: 0; margin: 0; } 검색버튼(돋보기모양)이 좌측 메뉴의 배경색에 가려져 잘 보이지 않아 색상을 조절하였고, 본 테마의 img 태그 CSS 속성이 검색창 모양을 삐뚫어져 보이게 만들어 해당 태그의 CSS속성을 상속받아 사이즈를 수정하였다. 마찬가지로 C:\\githubPages\\theorydb.github.io\\assets\\tipuesearch\\css\\tipuesearch.css 파일에서 아래와 같이 CSS 속성을 변경하였다. .tipue_search_icon { width: 19px; height: 19px; margin-bottom: 0rem; background-color: #626591; } .tipue_search_left { float: left; padding: 10px 5px 0 0; color: #e3e3e3; max-height: 20px; } 이로써 Tipue Search 오픈소스를 활용하여 블로그에 멋진 검색 기능을 구축하였다. 다음 기능으로는 Jekyll 기반 블로그의 Disqus 댓글 기능 추가에 대하여 포스팅 할 예정임을 미리알려드린다. Jekyll 기반의 깃헙 페이지 블로그 구축 포스팅은 계속될 예정입니다. 처음 구축하는 방법부터 올리려고 했는데 시간 부족으로 계속 미루고 있네요^^; 포스팅 순서가 어긋나더라도 차후 개인적으로 구현하려는 목표가 모두 완성되는대로 구축을 위한 설계도 개념의 포스팅에서 통합 정리 및 링크부여를 통해 가급적 편하게 보시며 구축하실 수 있도록 노력하겠습니다. 읽어주셔서 감사합니다.",
    "tags": "envops blog github pages jekyll site search tipue",
    "url": "/envops/2019/05/11/envops-blog-tipue-search/"
  },{
    "title": "[Jekyll Blog] Prose.io 연동으로 포스팅을 쉽게! 배포는 더 쉽게!",
    "text": "개요 Prose.io와 Github Pages를 연동하여 더 쉽게 수정하고 배포하는 방법을 알려드립니다. 목차 Prose.io란? Prose.io 회원가입 및 Github 연동 Prose.io로 포스트 수정하기 Prose.io란? Jekyll과 Git을 사용하면서 디자인에 집중하거나 웹 프로그래밍의 기능에 집중하거나 배포 시 SFTP 등을 이용하여 변경된 파일을 일일이 기억하고 하나씩 클릭하여 업로드하는 글쓰기에 집중되지 않는 혹은 반복적이고 불편한 작업에서 크게 해방되었다. 이 정도 수준이면 충분히 만족할 만 하지만 사람은 더 편한 환경을 찾기 마련이다. 낯선 장소에서 블로그를 수정할 일이 생긴다면 Git을 설치하고 Clone 명령어를 수행하고 다시 Commit, Push하는 작업을 피할 수는 없기 때문이다. 게다가 수정하고 싶은 부분이 아주 작은 부분일 때 더욱 그렇다. 글을 수정하는데 걸리는 시간은 겨우 10초 정도인데, 환경을 구성하고 배포를 위한 시간이 5분이 걸린다면 능률이 떨어질 수 밖에 없다. 이런 불편함을 해결하고 싶다면? 답은 Prose.io다. 분명 Jekyll 기반의 정적 컴파일을 해야 HTML로 변환이 될텐데 신기하게도 FTP 또는 DB에 직접 붙어서 글을 수정하는 것 같은 느낌을 받게된다. 연동도 매우 간단하다. 백번 설명하는 것보다 직접 연동하여 사용해보자. Prose.io ? Prose provides a beautifully simple content authoring environment for CMS-free websites. It’s a web-based interface for managing content on GitHub. Prose.io 회원가입 및 Github 연동 Github에 로그인 후, Prose.io에 접속하자. AUTHORIZE ON GITHUB 버튼을 클릭하면, Prose라는 써드파티 App이 Github으로의 접근권한을 요청하는 화면으로 이동하게 된다. Authorize prose를 클릭하여 Prose의 Github 접근을 허용해준다. 패스워드를 입력한 후, Confirm password를 클릭한다. Git의 Project들이 전부 연동된 것을 확인할 수 있다. 이것으로 연동이 끝났다! 참~ 쉽죠? Prose.io로 포스트 수정하기 연동을 완료하였으니, 테스트로 포스트 하나를 간단히 수정해보자. 아래 그림과 같이 블로그 Project 우측의 View Project를 클릭한다. 블로그 글들이 담긴 폴더를 클릭한다.(Jekyll은 대부분 _posts폴더에 작성한 글들이 모여있다.) 아무글이나 하나 선택하여 Edit 버튼을 클릭하면, markdown 편집기가 열려 글을 수정할 수 있다. 미리보기(눈 모양) 버튼을 클릭하면 미리보기로 중간 중간 수정이 잘 되고 있는지 확인할 수 있다. (필자의 경우 개요 맨 뒤에 느낌표 하나를 추가해보았다. 미리보기로도 잘 보여진다.) 수정이 완료되면 아래 그림과 같이 저장버튼을 누른다. 저장이 완료되면 COMMIT 버튼을 눌러 배포한다. 수정내역 알림 : 수정 전,후 변경된 부분을 하이라이트로 알려준다. 권장사항 반영 : markdown 권장 문법에 어긋나게 작성한 것은 자동으로 보정해준다. 블로그에 접속하면 수정한 사항이 정상적으로 반영된 것을 확인할 수 있다. 이제 Prose를 이용하여 어디서든 쉽게 블로그의 글을 수정할 수 있게 되었다. 관련 Eco 환경이 점점 좋아지고 있기 때문에 굳이 프로그래머가 아니더라도 누구든 쉽게 Jekyll 기반의 블로그를 운영할 수 있는 세상이 되어가고 있다. 프로그래머와 거리가 먼 분일지라도 필자의 블로그와 구글 검색을 통해 도전해시길 추천드린다.",
    "tags": "envops blog github pages jekyll prose io markdown editor",
    "url": "/envops/2019/05/04/envops-blog-posting-prose-io/"
  },{
    "title": "[Jekyll Blog] GitHub 연동 및 Jekyll 설치",
    "text": "개요 앞서 선정한 테마를 GitHub에 연동하고 Jekyll을 설치하여 웹브라우저에 직접 블로그를 띄워봅시다. 목차 GitHub 회원가입 및 Fork Git 설치 및 Clone Ruby &amp; Jekyll 설치 Jekyll 디렉토리 구조 파일 수정하기 GitHub에 올리기 블로그 운영하기 GitHub 회원가입 및 Fork 이전글 블로그 테마(Themes) 고르기 및 환경설정에서 멋진 Jekyll 테마를 골랐다는 가정하에 GitHub으로 연동하는 과정을 설명하겠다. GitHub 회원가입 먼저 GitHub https://github.com/에 접속 후 아래 그림과 같이 Sign Up 버튼을 클릭하여 회원가입을 진행한다. Username, E-mail, Password 등 간단한 정보를 입력하면 계정이 생성된다. 사전체크 다음으로 선택한 테마의 소스를 보관할 저장소를 GitHub에 만들어야 한다. 만드는 방법은 크게 2가지가 있다. 하나는 Fork 버튼을 클릭하는 방법이고, 다른 하나는 로그인 후 [Start a Project] 버튼을 클릭하여 직접 Repository를 생성하는 방법이 있다. 후자의 경우 _config.yml을 복사한 후 여러 단계를 거쳐야하므로 초보자에게는 복잡하고 어렵다. 따라서 여기서는 전자의 방법을 택한다. 더불어 각자 선택한 테마가 상이할 것이므로 공통으로 필자의 블로그를 Fork하는 방법으로 실습하겠다. 만약 초보자라면 필자가 운영중인 테마를 실습삼아 Fork 후 전 과정을 구축하며 미리 시뮬레이션 학습한 후에 선정한 테마를 동일한 방식으로 진행하시며 설치하시는 것을 권장한다.(이런 과정없이 처음부터 한번에 구축하기는 결코 쉽지않다.) Fork(Clone)을 통한 저장소(Repository) 생성 필자의 테마 저장소https://github.com/theorydb/theorydb.github.io에 접속 후, 아래 그림과 같이 우측 상단의 Fork 버튼을 클릭한다.(또는 Clone을 통해 PC에 다운로드 후 Git 설치를 통해 Commit, Push로 올리는 방법도 있는데 여기서는 생략한다. 추후 별도의 포스팅을 통해 Git을 설치하고 사용하는 방법에 대해 설명하겠다. 현 시점에서 소개하기엔 분량이 과하여 집중력을 잃고 자칫 포스팅 전체의 큰 흐름을 잃을까 우려되기 때문이다.) 잠시 기다리면 여러분의 계정에 저장소가 생성되어 필자 블로그의 모든 파일들이 저장소 내에 Copy된 것을 확인할 수 있다. (참고)Reporitory 직접 생성하는 방법 https://github.com/theorydb &gt; repository &gt; new &gt; repository name에 username.github.io 입력 &gt; create repository 버튼을 클릭하면 생성된다. Repository name 변경 및 기타설정 Fork가 완료된 저장소에서 아래 그림과 같이 Settings를 클릭하면 최상단에 Repository name이 나온다. 가급적 회원가입 시 사용했던 username을 이용하여 username.github.io 형식으로 저장할 것을 추천한다. 다른 명칭으로 저장할 경우 서브디텍토리가 포함된 보다 복잡한 URL로 접속해야하는 번거로움이 따른다. 그 외 동일페이지 하단으로 스크롤하여 내려가면서 Issues 체크, 브랜치는 Master로 설정할것을 권장한다. (Tip-생략가능) Fork한 저장소를 초기화(삭제)하고 싶은 경우 처음 저장소를 만든다면 실수 및 마음에 드는 형태로 전부 삭제 후 다시 만들고 싶은 경우가 종종 발생한다. 이럴땐 위 3번 과정과 동일한 페이지 맨 밑에 그림과 같은 Danger Zone이라는 빨간색으로 둘러쌓인 영역으로 이동한다. Delete this repository를 클릭 후 패스워드를 한번 더 입력하게 되면 지금까지 만든 저장소가 전부 삭제된다. (Tip-생략가능) username이 마음에 안드는 경우 위 3번 과정에서 URL이 마음에 들지 않아 username을 바꾸고 싶은 경우가 있다. 아래 그림과 같이 우측상단 프로필 아이콘 클릭 &gt; Settings &gt; Account &gt; Change username 버튼을 클릭하면 변경 가능하다. 최초의 블로그를 웹브라우저로 접속 Fork를 통해 아주쉽게 블로그를 완성하였다. 크롬, IE 등 웹브라우저 열어 위에서 설정한 본인의 URL주소 username.github.io에 접속하여 페이지가 잘 뜨는지 확인해보자. Git 설치 및 Clone 위 과정을 통해 비록 알맹이는 남의 블로그지만 껍데기만큼은 내 URL로 접속가능한 반쪽자리 블로그가 완성되었다. 당연히 누구도 이 상태로 블로그를 운영하고 싶진 않을 것이다. 다운받아 수정 후 업로드 하는 과정이 필요한데, 이를 위해서 먼저 PC에 Git을 설치해야 한다. Git설치 Git 다운로드 링크 https://git-scm.com/에 접속하여 Download 메뉴를 클릭한다. 아래 그림과 같이 본인의 운영체제(OS)와 일치하는 아이콘을 클릭한 후, OS버전에 맞는 Git 설치파일을 다운로드 받아 설치한다.(필자의 경우는 Windows이다.) 설치과정은 별 어려움 없이 디폴트 환경 그대로 Next 버튼을 클릭하여 진행해준다. 위에서 Fork 한 저장소를 PC 내 다운로드 할 폴더를 만든다. 탐색기로 해당 폴더에 들어가 우클릭하면 Git Bash Here라는 추가된 메뉴가 보일것이다. 클릭하여 Git Bash창을 실행시킨다. Git 사용자 등록을 진행한다. git config --global user.name \"사용자\" git config --global user.email \"사용자 이메일\" 저장소 PC에 Clone 위 과정에서 생성한 본인의 블로그 저장소에 접속하여 아래 그림과 같이 Clone 명령어를 복사한다. 위에서 실행한 Git Bash 실행창에서 아래 그림과 같이 git clone + [복사한 Clone 명령어] 형태로 다음과 같이 붙여넣기한다.(우클릭만 하면 붙여넣기 효과) $ git clone https://github.com/theorydb/theorydb.github.io.git 엔터키를 입력하면 위에서 만든 PC내 폴더에 파일이 복사될 것이다. 이제 복사한 파일들을 여러분의 환경에 맞게 수정하는 작업을 진행하겠다. Ruby &amp; Jekyll 설치 파일을 수정하기 전에 먼저 Jekyll을 설치해보자. 우리가 Fork한 테마는 Jekyll 기반으로 개발이 되어있기 때문에 Jekyll을 설치하지 않을 경우 우리가 원하는 방식으로 테마를 수정할 수가 없다. 이전글에서 설명했던 것을 복습하자면 Jekyll은 정적 컴파일러이다. Text로 우리가 작성한 Markdown, _config.yml 등의 파일들은 Jekyll을 통해서 _site폴더내의 산출물로 변환되고 해당 산출물이 WEB에서 실행되는 형태라고 할 수 있다. Ruby 설치 Ruby는 또 왜 설치해야 하냐고? Jekyll이 Ruby로 만들어져서 그렇다. Ruby를 몰라도 걱정하실 것 없다. 필자도 Ruby를 쓴 경험이 전무하지만 설치과정이 매우 간단하여 어려움 없이 설치 완료하였다. 윈도우 OS의 경우 Ruby와 개발툴킷을 별도로 설치해줘야 하므로 루비 인스톨러 공식페이지 https://rubyinstaller.org/downloads/에 접속하자. 아래 그림과 같이 =&gt;로 표시된 Ruby+Devkit 2.5.5-1 (x64)을 클릭하여 다운로드 한 후 Next만 누르며 디폴트로 설치하면 된다. 아래 그림과 같이 윈도우 검색창에서 Start Command Prompt with Ruby를 실행한다. 아래 그림과 같이 프롬프트 상에서 chcp 65001를 실행한다. 인코딩을 부여하기 위한 명령어인데 실행하지 않을 경우 이후 진행하게 될 온갖 명령어에서 오류가 발생하므로 꼭 진행하여야 한다. &gt; chcp 65001 PC내 저장소를 Clone했던 위치로 이동한다. (아래 명령어는 필자의 예시로, 여러분이 지정한 경로로 이동하여야 한다.) C:\\&gt;cd \"C:\\githubPages\\theorydb.github.io\" 이제 Ruby에서 지원하는 gem 명령어를 통해 Jekyll은 물론 종속된 필요한 라이브러리를 설치하자. 아래 코드와 그림을 참고하면 된다. 참고로, gem이란 루비에서 제공하는 라이브러를 편리하게 설치할 수 있도록 지원되는 도구다. C:\\githubPages\\theorydb.github.io&gt;gem install bundler jekyll minima jekyll-feed tzinfo-data rdiscount 초기화 설정 Clone을 끝낸 Repository에 아래 코드와 같이 초기화 설정을 진행한다. C:\\githubPages\\theorydb.github.io&gt;jekyll new theorydb.github.io 설치 및 초기화가 완료되면 아래 코드와 그림과 같이 지킬 서버를 구동해보자. C:\\githubPages\\theorydb.github.io&gt;bundle exec jekyll serve 이제 로컬에서 웹브라우저를 실행하여 http://127.0.0.1:4000/주소로 접속해보자. Apache 등을 설치하지 않았지만 블로그가 로컬에서 아래 그림과 같이 잘 실행되고 있음을 확인할 수 있다. 설치는 1회에 걸쳐 끝나기 때문에 앞으로는 위의 복잡한 과정을 전부 숙지할 필요는 없다. 평소 포스팅 작성 시 루비에 접속하여 지킬을 실행하는 정도의 명령어만 알면 되기 때문에 편의상 요약 명령어를 알려드리니 숙지하시기 바란다. 시작 -&gt; start command prompt with ruby -&gt; C:\\githubPages\\theorydb.github.io -&gt; jekyll serve 마무리 사실 단순하고 쉬운 과정이었지만 세상 만사가 어디 그렇게 쉬운가.. 분명 위 과정을 따라하시다가 오류가 나신 분도 계실 것이기에 트러블 슈팅에 관한 포스팅을 따로 정리할 예정이다. 그때까지는 번거로우시더라도 하단에 댓글을 남겨주시면 대답해 드릴 예정이다. Jekyll 디렉토리 구조 자. 드디어 수정을 위한 모든 과정이 갖추어졌다. 그렇지만 저장소 내 수많은 파일 중 어떤 파일을 어떻게 고쳐야 하는걸까? 필자 역시 처음 Jekyll 블로그를 Clone 후 어떤걸 고쳐야 하는지 막막했던 기억이 있다. 하지만 다행인 것은 많은 폴더와 파일들 중 반드시 고쳐야 하는 것들은 사실 많지 않다는 것이다. 이에 도움이 될 수 있도록 디렉토리 구조를 한 눈에 파악할 수 있도록 하단에 디렉토리 구조에 대한 설명을 추가한다. 특히, 반드시 변경해야 하는 사항을 1번에 명시하였으니 도움이 되었으면 좋겠다. 참고로 2번 사항은 주로 디자인, 기능을 커스터마이징하는 경우 참조하게 되고 3번 사항은 거의 건드릴 일이 없는 파일들인데 지금은 건너 뛰셨다가 나중에 필요할 때 다시 돌아와 참고하시면 도움이 될 것이다. 1. 반드시 변경 _featured_tags/ : 카테고리 대분류 폴더 _featured_categories/ : 카테고리 소분류(태그) 폴더 _data/ : 개발자 및 운영자, 기타 정보 폴더 (author.yml 수정이 필요) _config.yml : 가장 중요한 환경변수 설정 파일 README.md : GitHub 프로젝트 애서 소개하게 될 글 favicon.ico : 블로그 접속 시 브라우저 주소창에 표시되는 대표 아이콘 about.md : About 메뉴 클릭 시 나타나는 블로그에 대한 소개글 2. 필요시 변경 assets/ : 이미지, CSS 등을 저장 폴더 _layouts/ : 포스트를 감싸기 위한 레이아웃 정의 폴더(페이지, 구성요소 등 UI변경 시 수정) _includes/ : 재사용을 위한 기본 페이지 폴더 Gemfile.lock : Gemfile에 기록한 레일 기반 라이브러리를 설치 후 기록하는 파일(중복설치 방지) Gemfile : 필요한 레일 기반 라이브러리를 자동으로 설치하고 싶을 때 명시하는 설정 파일 .gitignore : GitHub에 올리고 싶지 않은 파일들은 이 파일에 경로지정 가능(예: _site 산출물, 환경설정, 개인정보, 작성중인 글 등) sitemap.xml : 테마의 사이트맵 search.html : Tipue Search 설치 시, 검색결과를 출력하는 페이지로 활용 robots.xml : 구글 웹로봇 등 검색엔진 수집 등에 대한 정책을 명시하는 설정파일 posts.md : 포스트 작성 관련 설정파일 3. 변경 필요없음(참고) _posts/ : 포스트를 저장하는 폴더 .git/ : GitHub 연동을 위한 상태정보가 담긴 폴더 _site/ : Jekyll 빌드 생성 결과물 폴더(실제 GitPages에서 WEB으로 보여지는 산출물) .sass-cache/ : 레일 엔진에서 사용하는 캐시 저장폴더(변하지 않는 산출물들에 대한 파싱을 하지 않아 속도보장) _sass/ : 일종의 CSS 조각파일 저장 폴더 _js/ : JavaScript 저장 폴더 _plugins/ : 플로그인 저장 폴더(크롬 정책상 어차피 사용안함) LICENSE.md : 테마 개발자의 라이센스 설명 index.html : 블로그 최초 접속 페이지 googlea0d1f22cc8208170.html : 구글 검색엔진에 블로그를 등록하는 과정의 소유권 확인 파일 feed.xml : RSS Feed 활용을 위한 XML browserconfig.xml : 윈도우8 이상 IE11 접속 시 클라이언트가 요청하는 환경설정 파일(윈도우의 표준 파괴 본능은 여기에도 숨어있다. ㅡ,.ㅡ) 404.md : 404 Not Found Page(블로그에 없는 페이지 요청 시 등장하는 페이지) .eslintrc : EcmaScript Lint(자바스크립트 협업 개발을 위한 규칙 정의) 환경설정 파일 .eslintignore : EcmaScript Lint 무시할 규칙 지정(전역변수 에러표시 예외처리 등) .babelrc : Babel(자바스크립트 컴파일러) 설정파일 보다 자세한 사항은 지킬 한글화 공식 홈페이지를 참고하시기 바란다. 파일 수정하기 드디어 파일을 수정해보자. 이제 저장소 내 대충 어떤 파일과 폴더가 있는지도 알았겠다 자신있게 _config.yml파일부터 수정해보자. 위에서 명시한바와 같이 가장 중요한 환경설정 파일으로 여러분의 개인화 정보와 관련된 거의 모든 환경변수가 존재한다. 반드시 수정해야 할 파일이다. 저장소 폴더 루트위치에 있는 _config.yml 파일을 아무 편집기로나 열어보면 주석에 자세한 사항이 명시가 되어있다. 주석의 지시대로 여러분의 환경에 맞게 수정하시기 바란다. 참고로 필자의 경로는 C:\\githubPages\\theorydb.github.io\\_config.yml이다. 더불어, 파일이 제대로 수정되었는지 파악하기 위해선 위에서 말씀드린 로컬사이트에 접속하면 바로 확인이 가능하다. 단, 지금 수정중인 _config.yml 파일의 경우 Jekyll을 껐다 켜줘야 반영된다. Jekyll 로컬 웹서버는 [CTRL]+C 단축키를 누르고 Y를 누르면 자동으로 꺼지고 bundle exec jekyll serve 명령어를 실행하면 다시 구동되니 참고하시기 바란다. GitHub에 올리기 드디어 내가 수정한 파일을 GitHub 저장소에 올려보자. 여기까지 따라오시느라 정말 고생 많으셨다. 마지막 관문이니 만큼 조금만 더 힘을 내서 수정된 블로그를 웹에서 감상하시기 바란다. Git Bash를 실행한 후, 아래 코드와 같이 수정한 파일을 포함한 모든 파일을 로컬 저장소에 업로드 한다. $ git add --all 수정된 파일들을 로컬저장소에 커밋한다. $ git commit -m \"updates\" 로컬저장소에 커밋된 파일을 원격저장소에 업로드한다. 업로드 도중 본인의 GitHub 아이디와 비밀번호 인증을 통과해야 업로드가 성공적으로 완료된다. $ git push -u origin master 여러분의 블로그 URL(https://[username].github.io)에 접속하면 몇 초 뒤 수정된 사항이 반영된 것을 확인할 수 있다. 블로그 운영하기 위에서 열거한 방법은 최초 구축을 위한 방법으로 블로그 포스팅을 작성하고 운영하며 매일 다루게 되는 방법과는 다르다. 이에 운영하며 필요로 하게 되는 방법들은 간략하게 모아 별도의 포스팅으로 작성하였다. 관심있는 분들은 [Jekyll Blog] (운영에 필요한) GitHub &amp; Jekyll 사용법을 참고하시기 바란다. 이로써 여러분의 블로그가 완성되었음을 축하드린다. 다만 Jekyll과 GitHub에 적응하기까지 제법 오랜 시간이 걸릴지도 모른다. 그래서 다음글 Prose.io 연동으로 포스팅을 쉽게! 배포는 더 쉽게!에서는 배포없이 좀 더 편하게 포스팅 할 수 있는 방법에 대해 알려드리고자 한다. 필자가 수정 보완한 테마는 Free License이며 별도 동의없이 자유롭게 사용하실 수 있습니다. 마음에 드신다면 필자의 블로그 저장소 [Fork] 버튼 왼쪽에 있는 [★Star] 버튼을 눌러주시면 큰 힘이 날 것 같네요. ^^. 긴 글 읽으시느라 고생 많으셨습니다.",
    "tags": "envops github pages blog jekyll install push",
    "url": "/envops/2019/05/03/envops-blog-github-pages-jekyll/"
  },{
    "title": "[Jekyll Blog] 블로그 테마(Themes) 고르기 및 환경설정",
    "text": "개요 블로그 구축의 첫 관문. 테마를 고르고 GitHub 저장소에 등록하기 위한 사전 지식에 대하여 알아봅니다. 목차 Jekyll Themes 고르기 Clean Blog 테마를 선택한 이유 Jekyll Themes 고르기 이전글 (개요) 블로그를 만들어 봅시다!에서 Jekyll 블로그를 선택한 이유를 말씀드렸다. 여기까지 읽고 계시다면 이미 Jekyll 블로그를 구축하는데 관심이 있는 독자일 것이다. 그렇다면 Jekyll 테마에는 어떤 것들이 있으며 선정하는 기준에 대해서 언급해 보겠다. Jekyll Themes 제공 사이트 대부분의 오픈소스 개발자들이 그러하듯 지킬 역시 많은 양의 테마를 오픈 소스로 제공하고 있다. 아래 사이트에 접속하시면 무료로 제공되는 테마를 원없이 구경하고 선택하실 수 있다. 오히려 많아서 선택하기 어려울 지경이다. http://jekyllthemes.org/ https://jekyllthemes.io/free http://themes.jekyllrc.org/ https://github.com/topics/jekyll-theme Jekyll Themes의 선정기준 아래 기준에서 반드시 포함시킬 요소를 미리 염두에 둔다면 위 사이트에서 테마를 고르기 더 쉬워질 것이다. 모바일에서도 보기 편한 반응형인가? 한글 폰트 가독성이 좋은가? 커스터마이징하기 쉬운 구조인가? 레이아웃, 줄간격 등 디자인 요소가 마음에 드는가? 검색, 태그, 댓글, syntax highlighting, Summary, Google Analytics, 수식입력 등 기능 지원 여부 Clean Blog 테마를 선택한 이유 필자가 선정한 테마는 Clean Blog이다. 이유는 다음과 같다. 깔끔한 디자인으로 Simple is best 철학이 돋보임 반응형 지원으로 모바일 가독성에도 손색이 없음 한글 폰트가 다른테마 대비 마음에 듬 커스터마이징이 편리함 혹시 필자가 선정한 블로그가 크게 마음에 들지 않으신다면 아래 후보군을 참고하시기 바란다. 필자도 아래 후보군들이 꽤 마음에 들어서 많은 고민을 했다. 필자에게는 낙점되지 못 했지만 독자분들께는 더 마음에 드실지도 모른다. 후보군 Clean Blog : https://blackrockdigital.github.io/startbootstrap-clean-blog/ folio : http://bogoli.github.io/-folio/ Hydejack : https://hydejack.com/blog/ EXAMPLE POST : http://the-development.github.io/flex/ Read Only : https://html5up.net/read-only Mediator : https://blog.base68.com/ Skinny Bones : https://mmistakes.github.io/skinny-bones-jekyll/ lanyon : https://github.com/poole/lanyon 맘에드는 테마를 선택하셨다면 다음글GitHub 연동 및 Jekyll 설치에서 GitHub 연동 및 Jekyll 설치를 통해 웹서버에서 실행하는 방법을 배워보겠다.",
    "tags": "envops blog github pages jekyll themes personal preferences",
    "url": "/envops/2019/05/02/envops-blog-theme/"
  },{
    "title": "[Jekyll Blog] (개요) 블로그를 만들어 봅시다!",
    "text": "개요 블로그를 만들 때 고려할 사항과 만드는 방법에 대하여 알려드립니다. 목차 블로그를 만든 이유 다른 사람들의 이유 미리 고민하면 좋을 것들 어떤 블로그가 완성되어 있을지 상상해보세요!(Check-List) (Where) 어떤 서비스(플랫폼)와 기술을 활용할까? 이 블로그는 어떤 플랫폼을 쓰나요? (How-To) 블로그 구축부터 포스팅까지 배워봅시다. 블로그를 만든 이유 지금 이 포스트를 읽고 계신 여러분은 블로그 하나 만들어 볼까?라는 생각을 분명 한번 이상은 해 보셨을 것이다. 같은 질문에 대한 필자의 대답과 고민의 흔적을 아래에 포스팅 하였으니 이 글이 시행착오를 줄이는데 조금이라도 보탬이 되었으면 한다. 종이가 없어서 바위에 정으로라도 글을 새기겠다는 마음가짐 “글쓴이에게 이 정도 열망은 있어야 명작이 탄생할 수 있는 것이다.” 학부시절 한 은사께서 하신 말씀이다. 이 정도는 너무 거창하지만, 1만 시간의 법칙 때문일까? 요즘들어 부쩍 배우고 익혀온 기술들을 정리하고 싶어 손과 머리가 근질거릴 때가 많았다. 더불어 요즘 큰 관심을 가지고 있는 데이터 사이언스 분야에 대한 배움을 정리할 곳이 간절했다. 비슷한 느낌이 든다면 여러분도 블로그를 운영할 때가 온 것은 아닐까? Relation &gt; Entity 사고방식의 취향이라고 해야할까? 학창 시절부터 단편 지식을 완벽하게 숙지 못했더라도 그 지식이 다른 지식과 연결되어 큰 숲을 이루는 관계에 관심이 많았다. 새로운 것을 배우면 머리 속 수많은 거미줄 어느 모퉁이에 매달아야 하는지 고민을 하게된다. 이런 사고 방식을 충족시킬 수 있는 나만의 지식과 데이터를 쌓기에 블로그만한 것이 없을 것이라 생각했다. 프로그래머라면 Jekyll? 이런 허세 때문에 Jekyll 기반의 블로그를 운영하는 것은 아니지만 직업적으로는 꽤 의미있는 일이 된다. 어느 분야에 종사하시는 분일지라도 제너럴리스트 vs 스페셜리스트사이의 고민은 항상 있으실 것이다. 둘 다 정통한 T자형 인재가 정답이겠지만 시간, 재능, 운의 한계로 결코 쉽지만은 않다. 필자는 프로그래머로 Front-end 보다는 Back-end 기술을 주로 다루며, 서비스보다는 공공 위주 프로젝트를 다루기 때문에 트렌드에 노출될 기회가 적다. 적어도 블로그만큼은 자주 다루기 힘든 기술(Front-end), 트렌드에 뒤떨어지지 않는 기술 그리고 다루고 싶은 기술을 활용하여 지적인 욕구도 충족시키고 제너럴리스트로서의 요건도 갖추고 싶었던 것이 이 블로그를 운영하는 또 다른 이유라고 할 수 있겠다. 내 두뇌는 CPU, Memory, 하드디스크 어디에 가깝지? 컴퓨터는 크게 사고력을 상징하는 CPU, 메타인지를 상징하는 Memory, 기억력을 상징하는 HardDisk, 습득이 빠르고 생각을 전달하는 능력이 탁월한 I/O 등으로 이루어져 있다고 생각한다.(필자가 멋대로 정한 비유이므로 큰 의미는 두지 않으셨으면 한다.) 필자의 경우는 두뇌를 주로 CPU와 Memory의 기능 위주로 활용한다. 때문에 자연스럽게 타 영역에 대한 보완이 필요했는데 특히 기억력을 보완하기 위한 수단으로 블로그를 운영한다. 진리탐구의 연장선 거창하게 써서 부끄럽다. 개똥철학일지는 모르겠지만 내 자식, 지인들에게 그간 노력하며 얻은 지식을 쉽고 빠르게 전달하고 싶은 마음이 컸다. 조금 더 욕심을 부리면 세상 모든 사람들의 시간을 조금이라도 줄여주는데 보탬이 되고 싶다. 다른 사람들의 이유 그렇다면 다른 사람들은 왜 블로그를 운영하는 것일까? 이 포스팅을 읽는 분들께는 이 질문이 더 중요할지도 모르겠다. 조사한 바 크고 작은 이유를 간추려 보았다. 공유 정신으로 스스로 사회에 기여하는 사람이 되고 싶어서 시간을 효율적으로 쓰는 사람으로 보여지는게 마음에 든다. 적극적인 사람 또는 열정을 가진 사람으로 보여지는게 좋다. 사업가 마인드를 가진 사람으로 보인다. 구글 애드센스를 장착하여 광고 수익을 내기 위해서 사업상 마케팅 수단이 필요하여 1인 기업, 파워블로그 등의 활동으로 수익 창출을 위하여(유튜버와 유사) 취미가 글쓰기이거나 쓰는 동안 생각이 정리되며 스트레스가 풀려서 많은 이들과 육아, 가정 등 지식, 노하우를 공유하고 싶어서 배운 지식을 정리하며 메타적 사고를 넓히고자 취업, 경력관리, 이직에 도움이 되어서 PC에 쌓여가는 지식을 정리하고, 온라인에 연결하여 접근성을 확보하기 위해서 SNS 할성화 및 온오프라인 모임을 위하여 Markdown 등의 기술을 써보고 싶어서, 내용에 방해를 받고싶지 않아서 다른 사람의 정보를 모아놓기 위해서, 즐겨찾기 용도로 기타 외로워서, 관심이 필요, 똑똑해보여서, 주위에서 많이들 하길래, 심심해서 등 그 외 여러 목적이 있지만 어떤 이유든 글을 쓰기 위한 충분한 동기가 되지 않을까? 미리 고민하면 좋을 것들 미리 고민하지 않으면 블로그 구축 중 은근히 시간을 많이 잡아먹는 요소들로, 만들기로 결정했다면 일상생활 중 미리 고민해놓으면 시간을 상당히 절약할 수 있다. 네이밍 : 블로그명, Sitename, .. 등 각종 이름짓기 도메인 보유여부 : 도메인 유지비용 및 네이밍 고려 개발스택, 언어 : MEAN, LAMP, Jsp, Asp, .. 등 솔루션(플랫폼) : 미디엄, 브런치, 티스토리, 네이버, .. 등 호스팅업체 : AWS, Google Cloud, MS Azure, KT Cloud, CAFE24, 가비아, .. 등 연동할 SNS, E-mail 등 어떤 블로그가 완성되어 있을지 상상해보세요!(Check-List) 블로그를 만들기로 마음 먹었다면, 미래에 어떤 모습으로 운영되고 있을지 생각해 볼 필요가 있다. (What) 뭘 쓰고있나요? 주로 다루게 될 주제가 무엇인지 생각해보는 것이 좋다. 분야는 너무나도 다양하다. 일기장, 일상, 전원생활, 철학, 소설, … 리뷰, 육아, 교육, 주식, 자기계발, … 게임, 취미, 여행, 요리, 건축, … 역사, 문화, 정치, 종교, 사회, 과학, 예술, …등 흥미있는 주제로 어느 정도 범위를 좁혀놔야 글쓰기의 일상화에 부담이 되지 않아, 가볍게 출발하며 지구력을 얻게되고, 블로그 플랫폼 및 기술 선택 시 도움이 된다. (Who) 누가 읽을까요? 깊게 생각해 볼 문제다.(물론 아무도 없을지도 모른다.ㅎㅎㅎ) 문체, 전개 방식을 선택하는 계기가 되고 플랫폼 선택 후 여러번 삭제했다 다시 만드는 등 시행착오를 줄이게 된다. 더불어 글의 분량, 수준(난이도)을 결정하는데 도움이 된다. (When) 언제 써야 할까요? 계속 쓸 수는 있을까요? 귀중한 시간을 들여 블로그를 제작한 의미가 퇴색하지 않도록 꾸준히 글을 쓰는 습관이 중요하다. 시간 공간적으로 글쓰기에 문제는 없을지, 계속 운영할 수 있게하는 원동력이 무엇일지, 다시금 Why를 새기면서 스스로의 동기부여가 중요하다. (Why) 왜 쓰고 있나요? 이 질문이 가장 중요하지만, 위에서 이미 답이 나왔기에 블로그를 쓰기로 결정했을 것이다. 여기까지는 생각을 정리해보는 것만으로 충분했다. 이제부터 직접 블로그를 개설하는 방법에 대하여 구체적으로 다루어보겠다. (Where) 어떤 서비스(플랫폼)와 기술을 활용할까? 서비스(플랫폼)별 각각 장.단점이 존재하므로, 블로그의 주제 포커싱에 따라 선택이 필요하다. 아래 선정기준을 체크해 본 후, 이어서 한눈에 제시한 비교표를 통해 본인에게 가장 적합한 블로그를 선택하길 바란다. 서비스(플랫폼) 선정기준 (비용) 블로그를 만들 수 있다면 돈을 써도 상관없는지? AWS, Google Cloud, MS Azure, KT Cloud, CAFE24, 가비아,.. 등 웹 호스팅 업체를 알아봐야 한다. (글쓰기 집중) 글쓰기에만 집중하고 싶은지? (커스터마이징) 홈페이지처럼 내 마음대로 만들고, 바꾸고, 꾸미고 싶은지? (개인DB) 데이터베이스처럼 평생 내 자료를 소장하고, 에디팅하고 싶고, 분석도 하고 싶은지? (클릭율) 유저들이 많이 접속하길 희망하는지? (프로젝트 관리)프로그래머 등 직업적인 특성 상 Git 등의 연동이 필요한지? &lt;한눈에 보는 서비스별 장.단점&gt; 서비스(플랫폼) 장점 단점 지킬Jekyll -디자인,기능 거의 무제한 -비용 무료/속도 빠름 -개인DB화 관리 -양방향 서비스 불가 -기능 연동 시 기술적 노하우 필요 워드프레스Wordpress -이지위그(Wysiwyg) 탁월 -APM 웹 프로그래밍 운영가능 -유료 호스팅 운영 필요(AWS, CAFE24 등) -유지보수 상당한 노력 필요 미디엄Medium -깔끔한 디자인 -서비스 오래 지속될 것으로 예상 -포스트에만 집중가능 -커스터마이징, 기능확장 불가 -한글 폰트 문제 -낯선기능(respanse 등) 티스토리Tistory -커스터마이징 자유도 -깔끔한 디자인 -카운팅 등 다양한 기능 -에디터 Wysiwyg 불편 -신규 제작 시 초대장이 필요 -기능의 파편화 텀블러Tumblr -Markdown 지원 -이미지업로드 간편 -다양한 기능 -카테고리, 추천글 기능이 없음 -속도문제 -기술 블로그에는 취약 스팀잇Steemit -블록체인 포스트에 효과적 -스팀달러 Coin 보상 -에디터, 기능이 약간 부족함 브런치Brunch -유사글 연결 기능 -깔끔한 디자인 -포스트에만 집중 가능 -모바일 탁월 -구글 등 해외 검색엔진 노출 문제 -커스터마이징 불가 -코드 삽입과 gist 지원 불가 빙글Vingle -카테고리, 태그 기반 구조 -노출이 약함-사용자 많지 않음 포스타입Postype -창작물 판매,후원 가능 -서비스 조기종료 가능성(스타트업) 구글블로그Blogger -검색노출 최고 -검색 노출 외 좋은 점이 없다. 네이버Naver -국내 클릭율(CTR) 우수 -상품 리뷰에 효과적 -구글 등 해외 검색엔진 노출 문제 -커스터마이징 불가(Markdown 사용불가 등) 그 외 블로그 블로그 서비스는 너무나 다양해서 모든 것을 조사할 수는 없다. 다만 입소문이 제법 나있는 블로그 중 개인적인 직관으로 후보에서 제외시켰던 서비스들을 정리하였으니 좀 더 완벽한 조사를 원하신다면 참고하시기 바란다. Squarespace TypePad Movable Type Drupal Joomla Wix Weebly Strikingly Ghost HubSpot Shopify Jimdo 이 블로그는 어떤 플랫폼을 쓰나요? github.io 형태의 URL로 서비스가 제공되는 Jekyll 기반의 Github Pages 블로그이다. 바로 지금 보고계신 블로그가 그 예이다. 선택한 이유는 다음과 같다. 공짜다. 그런데 유료보다 빠르다. 마크다운(Markdown) 기반 포스트 작성 글 작성이 쉽다. 글쓰기에 집중하기 쉽다. 나만의 데이터베이스를 보유한 효과가 있다. 기능 확장을 위한 Front-End 생태계(Eco)가 풍부하고 수준이 뛰어나다. Git, Ruby등의 기술을 습관처럼 익히게 된다. 협업도 가능하다. 디자인, 기능 등 어떤 홈페이지 보다도 다양한 기능을 구현할 수 있다. 프로젝트 개발 시 Git을 통한 연동으로 기술 블로그로서 최적의 기능을 제공한다. 블로그 만들고 꾸미는 것보다 Publish에 집중하고 싶다. 서비스(플랫폼, 호스팅 등)가 종료되어도 쉽게 보관, 이동이 가능하다. 광고를 달 수 있다. Jekyll외 Ghost, Hexo 같은 정적 사이트 생성기와 연동 가능하다. (How-To) 블로그 구축부터 포스팅까지 배워봅시다. 만약 필자와 동일하게 Jekyll 기반의 GitHub Pages를 만드시기로 결정했다면 아래 순서대로 포스팅을 읽고 따라하시기만 하면 된다.(각 링크를 클릭) 필자와 다른 어떤 서비스를 이용하시든 쭉 훑어보는 것만으로도 큰 도움이 되실 것이므로 블로그를 구축하는 전체 과정에 대한 개요를 잡는다는 생각으로 읽어주시기 바란다. 대부분 구축방법은 이 범위 내 부분 집합일 뿐이다. 참고로 아직 링크가 걸리지 않은 경우, 조만간 포스팅할 예정이니 양해를 부탁드린다. 블로그 구축 블로그 테마(Themes) 고르기 및 환경설정 GitHub 연동 및 Jekyll 설치 Prose.io 연동으로 포스팅을 쉽게! 배포는 더 쉽게! 개인의 취향에 맞도록 디자인 및 환경설정 변경 파비콘 만들기 기능 확장 및 연동 Tipue Search를 이용하여 블로그 검색 기능 만들기 블로그 댓글기능 추가 구글 애널리틱스로 방문자 유입 분석 구글 검색엔진 등록을 유리하게 구글 애드센스를 장착하여 용돈벌기 편리한 운영을 위한 기타 설정 및 오류 발생시 대처방법 글쓰기를 편하게 GitHub &amp; Jekyll 사용법 마크다운(Markdown) 사용법 및 예제 이미지 캡션 활용 이로써 블로그를 구축해야 하는 목적, 방법 등 전반적인 모든 것을 알아보았다. 앞으로도 유용한 기능을 계속 올릴 예정이니 관심있게 지켜봐주시길 부탁드린다.",
    "tags": "envops blog intro",
    "url": "/envops/2019/05/01/envops-blog-intro/"
  },{
    "title": "[R] R 설치 및 환경구성(10분만에 끝내는)",
    "text": "개요 R을 활용한 데이터 분석의 첫관문. 설치 및 환경구성에 관한 포스트입니다. 목차 R이란 무엇인가? R설치(Window PC 버전) R Studio설치(Window PC 버전) R Studio 환경설정 R이란 무엇인가? R은 __데이터 분석을 위한 통계 및 그래픽스를 지원하는 오픈소스__로 Python과 더불어 데이터 분석을 위한 도구로 가장 널리 활용되고 있다. Python이 범용 언어로서 개발자가 보다 선호하는 도구임에 비해 R은 IT 비전공자의 경우에도 널리 사용되는 장점이 있다. 머신러닝, 통계는 물론 금융, 그래픽스, 논문작성 등 다양한 분야에 활용되고 있으며 CRAN에서 이를 가능하게 하는 다양한 패키지를 제공한다. 더불어 참조할 수 있는 레퍼런스가 다양하고, Eco환경이 훌륭하여 데이터 분석을 위한 최고의 도구임에 손색이 없다고 할 수 있겠다. R설치(Window PC 버전) R의 설치는 그 어떤 개발언어의 IDE 설치 과정보다 쉽다. R 공식사이트에 접속 후, Download &gt; CRAN을 클릭 : http://www.r-project.org/ Ctrl+F을 눌러 Korea 검색 후, 원하는 사이트 아무데나 클릭 Download R for Windows 클릭 base 클릭 Download R 3.6.0for Windows 클릭 (버전은 계속 변경됨) 다운로드 완료 후, R-3.6.0-win.exe파일을 더블 클릭하여 설치(디폴트로 Next만 누르면 설치됨) 정상적으로 설치되었는지 확인하기 위해, 아래와 같이 소스코드를 입력 print(\"welcome\") 그림과 같이 입력한 문자열이 그대로 나오면 정상적으로 설치가 완료된 것이다. R Studio설치(Window PC 버전) R Studio는 R 프로그램을 보다 편리하게 활용하기 위한 IDE(통합개발환경)으로 GUI를 지원하며 프로젝트 관리 및 패키지 설치 관리, 환경 설정을 용이하게 해주므로 개발 생산성 향상을 위해 반드시 설치하는 것이 좋다. 마찬가지로 R만큼 설치과정이 단순하다. R Studio 공식사이트에 접속 후, Download RStudio버튼을 클릭 : https://www.rstudio.com/ Free 버전 DOWNLOAD 버튼을 클릭(그 외 버전은 사용제품으로 비용을 지불해야 한다.) RStudio 1.2.1355-Windows 7+ (64bit) 버튼을 클릭(버전은 계속 변경됨) 다운로드 완료 후, RStudio-1.2.1335.exe파일을 더블 클릭하여 설치(디폴트로 Next만 누르면 설치됨) 설치가 완료되면 윈도우 시작버튼을 눌러 하단 검색창에 rstudio를 입력 후, 검색된 프로그램을 클릭하여 실행한다. 정상적으로 설치되었는지 확인하기 위해, Console탭에서 아래와 같이 소스코드를 입력 print(\"welcome\") 그림과 같이 입력한 문자열이 그대로 나오면 정상적으로 설치가 완료된 것이다. R Studio 환경설정 R Studio을 보다 편리하게 사용하기 위한 몇가지 Tip을 소개한다. 편집기 인코딩 방식 변경 그림과 같이 Tools&gt; Global Options &gt; Code &gt; Saving &gt; Change버튼 &gt; UTF-8선택 순서로 클릭) 설정을 변경하면 자동으로 R를 껐다 켤것인지 묻는데 Yes를 눌러준다. 편집기 코딩 폰트 등 스타일 변경 : Tools&gt; Global Options &gt; Appearance &gt; 폰트, 사이즈, 테마 등 선택 화면 레이아웃 변경 : Tools&gt; Global Options &gt; Pane Layout &gt; 화면 위치별 원하는 레이아웃 선택 자동줄바꿈 기능 해제 : Tools&gt; Global Options &gt; Code &gt; Editing &gt; Soft-wrap R source files 체크해제 ※ 참고로 코드 실행 시 Ctrl+Enter키를 누르면 멀티라인 실행이 가능하다. 이로써 R을 사용하기 위한 사전작업이 모두 완료되었다. 다음 포스트에는 간단한 R의 사용방법에 대하여 설명하겠다.",
    "tags": "r install dev",
    "url": "/dev/2019/05/01/dev-r-rinstall/"
  },{
    "title": "[리뷰] 밑바닥부터 시작하는 딥러닝",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"밑바닥부터 시작하는 딥러닝\"을 읽고 얻은 지식을 정리한 글입니다. 목차 IT 분야 15년, 항상 시간과 체력에 목마르다 밑바닥부터 시작하는 딥러닝, 그리고 인생 최대의 격변 책의 장점 및 개요 요약하며… IT 분야 15년, 항상 시간과 체력에 목마르다 개발자로 살아온 지 벌써 15년이 되었다. 그동안 많은 책을 읽고 다양한 프로그램을 만들었지만 요즘만큼 책을 읽고 싶어진 시절이 또 있었나 싶다. 모바일, SNS, 실리콘밸리 등의 영향으로 가뜩이나 빠르게 움직이던 IT기술들은 최근 머신러닝과 만나면서 미래를 예측할 수 없는 속도로 배울 것이 ﻿많아졌고 그 결과로 셀 수 없는 변화가 일어나고 있다. 덕분에 개발자로서 심장이 두근거릴 정도의 기대를 갖게되었고 동시에 우주같이 넓은 지식에 나의 시간과 몸은 참으로 보잘것 없다는 막연함과 두려움을 느끼기도 한다. 밑바닥부터 시작하는 딥러닝, 그리고 인생 최대의 격변 이러한 불안하면서도 스릴있는 이상 상태를 잠시 진정시켜주는 책이 밑바닥부터 시작하는 딥러닝이라는 책이다. 선무당이 사람 잡는다고 그간 프로그램에 창의성을 부여한다는 것이 회의적이었다. 알파고가 처음 등장했을 때 이세돌의 100% 승리를 장담했던 이유도 앨런튜링 그리고 인공지능의 겨울 이후 큰 변화가 없는 인공지능 기술 및 연구에 대하여 무시하는 마음이 적지 않았던 듯 싶다. 하지만 SNS, 모바일 등으로 빅데이터가 수집되고 컴퓨팅 파워의 발전으로 딥러닝 수준의 연산을 처리할 수 있게된 근래의 환경. 그리고 연역법과 귀납법의 오랜 싸움에서 100% 정답을 장담하지 못하는 귀납법을 택한 통계학과 머신러닝의 선택이 이런 거대한 변화를 가져올 줄은 예전엔 상상할 수 없었다. 책의 장점 및 개요 나름 15년간 이 분야에서 많은 노력을 기울였고 스스로 만족스러운 성과를 달성하며 살았다고 스스로에겐 후한 점수를 주곤 하지만 눈에 보이는 가시적인 성과로 이어지지 못한것이 일상을 가끔 허탈하게 만들곤 했는데, 그간의 빛바랜 줄로만 알았던 노력이 딥러닝을 이해하는 과정에서 더불어 이책을 이해해가는 과정에서 좋은 자양분이 되었다는 점에 스스로에게 위안을 주는 계기가 되었고 더불어 내 미래의 방향과 목표까지 제시해 주었기에 책에 후한 점수를 주고 싶다. 책의 장점과 개요를 몇가지로 간단히 요약해 보았다. 개앞맵시라고 소개한 로드맵 사이트를 통해 근래 출시된 머신러닝 관련 우수한 책을 소개하고 지속 발전이 가능한 학습의 방향을 제시한다. 카페, 텐서플로 등 딥러닝 프레임워크로 딥러닝 학습을 시작하는 경우 내재된 근본 원리를 경시하게 되기 마련인데 제목 그대로 밑바닥의 기초 내공을 튼튼히 해준다. 책을 읽으며 이런 질문도 하게 된다. 컴퓨터는 사람이 가진 수천억 뉴런의 동시다발적 상호작용을 전기 신호로 모방한다. 그렇다면 인간의 에너지는 어떤 신호를 모방하는가? 이렇듯 꼬리에 꼬리를 무는 근원적인 질문이 자연스럽게 떠오르며 보다 넓은 철학 영역에의 사고를 쉽게 허락한다. 파이썬 프로그래밍 문법, 패턴을 간결하고 길지 않은 지문을 할당하여 빠른 시간내에 익힐 수 있게 도와준다. numpy 등 파이썬의 딥러닝 관련 라이브러리를 집중적으로 설명하여 파이썬 익히다 지쳐 딥러닝까지 가지도 못하는 주화입마를 해결해준다. 무엇이 초점인지 분명하게 해준다. 나아가 미적분, 선형대수의 기본 등 딥러닝에 필요한 기초 수학지식을 든든하게 채워준다. 70년대에 인공지능의 한계에 부딪히게 했던 XOR의 비선형에 대한 고찰을 하게 해준다. 딥러닝은 빅데이터 기반에 다양한 층을 기반으로 한 매개변수 및 가중치가 핵심이라는 점을 분명하게 해줘 나아갈 방향에 흔들림이 없게 초점을 맞춰둔다. 특히, 퍼셉트론에서 신경망으로 넘어가기에 다소 매끄러워진다. 시그모이드, ReLU, 다차원 배열 연산에 대해 간결하면서도 핵심을 놓치지 않도록 알기 쉽게 정리해준다. 역전파에 대하여 이 책보다 알기 쉽게 설명한 책이 또 있을까? 요약하며… 딥러닝을 위한 기본적인 지식을 튼튼하게 하여 역전파까지 쉽게 이해시키는 과정을 볼 때 시중의 어떤 책을 읽어봐도 핵심에 집중하게 해주는 책은 본 적이 없는 듯 하다. 흔한 알기쉽게 설명한 인공지능 서적은 그 핵심 기술을 익히는데 둔하게 만들고, 기술서적은 각종 장애물 때문에 많은 시간을 소모하게 하는데 이 책은 그런 문제점이 없는 거의 유일한 책이다. 딥러닝에 관심이 있는 사람이라면 이 책을 필독서로 권장한다.",
    "tags": "review dl",
    "url": "/review/2019/04/18/review-book-dl-from-scratch/"
  },{
    "title": "[리뷰] 하이퍼레저 블록체인 개발",
    "text": "개요 본 리뷰는 한빛미디어 출판사 \"하이퍼레저 블록체인 개발\"을 읽고 얻은 지식을 정리한 글입니다. 목차 블록체인과 비트코인, 그리고 리눅스 재단의 선택 하이퍼레저 패브릭(Hyperledger Fabric) 누가 읽어야 하는가? 책의 구성 및 요약 요약하며… 블록체인과 비트코인, 그리고 리눅스 재단의 선택 2017년 여름부터 2018년 1월까지 비트코인의 열풍은 그 어느 때 보다도 뜨거웠다. 연령이 젊은 사람들은 대부분 소액이라도 투자를 해봤을 것이고 노년층도 예외는 아니었다. 투기의 광풍은 블록체인의 진가를 바래게 만들었지만 다른 한편으로는 블록체인의 인지도 대중화에 기여를 했다고 생각한다. 비트코인이 블록체인 기술 및 시장에 끼치는 양면은 이 책에도 언급하고 있듯이 퍼블릭 블록체인(무허가형 원장)과 프라이빗 블록체인(허가형 원장)의 대립으로 나타났고 과연 관련 업계, 기술 및 비지니스 진영 그리고 대중들은 어느 진영을 손을 들어주었는지 궁금했다. 퍼블릭 블록체인(Public Blockchain) 비트코인의 창시자 나가모토 사토시의 개발 철학이 핵심. 리먼브라더스 부도를 수습하고자 정부는 양적완화 정책을 펼쳤고 그 피해는 고스란히 서민이 떠안게 되어 자본주의 경제체제는 결코 민주주의가 아님을 천명하였다. 모두에게 공정한 경제 체제를 유지하고자 비트코인을 창시한 것으로 알려져있으며 현재 우리의 불공평한 자본주의 시장을 해결하는데 가상 이상적인 제도라고 주장한다. 퍼블릭블록체인의 체계를 유지하기 위해 POW(작업증명) 방식으로 채굴을 통한 경제적 보상이 이루어지며 이는 금본위제 당시의 공정성에 착안한다. 프라이빗 블록체인(Private Blockchain) 반면 프라이빗 블록체인의 경우 법적인 책임을 지는 허가받은 사람만이 참여할 수 있는 블록체인으로 별도의 코인을 발행하여 인센티브를 지급하지 않는다. 이상적인 모토를 가지고 있지 않으나 국가, 금융기관, 기업, 개인이 참여하기에 현실적으로 더 실현 가능성이 높으며 기존 경제체제를 흔드는 개념이 아니라 기술적인 효율성을 추구하는데 포커스가 맞춰져 있다. \"여러분은 어느 진영의 블록체인을 선호하시는지?\" 대다수의 필자와 같은 서민이라면 퍼블릭을 선호할지도 모른다. 하지만 이상은 올바를지 몰라도 비트코인은 모든 경제주체를 고려하지 않은 이상적인 모델의 일부를 구현한 것에 불과한 것이라 생각하며, 블록체인을 구성하기 위한 가장 중요한 문제인 합의에 있어서도 실현 가능성이 제로에 가깝다. 현실적으로 투기의 광풍을 가져왔으며 오히려 일부 서민 경제를 흔들리게 만들었다. 더불어 탈중앙화로 강력한 보안을 자랑하지만 거래 자체가 이미 거래소를 중심으로 이루어지기 때문에 전체적인 보안이 보장된다고 보기 어렵다. 과연 양쪽 진영의 블록체인 중 어떤 기술을 받아들이고 어디에 투자를 해야 할까? 필자는 이 질문에 이 시대를 밝은 방향으로 이끌고자 노력해 온 선구자들의 혜안을 빌려보고자 한다. 필자가 선택한 선구자는 리눅스의 창시자 리누스 토르발즈 그리고 그의 사상과 활동을 지원하는 Linux Foundation(리눅스재단)의 행보를 살펴보며 방향을 판단하고자 한다. 리눅스 재단은 대다수의 사람들이 아는 바와 같이 CopyLeft 정신으로 유명한 재단으로, 정보 및 자본의 불균형에 저항하여 공유의 위대함을 증명해왔던 단체로 세상 그 어떤 기업, 단체보다 보다나은 삻에 기여한 바가 크다고 생각한다. 결론부터 말하면 그들의 선택은 프라이빗 블록체인이었다. 하이퍼레저 패브릭 본 서적과 관련하여 가장 중요한 2개의 단체를 꼽으라면 리눅스 재단과 IBM이다. 책의 제목에서 소개한 Hyper ledger는 앞서 언급한 리눅스 재단에서 주관하는 프로젝트이며, 그 중에서도 Hyper ledger Fabric은 IBM이 주도적으로 개발한 프레임워크이다. 평소 개발자로써 기술로 사회에 효율성을 제공하고 싶었으며, 리눅스 재단과 그 방향을 같이 하고 싶어 본 진영의 선두에 자리잡은 Hyper ledger fabric에 깊은 관심을 가지고 있었으나 부족한 레퍼런스에 어려움을 겪고 있던 중 이 책을 접하고 기쁨을 감출 수가 없었다. 하이퍼레저 패브릭은 모듈형 아키텍처를 고수하여 솔루션을 개발하기 적합하도록 설계되었으며 플러그 앤 플레이 구성요소를 허용하고, 컨테이너가 응용프로그램 논리 부분을 구성하는 체인코드라는 스마트 계약을 호스트할 수 있게 설계되었다. 전체 구성도는 아래와 같다. 구현 소스는 Github 리파지토리에 공개되어 있다. 누가 읽어야 하는가? 블록체인 비지니스 실무자, 스타트업 종사자 이 책은 그저 뛰어나기만한 레퍼런스가 아니다. 저자가 바로 Hyper ledger fabric을 주도적으로 개발하는 IBM 블록체인 랩의 이사와 관련 부서의 실무진들이다. 더불어 그런 고수들이 집필한 책인데도 불구하고 블록체인의 지식에 대해 굉장히 겸허하고 조심스럽게 의견을 피력해서 더욱 진솔하게 읽어나갈 수 있었다. IBM이 주도한 특성상 기업과 관련된 측면의 서술이 많았기에 이 분야의 시장에서 성과를 얻고자 하는 사람이라면 필독서라고 할 수 있겠다. 개발자(Dapp개발자, 블록체인에 관심이 많은 프로그래머, 그 외 개발자) 굳이 블록체인 개발자가 아닐지라도 빅데이터에서 등장하는 CAP Theory를 다른 각도로 바라볼 수 있는 계기가 되며, RDBMS가 가지는 한계를 극복하기 위한 아이디어도 얻을 수 있다. 뿐만 아니라 이 책의 예제코드를 실습하면서 자연스럽게 최신 개발환경인 Git, Docker, 애자일 등의 활용 기술에 능숙해지는건 보너스다. 투자자 퍼블릭 블록체인을 맹목적으로 투자할 것이 아니라 앞으로의 금융, 경제 관련 변화를 심도있게 고민해볼 수 있는 계기를 준다. 특히, 퍼블릭의 인센티브 개념과 관련하여 단순히 채굴 등으로 쉽게 얻는 코인이라는 생각, 투자로 인해 수익이 발생하는 주객이 전도된 위험한 개념에서 벗어나 마치 스타트업처럼 가치 창출을 얻었을 때 진정한 인센티브가 얻어지는 것이라는 개념을 깨달을 필요가 있다. 그럼에도 투자를 하고 싶다면 이 책에서 소개하는 Git 등의 기술적인 개념을 파악하여 투자 대상의 블록체인이 과연 기술적으로 어느정도 성숙도를 가지고 있는지 판단하시길 권한다. 책의 구성 및 요약 이 책은 크게 세 부분으로 구성된다. 1. 블록체인, 하이퍼레저, 그리고 기업(1 ~ 3장) 블록체인의 개념 및 앞으로의 발전방향을 전망하고 기술생태계를 조망한다. 하이퍼레저 패브릭의 아키텍처 및 개발 목적을 다룬다. 기업 관점에서 비지니스와 블록체인의 관계를 심도있게 설명한다. 2. 스마트계약, 응용프로그램 통합, 비지니스 네트워크 구성 등 구축 방법(3 ~ 7장) 구글의 Go언어를 활용하여 스마트 계약관련 체인코드를 개발한다. 전체적인 종단 간 비지니스 네트워크를 모델링, 설계, 배포하는 방법을 익힌다. 사용자 응용프로그램에서 전체 스마트 계약에 이르는 완전한 비지니스 네트워크를 구성한다. 3. 데브옵스 애자일, 주요 운영이슈, 거버넌스, 보안, 미래 등 운영 방법(8 ~ 12장) 애자일 기법의 운영을 통한 데브옵스 달성 및 CI 파이프라인 구현 조직 및 컨소시엄이 직면하게 될 주요 활동 및 과제에 대한 인식 거버넌스 및 규제에 대한 통찰. 네트워크의 수명 및 확장성을 보장하는 방법 보안설계 기법 및 향후 과제와 기회에 대한 전망 요약하며… 책이 굉장히 심도깊고 기술적으로 고난이도의 내용을 담고 있음에도 불구하고, 책을 다 읽는데 일주일이 걸리지 않았다. (물론 코드를 구현하고 완벽히 이해하려면 몇 달은 족히 걸릴 듯 하다.) 그만큼 흥미진진하고 블록체인은 물론이거니와 개발자로서 평소 고민했던 부분에 대한 해답과 철학이 담겨져있기 때문일 것이다. 위에 추천한 독자의 대상이라면 반드시 일독할 것을 권한다. 이 책은 나의 인생책 중 한권으로 서재에 남아 오랜기간 두고두고 읽혀지게 될 것 같다. 끝으로 이 책을 읽을 기회를 주신 한빛미디어에 깊은 감사를 드린다. &lt;한빛미디어 출판사&gt; 개발자라면 믿고보는 “한빛미디어 출판사”라는 수식어가 따라다닐 만큼 IT분야는 물론 다른 분야에서도 양질의 도서를 끊임없이 출판하는 회사입니다. 개발자로서 “나는 프로그래머다”라는 유익한 팟캐스트를 즐겨 듣곤 했는데 한빛미디어에서 후원을 하였기에 수년간 방송이 이어져올 수 있었다 생각하며, 그외에도 리뷰어 활동, 학습지원 등 다양한 분야에서 사회에 공헌하는 개발자와 공생하는 업체입니다. IT 분야에 관심이 많은 분이라면 한빛미디어의 책으로 시작하시면 후회없는 출발을 하실 수 있습니다.(저 역시 최근에 출간된 ‘이렇게 쉬운 통계학’까지 50권은 넘게 산 것 같네요…^^;) 한빛미디어 바로가기",
    "tags": "review blockchain",
    "url": "/review/2019/04/02/review-book-hyperledger-fabric/"
  },{
    "title": "Welcome to Jekyll!",
    "text": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for code snippets: def print_hi(name) puts \"Hi, #{name}\" end print_hi('Tom') #=&gt; prints 'Hi, Tom' to STDOUT. Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.",
    "tags": "jekyll update",
    "url": "/jekyll/update/2019/03/30/welcome-to-jekyll/"
  }]};
